{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import random\n",
    "\n",
    "import kerastuner as kt\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "plt.rc('font', family=fm.FontProperties(fname=\"c:/Windows/Fonts/malgun.ttf\").get_name()) # for Windows OS user\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from tensorflow import keras\n",
    "from scipy.stats.mstats import gmean\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import sys, warnings\n",
    "if not sys.warnoptions: warnings.simplefilter(\"ignore\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM SEEDS RESET\n"
     ]
    }
   ],
   "source": [
    "# 매번 모델링을 할 때마다 동일한 결과를 얻으려면 아래 코드를 실행해야 함.\n",
    "\n",
    "def reset_seeds(reset_graph_with_backend=None):\n",
    "    if reset_graph_with_backend is not None:\n",
    "        K = reset_graph_with_backend\n",
    "        K.clear_session()\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        print(\"KERAS AND TENSORFLOW GRAPHS RESET\")  # optional\n",
    "\n",
    "    np.random.seed(1)\n",
    "    random.seed(2)\n",
    "    tf.compat.v1.set_random_seed(3)\n",
    "#    os.environ['CUDA_VISIBLE_DEVICES'] = ''  # for GPU\n",
    "    print(\"RANDOM SEEDS RESET\")  # optional\n",
    "   \n",
    "reset_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn = pd.read_csv(os.path.abspath(\"../input\")+'/X_train.csv', encoding='cp949')\n",
    "tst = pd.read_csv(os.path.abspath(\"../input\")+'/X_test.csv', encoding='cp949')\n",
    "y = pd.read_csv(os.path.abspath(\"../input\")+'/y_train.csv', encoding='cp949')\n",
    "IDtest = tst.custid.unique()\n",
    "data = pd.concat([trn, tst], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1040039 entries, 0 to 414954\n",
      "Data columns (total 19 columns):\n",
      " #   Column           Non-Null Count    Dtype \n",
      "---  ------           --------------    ----- \n",
      " 0   custid           1040039 non-null  int64 \n",
      " 1   sales_month      1040039 non-null  int64 \n",
      " 2   sales_day        1040039 non-null  int64 \n",
      " 3   sales_dayofweek  1040039 non-null  object\n",
      " 4   sales_time       1040039 non-null  int64 \n",
      " 5   str_nm           1040039 non-null  object\n",
      " 6   goodcd           1040039 non-null  int64 \n",
      " 7   brd_nm           1040039 non-null  object\n",
      " 8   corner_nm        1040039 non-null  object\n",
      " 9   pc_nm            1040039 non-null  object\n",
      " 10  part_nm          1040039 non-null  object\n",
      " 11  team_nm          1040039 non-null  object\n",
      " 12  buyer_nm         1040039 non-null  object\n",
      " 13  import_flg       1040039 non-null  int64 \n",
      " 14  tot_amt          1040039 non-null  int64 \n",
      " 15  dis_amt          1040039 non-null  int64 \n",
      " 16  net_amt          1040039 non-null  int64 \n",
      " 17  inst_mon         1040039 non-null  int64 \n",
      " 18  inst_fee         1040039 non-null  int64 \n",
      "dtypes: int64(11), object(8)\n",
      "memory usage: 158.7+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mean_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_mean = []\n",
    "newfeat = trn.merge(y, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = newfeat[['custid','str_nm','age']].set_index('custid')\n",
    "f= f.groupby('str_nm').agg('mean')\n",
    "f = f.to_dict()['age']\n",
    "data['str_nm_target'] = data.str_nm.apply(lambda x: f.get(x,0))\n",
    "f = data.groupby('custid')['str_nm_target'].agg([('str_nm_tar_mean','mean')]).reset_index()\n",
    "features_mean.append(f); f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>custid</th>\n",
       "      <th>pc_nm_tar_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>36.444439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>37.502349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>37.968566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>37.189765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>38.718581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35962</th>\n",
       "      <td>49988</td>\n",
       "      <td>36.418850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35963</th>\n",
       "      <td>49990</td>\n",
       "      <td>35.994433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35964</th>\n",
       "      <td>49992</td>\n",
       "      <td>37.538058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35965</th>\n",
       "      <td>49993</td>\n",
       "      <td>37.722417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35966</th>\n",
       "      <td>49994</td>\n",
       "      <td>37.218751</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35967 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       custid  pc_nm_tar_mean\n",
       "0           0       36.444439\n",
       "1           2       37.502349\n",
       "2           3       37.968566\n",
       "3           4       37.189765\n",
       "4           5       38.718581\n",
       "...       ...             ...\n",
       "35962   49988       36.418850\n",
       "35963   49990       35.994433\n",
       "35964   49992       37.538058\n",
       "35965   49993       37.722417\n",
       "35966   49994       37.218751\n",
       "\n",
       "[35967 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = newfeat[['custid','pc_nm','age']].set_index('custid')\n",
    "f= f.groupby('pc_nm').agg('mean')\n",
    "f = f.to_dict()['age']\n",
    "data['pc_nm_target'] = data.pc_nm.apply(lambda x: f.get(x,0))\n",
    "f = data.groupby('custid')['pc_nm_target'].agg([('pc_nm_tar_mean','mean')]).reset_index()\n",
    "features_mean.append(f); f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>custid</th>\n",
       "      <th>part_nm_tar_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>37.824731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>38.022331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>37.952269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>38.056806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>39.415123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35962</th>\n",
       "      <td>49988</td>\n",
       "      <td>37.931375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35963</th>\n",
       "      <td>49990</td>\n",
       "      <td>36.931327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35964</th>\n",
       "      <td>49992</td>\n",
       "      <td>37.931375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35965</th>\n",
       "      <td>49993</td>\n",
       "      <td>36.870821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35966</th>\n",
       "      <td>49994</td>\n",
       "      <td>37.528839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35967 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       custid  part_nm_tar_mean\n",
       "0           0         37.824731\n",
       "1           2         38.022331\n",
       "2           3         37.952269\n",
       "3           4         38.056806\n",
       "4           5         39.415123\n",
       "...       ...               ...\n",
       "35962   49988         37.931375\n",
       "35963   49990         36.931327\n",
       "35964   49992         37.931375\n",
       "35965   49993         36.870821\n",
       "35966   49994         37.528839\n",
       "\n",
       "[35967 rows x 2 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = newfeat[['custid','part_nm','age']].set_index('custid')\n",
    "f= f.groupby('part_nm').agg('mean')\n",
    "f = f.to_dict()['age']\n",
    "data['part_nm_target'] = data.part_nm.apply(lambda x: f.get(x,0))\n",
    "f = data.groupby('custid')['part_nm_target'].agg([('part_nm_tar_mean','mean')]).reset_index()\n",
    "features_mean.append(f); f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>custid</th>\n",
       "      <th>buyer_nm_tar_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>36.575116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>37.687634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>38.078728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>36.520959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>38.202271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35962</th>\n",
       "      <td>49988</td>\n",
       "      <td>35.698535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35963</th>\n",
       "      <td>49990</td>\n",
       "      <td>35.893100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35964</th>\n",
       "      <td>49992</td>\n",
       "      <td>36.539181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35965</th>\n",
       "      <td>49993</td>\n",
       "      <td>37.999016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35966</th>\n",
       "      <td>49994</td>\n",
       "      <td>35.822532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35967 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       custid  buyer_nm_tar_mean\n",
       "0           0          36.575116\n",
       "1           2          37.687634\n",
       "2           3          38.078728\n",
       "3           4          36.520959\n",
       "4           5          38.202271\n",
       "...       ...                ...\n",
       "35962   49988          35.698535\n",
       "35963   49990          35.893100\n",
       "35964   49992          36.539181\n",
       "35965   49993          37.999016\n",
       "35966   49994          35.822532\n",
       "\n",
       "[35967 rows x 2 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = newfeat[['custid','buyer_nm','age']].set_index('custid')\n",
    "f= f.groupby('buyer_nm').agg('mean')\n",
    "f = f.to_dict()['age']\n",
    "data['buyer_nm_target'] = data.buyer_nm.apply(lambda x: f.get(x,0))\n",
    "f = data.groupby('custid')['buyer_nm_target'].agg([('buyer_nm_tar_mean','mean')]).reset_index()\n",
    "features_mean.append(f); f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 날짜 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sales_year']=data['sales_month'].apply(lambda x: 2018 if x >12 else 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 월 정리\n",
    "data['sales_month']=data['sales_month'].apply(lambda x: x-12 if x >12 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주말\n",
    "data['weekend'] = data['sales_dayofweek'].apply(lambda x: 1 if (x=='토' or x=='일') else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sales_year']=data['sales_year'].astype('str')\n",
    "data['sales_month']=data['sales_month'].astype('str')\n",
    "data['sales_day']=data['sales_day'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date_time']=data['sales_year']+'-'+data['sales_month']+'-'+data['sales_day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sales_year']=data['sales_year'].astype('int')\n",
    "data['sales_month']=data['sales_month'].astype('int')\n",
    "data['sales_day']=data['sales_day'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date_time'] = pd.to_datetime(data['date_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=data.groupby(['custid'])['date_time'].diff()\n",
    "a=a.reset_index().set_index(data.custid)\n",
    "a=a.reset_index()\n",
    "a.set_index('index',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date_time_diff']=a.date_time\n",
    "dtdt=data.groupby(['custid','date_time'])['date_time_diff'].sum().reset_index()\n",
    "dtdt.date_time_diff=dtdt.date_time_diff.astype('str')\n",
    "dtdt.date_time_diff=dtdt.date_time_diff.apply(lambda x: x.split(' ')[0])\n",
    "dtdt.date_time_diff=dtdt.date_time_diff.astype('int')\n",
    "dtdt_mean=dtdt.groupby('custid')['date_time_diff'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 쇼핑시간"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab=data.groupby(['custid','sales_day','sales_month'])['sales_time'].agg([('sales_time_min', 'min')]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc=data.groupby(['custid','sales_day','sales_month'])['sales_time'].agg([('sales_time_max', 'max')]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각의 자릿수가 두자리이면 문자 갯수가 4이기 때문에 다르게 인식하는 것\n",
    "ab['time_min']=ab['sales_time_min'].apply(lambda x: int(str(x)[:2]) if len(str(x)) == 4 else int(str(x)[:1]))\n",
    "ab['minute_min']=ab['sales_time_min'].apply(lambda x: int(str(x)[2:]) if len(str(x)) == 4 else int(str(x)[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab['time_max']=abc['sales_time_max'].apply(lambda x: int(str(x)[:2]) if len(str(x)) == 4 else int(str(x)[:1]))\n",
    "ab['minute_max']=abc['sales_time_max'].apply(lambda x: int(str(x)[2:]) if len(str(x)) == 4 else int(str(x)[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab['shop_time']=(ab['time_max']-ab['time_min'])*60+(ab['minute_max']-ab['minute_min'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_time=ab.groupby('custid')['shop_time'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### customer_info = brd_nm + corner_nm + pc_nm + part_nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구매 정보 관련\n",
    "trn['customer_info'] = trn['brd_nm'].astype(str) + '_' + trn['corner_nm'].astype(str) + '_' + trn['pc_nm'].astype(str) + '_' + trn['part_nm'].astype(str)\n",
    "\n",
    "tst['customer_info'] = tst['brd_nm'].astype(str) + '_' + tst['corner_nm'].astype(str) + '_' + tst['pc_nm'].astype(str) + '_' + tst['part_nm'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6065"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn['customer_info'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5546"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst['customer_info'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "train_data = list(trn.groupby('custid')['customer_info'].unique())\n",
    "test_data = list(tst.groupby('custid')['customer_info'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversample(x, n,seed=0):\n",
    "    lst = []\n",
    "    for i in x:\n",
    "        tmp = []\n",
    "        np.random.seed(seed)\n",
    "        for j in range(n):\n",
    "            random.shuffle(i)\n",
    "            tmp += list(i)\n",
    "            lst.append(tmp)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_input = oversample(train_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "w2v = word2vec.Word2Vec(sentences = w2v_input, size = 100, window = 5, min_count = 1, sg = 1,seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 21587/21587 [00:01<00:00, 11473.71it/s]\n"
     ]
    }
   ],
   "source": [
    "train_mean_vector = []\n",
    "for words in tqdm(train_data):\n",
    "    tmp = np.zeros(100)\n",
    "    cnt = 0\n",
    "    for word in words:\n",
    "        try:\n",
    "            tmp += w2v[word]\n",
    "            cnt += 1\n",
    "        except:\n",
    "            pass\n",
    "    tmp /= cnt\n",
    "    train_mean_vector.append(tmp)\n",
    "train_mean_vector = np.array(train_mean_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 14380/14380 [00:01<00:00, 11249.25it/s]\n"
     ]
    }
   ],
   "source": [
    "test_mean_vector = []\n",
    "for words in tqdm(test_data):\n",
    "    tmp = np.zeros(100)\n",
    "    cnt = 0\n",
    "    for word in words:\n",
    "        try:\n",
    "            tmp += w2v[word]\n",
    "            cnt += 1\n",
    "        except:\n",
    "            pass\n",
    "    tmp /= cnt\n",
    "    test_mean_vector.append(tmp)\n",
    "test_mean_vector = np.array(test_mean_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean_vector = pd.DataFrame(train_mean_vector)\n",
    "test_mean_vector = pd.DataFrame(test_mean_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 100)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mean_vector.isnull().sum().sum(),test_mean_vector.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mean_vector.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "## save\n",
    "with open('train_mean_vector.pickle', 'wb') as f:\n",
    "    pickle.dump(train_mean_vector, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "## save\n",
    "with open('test_mean_vector.pickle', 'wb') as f:\n",
    "    pickle.dump(test_mean_vector, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### customer_info_time = str_nm + sales_dayofweek + season_sales+time_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 계절별 구매 빈도 추가\n",
    "def f1(x):\n",
    "    if 3 <= x <= 5 :\n",
    "        return('spring_sales')\n",
    "    elif 6 <= x <= 8 :\n",
    "        return('summer_sales')\n",
    "    elif 9 <= x <= 11 :\n",
    "        return('fall_sales')\n",
    "    else :\n",
    "        return('winter_sales')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['season_sales']=data['sales_month'].apply(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=data[:625083]\n",
    "test=data[625084:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간대 구분 (회사원 기준)\n",
    "train['time_split'] = trn['sales_time'].apply(lambda x : 'MORNINGTIME' if (x>=700) and (x<=1130)\n",
    "                                               else 'LUNCHTIME' if (x>1130) and (x<=1300)\n",
    "                                               else 'AFTERNOON' if (x>1300) and (x<=1900)\n",
    "                                               else 'RESTTIME')\n",
    "\n",
    "test['time_split'] = tst['sales_time'].apply(lambda x : 'MORNINGTIME' if (x>=700) and (x<=1130)\n",
    "                                               else 'LUNCHTIME' if (x>1130) and (x<=1300)\n",
    "                                               else 'AFTERNOON' if (x>1300) and (x<=1900)\n",
    "                                               else 'RESTTIME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구매시간 정보\n",
    "train['customer_info_time'] = train['sales_dayofweek'].astype(str) + '_' + train['season_sales'].astype(str) + '_' + train['time_split'].astype(str)\n",
    "\n",
    "test['customer_info_time'] = test['sales_dayofweek'].astype(str) + '_' + test['season_sales'].astype(str) + '_' + test['time_split'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "train_data = list(train.groupby('custid')['customer_info_time'].unique())\n",
    "test_data = list(test.groupby('custid')['customer_info_time'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_input = oversample(train_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "w2v = word2vec.Word2Vec(sentences = w2v_input, size = 100, window = 5, min_count = 1, sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 21587/21587 [00:01<00:00, 17880.17it/s]\n"
     ]
    }
   ],
   "source": [
    "train_mean_vector1 = []\n",
    "for words in tqdm(train_data):\n",
    "    tmp = np.zeros(100)\n",
    "    cnt = 0\n",
    "    for word in words:\n",
    "        try:\n",
    "            tmp += w2v[word]\n",
    "            cnt += 1\n",
    "        except:\n",
    "            pass\n",
    "    tmp /= cnt\n",
    "    train_mean_vector1.append(tmp)\n",
    "train_mean_vector1 = np.array(train_mean_vector1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 14380/14380 [00:00<00:00, 17790.12it/s]\n"
     ]
    }
   ],
   "source": [
    "test_mean_vector1 = []\n",
    "for words in tqdm(test_data):\n",
    "    tmp = np.zeros(100)\n",
    "    cnt = 0\n",
    "    for word in words:\n",
    "        try:\n",
    "            tmp += w2v[word]\n",
    "            cnt += 1\n",
    "        except:\n",
    "            pass\n",
    "    tmp /= cnt\n",
    "    test_mean_vector1.append(tmp)\n",
    "test_mean_vector1 = np.array(test_mean_vector1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean_vector1 = pd.DataFrame(train_mean_vector1)\n",
    "test_mean_vector1 = pd.DataFrame(test_mean_vector1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "## save\n",
    "with open('train_mean_vector1.pickle', 'wb') as f:\n",
    "    pickle.dump(train_mean_vector1, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "## save\n",
    "with open('test_mean_vector1.pickle', 'wb') as f:\n",
    "    pickle.dump(test_mean_vector1, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### goodcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "# income_occyp_type, income_age_type, edu_occyp_type, edu_income_type, family_income_type\n",
    "\n",
    "import random\n",
    "\n",
    "trn2=trn.reset_index().drop('index',axis=1).copy()\n",
    "trn2=trn2.reset_index()\n",
    "trn2['goodcd']=trn2['goodcd'].astype('str')\n",
    "train_data=list(trn2.groupby('custid')['goodcd'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversample(x, n):\n",
    "    lst = []\n",
    "    for i in x:\n",
    "        tmp = []\n",
    "        for j in range(n):\n",
    "            random.shuffle(i)\n",
    "            tmp += list(i)\n",
    "            lst.append(tmp)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_input = oversample(train_data, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "w2v = word2vec.Word2Vec(sentences = w2v_input, size = 100, window = 3, min_count = 1, sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 21587/21587 [00:01<00:00, 12080.00it/s]\n"
     ]
    }
   ],
   "source": [
    "train_mean_vector = []\n",
    "for words in tqdm(train_data):\n",
    "    tmp = np.zeros(100)\n",
    "    cnt = 0\n",
    "    for word in words:\n",
    "        try:\n",
    "            tmp += w2v[word]\n",
    "            cnt += 1\n",
    "        except:\n",
    "            pass\n",
    "    tmp /= cnt\n",
    "    train_mean_vector.append(tmp)\n",
    "train_mean_vector = np.array(train_mean_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.144323</td>\n",
       "      <td>-0.268593</td>\n",
       "      <td>0.197830</td>\n",
       "      <td>-0.034078</td>\n",
       "      <td>-0.010634</td>\n",
       "      <td>-0.072571</td>\n",
       "      <td>-0.155758</td>\n",
       "      <td>0.024709</td>\n",
       "      <td>-0.004502</td>\n",
       "      <td>-0.118695</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022690</td>\n",
       "      <td>0.124745</td>\n",
       "      <td>0.036699</td>\n",
       "      <td>0.056032</td>\n",
       "      <td>0.276451</td>\n",
       "      <td>0.269137</td>\n",
       "      <td>0.011492</td>\n",
       "      <td>0.152564</td>\n",
       "      <td>-0.098052</td>\n",
       "      <td>0.039434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.098021</td>\n",
       "      <td>-0.395419</td>\n",
       "      <td>0.152359</td>\n",
       "      <td>-0.030305</td>\n",
       "      <td>-0.231516</td>\n",
       "      <td>-0.116648</td>\n",
       "      <td>0.003360</td>\n",
       "      <td>-0.252212</td>\n",
       "      <td>0.022645</td>\n",
       "      <td>-0.102189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004354</td>\n",
       "      <td>0.056690</td>\n",
       "      <td>0.000893</td>\n",
       "      <td>0.154616</td>\n",
       "      <td>0.271934</td>\n",
       "      <td>0.196364</td>\n",
       "      <td>-0.260559</td>\n",
       "      <td>0.236958</td>\n",
       "      <td>-0.080918</td>\n",
       "      <td>0.284316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.077051</td>\n",
       "      <td>-0.259584</td>\n",
       "      <td>0.277543</td>\n",
       "      <td>0.152713</td>\n",
       "      <td>0.250707</td>\n",
       "      <td>0.074979</td>\n",
       "      <td>0.097563</td>\n",
       "      <td>-0.147708</td>\n",
       "      <td>0.061654</td>\n",
       "      <td>-0.120720</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.073173</td>\n",
       "      <td>-0.001815</td>\n",
       "      <td>0.019788</td>\n",
       "      <td>0.132180</td>\n",
       "      <td>0.360719</td>\n",
       "      <td>0.214175</td>\n",
       "      <td>-0.097149</td>\n",
       "      <td>0.110110</td>\n",
       "      <td>-0.034951</td>\n",
       "      <td>0.074060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.018981</td>\n",
       "      <td>-0.279253</td>\n",
       "      <td>0.403917</td>\n",
       "      <td>0.218487</td>\n",
       "      <td>-0.159937</td>\n",
       "      <td>-0.061593</td>\n",
       "      <td>-0.060668</td>\n",
       "      <td>-0.040885</td>\n",
       "      <td>0.214933</td>\n",
       "      <td>-0.074717</td>\n",
       "      <td>...</td>\n",
       "      <td>0.225626</td>\n",
       "      <td>-0.050878</td>\n",
       "      <td>-0.088012</td>\n",
       "      <td>0.676289</td>\n",
       "      <td>0.240453</td>\n",
       "      <td>0.448307</td>\n",
       "      <td>-0.001719</td>\n",
       "      <td>-0.156652</td>\n",
       "      <td>0.359539</td>\n",
       "      <td>-0.122622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.006406</td>\n",
       "      <td>-0.147912</td>\n",
       "      <td>0.194968</td>\n",
       "      <td>-0.029545</td>\n",
       "      <td>0.029772</td>\n",
       "      <td>0.020132</td>\n",
       "      <td>-0.163050</td>\n",
       "      <td>-0.158720</td>\n",
       "      <td>-0.002155</td>\n",
       "      <td>-0.055020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015638</td>\n",
       "      <td>0.005054</td>\n",
       "      <td>0.095058</td>\n",
       "      <td>0.068094</td>\n",
       "      <td>0.216768</td>\n",
       "      <td>0.263295</td>\n",
       "      <td>-0.111932</td>\n",
       "      <td>-0.039474</td>\n",
       "      <td>-0.031801</td>\n",
       "      <td>0.107247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21582</th>\n",
       "      <td>-0.047208</td>\n",
       "      <td>-0.390859</td>\n",
       "      <td>0.099789</td>\n",
       "      <td>0.036606</td>\n",
       "      <td>0.005997</td>\n",
       "      <td>-0.249147</td>\n",
       "      <td>-0.232567</td>\n",
       "      <td>0.045683</td>\n",
       "      <td>0.069805</td>\n",
       "      <td>-0.080205</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.100574</td>\n",
       "      <td>-0.001459</td>\n",
       "      <td>-0.077207</td>\n",
       "      <td>0.162558</td>\n",
       "      <td>0.392169</td>\n",
       "      <td>0.276467</td>\n",
       "      <td>-0.034129</td>\n",
       "      <td>0.087867</td>\n",
       "      <td>-0.050174</td>\n",
       "      <td>0.097632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21583</th>\n",
       "      <td>-0.208979</td>\n",
       "      <td>-0.302581</td>\n",
       "      <td>0.082338</td>\n",
       "      <td>-0.019715</td>\n",
       "      <td>-0.084864</td>\n",
       "      <td>0.059955</td>\n",
       "      <td>-0.124320</td>\n",
       "      <td>-0.031594</td>\n",
       "      <td>0.043823</td>\n",
       "      <td>0.018893</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040590</td>\n",
       "      <td>0.017645</td>\n",
       "      <td>-0.195209</td>\n",
       "      <td>0.310201</td>\n",
       "      <td>0.265753</td>\n",
       "      <td>0.232559</td>\n",
       "      <td>-0.208179</td>\n",
       "      <td>0.013349</td>\n",
       "      <td>-0.010042</td>\n",
       "      <td>0.141635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21584</th>\n",
       "      <td>-0.027314</td>\n",
       "      <td>-0.186412</td>\n",
       "      <td>0.036101</td>\n",
       "      <td>-0.142100</td>\n",
       "      <td>-0.014941</td>\n",
       "      <td>-0.246432</td>\n",
       "      <td>-0.051719</td>\n",
       "      <td>0.035959</td>\n",
       "      <td>0.185666</td>\n",
       "      <td>-0.067376</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019714</td>\n",
       "      <td>0.053179</td>\n",
       "      <td>-0.240394</td>\n",
       "      <td>0.152787</td>\n",
       "      <td>0.462484</td>\n",
       "      <td>0.291054</td>\n",
       "      <td>-0.170619</td>\n",
       "      <td>0.082360</td>\n",
       "      <td>0.050413</td>\n",
       "      <td>0.209269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21585</th>\n",
       "      <td>-0.125583</td>\n",
       "      <td>-0.143443</td>\n",
       "      <td>0.276443</td>\n",
       "      <td>0.187808</td>\n",
       "      <td>0.068731</td>\n",
       "      <td>0.002162</td>\n",
       "      <td>-0.131727</td>\n",
       "      <td>-0.101011</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>-0.059716</td>\n",
       "      <td>...</td>\n",
       "      <td>0.105391</td>\n",
       "      <td>0.084607</td>\n",
       "      <td>-0.049633</td>\n",
       "      <td>0.354610</td>\n",
       "      <td>0.232825</td>\n",
       "      <td>0.260237</td>\n",
       "      <td>-0.256420</td>\n",
       "      <td>0.166660</td>\n",
       "      <td>-0.066059</td>\n",
       "      <td>0.146053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21586</th>\n",
       "      <td>-0.295663</td>\n",
       "      <td>-0.304250</td>\n",
       "      <td>0.082317</td>\n",
       "      <td>-0.124228</td>\n",
       "      <td>0.146597</td>\n",
       "      <td>0.161468</td>\n",
       "      <td>0.146863</td>\n",
       "      <td>-0.352904</td>\n",
       "      <td>0.133646</td>\n",
       "      <td>-0.161941</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005737</td>\n",
       "      <td>0.045130</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>0.334347</td>\n",
       "      <td>0.072226</td>\n",
       "      <td>0.173440</td>\n",
       "      <td>-0.106688</td>\n",
       "      <td>-0.086150</td>\n",
       "      <td>0.156668</td>\n",
       "      <td>-0.097954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21587 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6   \\\n",
       "0     -0.144323 -0.268593  0.197830 -0.034078 -0.010634 -0.072571 -0.155758   \n",
       "1     -0.098021 -0.395419  0.152359 -0.030305 -0.231516 -0.116648  0.003360   \n",
       "2     -0.077051 -0.259584  0.277543  0.152713  0.250707  0.074979  0.097563   \n",
       "3     -0.018981 -0.279253  0.403917  0.218487 -0.159937 -0.061593 -0.060668   \n",
       "4     -0.006406 -0.147912  0.194968 -0.029545  0.029772  0.020132 -0.163050   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "21582 -0.047208 -0.390859  0.099789  0.036606  0.005997 -0.249147 -0.232567   \n",
       "21583 -0.208979 -0.302581  0.082338 -0.019715 -0.084864  0.059955 -0.124320   \n",
       "21584 -0.027314 -0.186412  0.036101 -0.142100 -0.014941 -0.246432 -0.051719   \n",
       "21585 -0.125583 -0.143443  0.276443  0.187808  0.068731  0.002162 -0.131727   \n",
       "21586 -0.295663 -0.304250  0.082317 -0.124228  0.146597  0.161468  0.146863   \n",
       "\n",
       "             7         8         9   ...        90        91        92  \\\n",
       "0      0.024709 -0.004502 -0.118695  ... -0.022690  0.124745  0.036699   \n",
       "1     -0.252212  0.022645 -0.102189  ...  0.004354  0.056690  0.000893   \n",
       "2     -0.147708  0.061654 -0.120720  ... -0.073173 -0.001815  0.019788   \n",
       "3     -0.040885  0.214933 -0.074717  ...  0.225626 -0.050878 -0.088012   \n",
       "4     -0.158720 -0.002155 -0.055020  ...  0.015638  0.005054  0.095058   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "21582  0.045683  0.069805 -0.080205  ... -0.100574 -0.001459 -0.077207   \n",
       "21583 -0.031594  0.043823  0.018893  ...  0.040590  0.017645 -0.195209   \n",
       "21584  0.035959  0.185666 -0.067376  ...  0.019714  0.053179 -0.240394   \n",
       "21585 -0.101011  0.186247 -0.059716  ...  0.105391  0.084607 -0.049633   \n",
       "21586 -0.352904  0.133646 -0.161941  ... -0.005737  0.045130  0.000198   \n",
       "\n",
       "             93        94        95        96        97        98        99  \n",
       "0      0.056032  0.276451  0.269137  0.011492  0.152564 -0.098052  0.039434  \n",
       "1      0.154616  0.271934  0.196364 -0.260559  0.236958 -0.080918  0.284316  \n",
       "2      0.132180  0.360719  0.214175 -0.097149  0.110110 -0.034951  0.074060  \n",
       "3      0.676289  0.240453  0.448307 -0.001719 -0.156652  0.359539 -0.122622  \n",
       "4      0.068094  0.216768  0.263295 -0.111932 -0.039474 -0.031801  0.107247  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "21582  0.162558  0.392169  0.276467 -0.034129  0.087867 -0.050174  0.097632  \n",
       "21583  0.310201  0.265753  0.232559 -0.208179  0.013349 -0.010042  0.141635  \n",
       "21584  0.152787  0.462484  0.291054 -0.170619  0.082360  0.050413  0.209269  \n",
       "21585  0.354610  0.232825  0.260237 -0.256420  0.166660 -0.066059  0.146053  \n",
       "21586  0.334347  0.072226  0.173440 -0.106688 -0.086150  0.156668 -0.097954  \n",
       "\n",
       "[21587 rows x 100 columns]"
      ]
     },
     "execution_count": 589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mean_vector = pd.DataFrame(train_mean_vector)\n",
    "train_mean_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "# income_occyp_type, income_age_type, edu_occyp_type, edu_income_type, family_income_type\n",
    "\n",
    "import random\n",
    "\n",
    "tst2=tst.reset_index().drop('index',axis=1).copy()\n",
    "tst2=tst2.reset_index()\n",
    "tst2['goodcd']=tst2['goodcd'].astype('str')\n",
    "test_data=list(tst2.groupby('custid')['goodcd'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 14380/14380 [00:01<00:00, 11884.44it/s]\n"
     ]
    }
   ],
   "source": [
    "test_mean_vector = []\n",
    "for words in tqdm(test_data):\n",
    "    tmp = np.zeros(100)\n",
    "    cnt = 0\n",
    "    for word in words:\n",
    "        try:\n",
    "            tmp += w2v[word]\n",
    "            cnt += 1\n",
    "        except:\n",
    "            pass\n",
    "    tmp /= cnt\n",
    "    test_mean_vector.append(tmp)\n",
    "test_mean_vector = np.array(test_mean_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.057559</td>\n",
       "      <td>-0.296036</td>\n",
       "      <td>0.114920</td>\n",
       "      <td>-0.076338</td>\n",
       "      <td>-0.164954</td>\n",
       "      <td>-0.168961</td>\n",
       "      <td>-0.260377</td>\n",
       "      <td>-0.166786</td>\n",
       "      <td>-0.047536</td>\n",
       "      <td>-0.068296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074330</td>\n",
       "      <td>0.016983</td>\n",
       "      <td>0.075861</td>\n",
       "      <td>0.028129</td>\n",
       "      <td>0.223909</td>\n",
       "      <td>0.196585</td>\n",
       "      <td>0.057957</td>\n",
       "      <td>0.157687</td>\n",
       "      <td>-0.186967</td>\n",
       "      <td>0.081998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.137682</td>\n",
       "      <td>-0.237492</td>\n",
       "      <td>0.163749</td>\n",
       "      <td>0.076731</td>\n",
       "      <td>0.130093</td>\n",
       "      <td>-0.093459</td>\n",
       "      <td>-0.191641</td>\n",
       "      <td>0.002772</td>\n",
       "      <td>0.130364</td>\n",
       "      <td>-0.098610</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.077211</td>\n",
       "      <td>-0.040014</td>\n",
       "      <td>-0.005195</td>\n",
       "      <td>0.178763</td>\n",
       "      <td>0.308321</td>\n",
       "      <td>0.248290</td>\n",
       "      <td>-0.129177</td>\n",
       "      <td>0.074483</td>\n",
       "      <td>-0.113529</td>\n",
       "      <td>0.058980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.096139</td>\n",
       "      <td>-0.252574</td>\n",
       "      <td>0.245184</td>\n",
       "      <td>-0.031221</td>\n",
       "      <td>-0.059827</td>\n",
       "      <td>0.201953</td>\n",
       "      <td>-0.209133</td>\n",
       "      <td>-0.000823</td>\n",
       "      <td>0.178194</td>\n",
       "      <td>-0.064730</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061576</td>\n",
       "      <td>-0.076105</td>\n",
       "      <td>-0.088217</td>\n",
       "      <td>0.242829</td>\n",
       "      <td>0.072702</td>\n",
       "      <td>0.099506</td>\n",
       "      <td>-0.145247</td>\n",
       "      <td>0.016494</td>\n",
       "      <td>0.138106</td>\n",
       "      <td>0.208594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.056233</td>\n",
       "      <td>-0.152458</td>\n",
       "      <td>0.087679</td>\n",
       "      <td>-0.047790</td>\n",
       "      <td>0.124579</td>\n",
       "      <td>0.045336</td>\n",
       "      <td>-0.156751</td>\n",
       "      <td>0.044902</td>\n",
       "      <td>0.053946</td>\n",
       "      <td>-0.111123</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024557</td>\n",
       "      <td>-0.085635</td>\n",
       "      <td>-0.213758</td>\n",
       "      <td>0.116544</td>\n",
       "      <td>0.173242</td>\n",
       "      <td>0.149649</td>\n",
       "      <td>-0.306621</td>\n",
       "      <td>0.086035</td>\n",
       "      <td>0.073079</td>\n",
       "      <td>0.100721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.171590</td>\n",
       "      <td>-0.276474</td>\n",
       "      <td>0.015301</td>\n",
       "      <td>0.053183</td>\n",
       "      <td>0.171842</td>\n",
       "      <td>0.101730</td>\n",
       "      <td>-0.173377</td>\n",
       "      <td>-0.062586</td>\n",
       "      <td>0.031539</td>\n",
       "      <td>-0.111422</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081755</td>\n",
       "      <td>-0.114964</td>\n",
       "      <td>-0.247074</td>\n",
       "      <td>0.248575</td>\n",
       "      <td>0.104175</td>\n",
       "      <td>0.293454</td>\n",
       "      <td>-0.267686</td>\n",
       "      <td>0.198240</td>\n",
       "      <td>-0.109773</td>\n",
       "      <td>-0.001764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14375</th>\n",
       "      <td>-0.168402</td>\n",
       "      <td>-0.311017</td>\n",
       "      <td>0.000747</td>\n",
       "      <td>-0.137941</td>\n",
       "      <td>0.118846</td>\n",
       "      <td>-0.234939</td>\n",
       "      <td>-0.325825</td>\n",
       "      <td>-0.137096</td>\n",
       "      <td>0.150449</td>\n",
       "      <td>-0.020354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052506</td>\n",
       "      <td>-0.240179</td>\n",
       "      <td>0.075712</td>\n",
       "      <td>0.027874</td>\n",
       "      <td>0.344606</td>\n",
       "      <td>0.200713</td>\n",
       "      <td>-0.157998</td>\n",
       "      <td>0.174613</td>\n",
       "      <td>-0.108493</td>\n",
       "      <td>0.120855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14376</th>\n",
       "      <td>-0.034267</td>\n",
       "      <td>-0.230976</td>\n",
       "      <td>0.106911</td>\n",
       "      <td>-0.203869</td>\n",
       "      <td>0.153405</td>\n",
       "      <td>0.116296</td>\n",
       "      <td>-0.047896</td>\n",
       "      <td>-0.111250</td>\n",
       "      <td>0.037653</td>\n",
       "      <td>-0.139375</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081653</td>\n",
       "      <td>0.095861</td>\n",
       "      <td>-0.145100</td>\n",
       "      <td>0.101180</td>\n",
       "      <td>0.128748</td>\n",
       "      <td>0.199945</td>\n",
       "      <td>-0.150141</td>\n",
       "      <td>0.143817</td>\n",
       "      <td>-0.106212</td>\n",
       "      <td>-0.008779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14377</th>\n",
       "      <td>-0.170406</td>\n",
       "      <td>-0.292972</td>\n",
       "      <td>-0.077190</td>\n",
       "      <td>-0.182222</td>\n",
       "      <td>-0.035382</td>\n",
       "      <td>-0.327804</td>\n",
       "      <td>-0.149496</td>\n",
       "      <td>0.053931</td>\n",
       "      <td>-0.133849</td>\n",
       "      <td>0.010821</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054716</td>\n",
       "      <td>0.118709</td>\n",
       "      <td>0.154340</td>\n",
       "      <td>0.186622</td>\n",
       "      <td>0.339342</td>\n",
       "      <td>0.393350</td>\n",
       "      <td>0.004752</td>\n",
       "      <td>-0.031219</td>\n",
       "      <td>0.061336</td>\n",
       "      <td>0.102499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14378</th>\n",
       "      <td>-0.080319</td>\n",
       "      <td>-0.147048</td>\n",
       "      <td>-0.038252</td>\n",
       "      <td>0.064179</td>\n",
       "      <td>0.098606</td>\n",
       "      <td>0.259903</td>\n",
       "      <td>-0.116665</td>\n",
       "      <td>-0.252119</td>\n",
       "      <td>-0.041359</td>\n",
       "      <td>-0.045233</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020405</td>\n",
       "      <td>-0.284774</td>\n",
       "      <td>-0.007716</td>\n",
       "      <td>-0.099579</td>\n",
       "      <td>0.046672</td>\n",
       "      <td>0.149953</td>\n",
       "      <td>-0.082328</td>\n",
       "      <td>0.034290</td>\n",
       "      <td>0.053548</td>\n",
       "      <td>-0.000764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14379</th>\n",
       "      <td>0.217033</td>\n",
       "      <td>-0.209253</td>\n",
       "      <td>-0.061514</td>\n",
       "      <td>-0.307617</td>\n",
       "      <td>0.045741</td>\n",
       "      <td>0.105171</td>\n",
       "      <td>-0.404014</td>\n",
       "      <td>-0.102746</td>\n",
       "      <td>0.079340</td>\n",
       "      <td>0.169499</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.126940</td>\n",
       "      <td>0.029686</td>\n",
       "      <td>0.333391</td>\n",
       "      <td>-0.019673</td>\n",
       "      <td>0.431627</td>\n",
       "      <td>-0.101592</td>\n",
       "      <td>-0.110796</td>\n",
       "      <td>0.120800</td>\n",
       "      <td>-0.205311</td>\n",
       "      <td>-0.290458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14380 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6   \\\n",
       "0     -0.057559 -0.296036  0.114920 -0.076338 -0.164954 -0.168961 -0.260377   \n",
       "1     -0.137682 -0.237492  0.163749  0.076731  0.130093 -0.093459 -0.191641   \n",
       "2     -0.096139 -0.252574  0.245184 -0.031221 -0.059827  0.201953 -0.209133   \n",
       "3     -0.056233 -0.152458  0.087679 -0.047790  0.124579  0.045336 -0.156751   \n",
       "4     -0.171590 -0.276474  0.015301  0.053183  0.171842  0.101730 -0.173377   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "14375 -0.168402 -0.311017  0.000747 -0.137941  0.118846 -0.234939 -0.325825   \n",
       "14376 -0.034267 -0.230976  0.106911 -0.203869  0.153405  0.116296 -0.047896   \n",
       "14377 -0.170406 -0.292972 -0.077190 -0.182222 -0.035382 -0.327804 -0.149496   \n",
       "14378 -0.080319 -0.147048 -0.038252  0.064179  0.098606  0.259903 -0.116665   \n",
       "14379  0.217033 -0.209253 -0.061514 -0.307617  0.045741  0.105171 -0.404014   \n",
       "\n",
       "             7         8         9   ...        90        91        92  \\\n",
       "0     -0.166786 -0.047536 -0.068296  ...  0.074330  0.016983  0.075861   \n",
       "1      0.002772  0.130364 -0.098610  ... -0.077211 -0.040014 -0.005195   \n",
       "2     -0.000823  0.178194 -0.064730  ...  0.061576 -0.076105 -0.088217   \n",
       "3      0.044902  0.053946 -0.111123  ...  0.024557 -0.085635 -0.213758   \n",
       "4     -0.062586  0.031539 -0.111422  ...  0.081755 -0.114964 -0.247074   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "14375 -0.137096  0.150449 -0.020354  ...  0.052506 -0.240179  0.075712   \n",
       "14376 -0.111250  0.037653 -0.139375  ... -0.081653  0.095861 -0.145100   \n",
       "14377  0.053931 -0.133849  0.010821  ... -0.054716  0.118709  0.154340   \n",
       "14378 -0.252119 -0.041359 -0.045233  ...  0.020405 -0.284774 -0.007716   \n",
       "14379 -0.102746  0.079340  0.169499  ... -0.126940  0.029686  0.333391   \n",
       "\n",
       "             93        94        95        96        97        98        99  \n",
       "0      0.028129  0.223909  0.196585  0.057957  0.157687 -0.186967  0.081998  \n",
       "1      0.178763  0.308321  0.248290 -0.129177  0.074483 -0.113529  0.058980  \n",
       "2      0.242829  0.072702  0.099506 -0.145247  0.016494  0.138106  0.208594  \n",
       "3      0.116544  0.173242  0.149649 -0.306621  0.086035  0.073079  0.100721  \n",
       "4      0.248575  0.104175  0.293454 -0.267686  0.198240 -0.109773 -0.001764  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "14375  0.027874  0.344606  0.200713 -0.157998  0.174613 -0.108493  0.120855  \n",
       "14376  0.101180  0.128748  0.199945 -0.150141  0.143817 -0.106212 -0.008779  \n",
       "14377  0.186622  0.339342  0.393350  0.004752 -0.031219  0.061336  0.102499  \n",
       "14378 -0.099579  0.046672  0.149953 -0.082328  0.034290  0.053548 -0.000764  \n",
       "14379 -0.019673  0.431627 -0.101592 -0.110796  0.120800 -0.205311 -0.290458  \n",
       "\n",
       "[14380 rows x 100 columns]"
      ]
     },
     "execution_count": 592,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mean_vector = pd.DataFrame(test_mean_vector)\n",
    "test_mean_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "## save\n",
    "with open('train_mean_vector_good.pickle', 'wb') as f:\n",
    "    pickle.dump(train_mean_vector, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "## save\n",
    "with open('test_mean_vector_good.pickle', 'wb') as f:\n",
    "    pickle.dump(test_mean_vector, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### brd_nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# income_occyp_type, income_age_type, edu_occyp_type, edu_income_type, family_income_type\n",
    "\n",
    "import random\n",
    "\n",
    "trn2=trn.reset_index().drop('index',axis=1).copy()\n",
    "trn2=trn2.reset_index()\n",
    "train_data=list(trn2.groupby('custid')['brd_nm'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversample(x, n):\n",
    "    lst = []\n",
    "    for i in x:\n",
    "        tmp = []\n",
    "        for j in range(n):\n",
    "            random.shuffle(i)\n",
    "            tmp += list(i)\n",
    "            lst.append(tmp)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_input = oversample(train_data, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "w2v = word2vec.Word2Vec(sentences = w2v_input, size = 100, window = 3, min_count = 1, sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 21587/21587 [00:01<00:00, 13817.00it/s]\n"
     ]
    }
   ],
   "source": [
    "train_mean_vector_2 = []\n",
    "for words in tqdm(train_data):\n",
    "    tmp = np.zeros(100)\n",
    "    cnt = 0\n",
    "    for word in words:\n",
    "        try:\n",
    "            tmp += w2v[word]\n",
    "            cnt += 1\n",
    "        except:\n",
    "            pass\n",
    "    tmp /= cnt\n",
    "    train_mean_vector_2.append(tmp)\n",
    "train_mean_vector_2 = np.array(train_mean_vector_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.099359</td>\n",
       "      <td>0.229848</td>\n",
       "      <td>0.001726</td>\n",
       "      <td>0.065618</td>\n",
       "      <td>-0.077882</td>\n",
       "      <td>0.141544</td>\n",
       "      <td>0.029603</td>\n",
       "      <td>0.131862</td>\n",
       "      <td>-0.104955</td>\n",
       "      <td>-0.206811</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.121027</td>\n",
       "      <td>0.149032</td>\n",
       "      <td>-0.143528</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.258949</td>\n",
       "      <td>-0.123598</td>\n",
       "      <td>-0.112917</td>\n",
       "      <td>0.118791</td>\n",
       "      <td>0.072575</td>\n",
       "      <td>-0.258159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.148271</td>\n",
       "      <td>0.168280</td>\n",
       "      <td>0.074423</td>\n",
       "      <td>-0.116408</td>\n",
       "      <td>-0.074596</td>\n",
       "      <td>0.164001</td>\n",
       "      <td>0.001238</td>\n",
       "      <td>0.089994</td>\n",
       "      <td>-0.035212</td>\n",
       "      <td>-0.181781</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.045649</td>\n",
       "      <td>0.178929</td>\n",
       "      <td>-0.097523</td>\n",
       "      <td>0.062516</td>\n",
       "      <td>0.158933</td>\n",
       "      <td>-0.129134</td>\n",
       "      <td>-0.052119</td>\n",
       "      <td>0.039353</td>\n",
       "      <td>0.006741</td>\n",
       "      <td>-0.270934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.209073</td>\n",
       "      <td>0.164122</td>\n",
       "      <td>-0.088313</td>\n",
       "      <td>0.026671</td>\n",
       "      <td>-0.099755</td>\n",
       "      <td>0.126713</td>\n",
       "      <td>0.088443</td>\n",
       "      <td>0.121720</td>\n",
       "      <td>-0.157667</td>\n",
       "      <td>-0.372412</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.067356</td>\n",
       "      <td>0.068995</td>\n",
       "      <td>-0.021621</td>\n",
       "      <td>0.138720</td>\n",
       "      <td>0.058073</td>\n",
       "      <td>-0.133802</td>\n",
       "      <td>-0.155317</td>\n",
       "      <td>-0.030555</td>\n",
       "      <td>-0.070853</td>\n",
       "      <td>-0.257945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.177481</td>\n",
       "      <td>0.377796</td>\n",
       "      <td>0.160337</td>\n",
       "      <td>-0.150246</td>\n",
       "      <td>-0.175173</td>\n",
       "      <td>0.289743</td>\n",
       "      <td>0.090508</td>\n",
       "      <td>0.314686</td>\n",
       "      <td>-0.299155</td>\n",
       "      <td>-0.010455</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.216118</td>\n",
       "      <td>0.011734</td>\n",
       "      <td>-0.159354</td>\n",
       "      <td>0.198609</td>\n",
       "      <td>0.043519</td>\n",
       "      <td>-0.381176</td>\n",
       "      <td>-0.071579</td>\n",
       "      <td>-0.004116</td>\n",
       "      <td>0.108335</td>\n",
       "      <td>0.077561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.188473</td>\n",
       "      <td>0.112612</td>\n",
       "      <td>0.000741</td>\n",
       "      <td>-0.003990</td>\n",
       "      <td>-0.086077</td>\n",
       "      <td>0.120941</td>\n",
       "      <td>0.107368</td>\n",
       "      <td>0.057337</td>\n",
       "      <td>-0.076843</td>\n",
       "      <td>-0.222488</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.045428</td>\n",
       "      <td>0.140131</td>\n",
       "      <td>-0.083319</td>\n",
       "      <td>-0.001670</td>\n",
       "      <td>0.101014</td>\n",
       "      <td>-0.217243</td>\n",
       "      <td>-0.080381</td>\n",
       "      <td>0.106238</td>\n",
       "      <td>0.016119</td>\n",
       "      <td>-0.262751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21582</th>\n",
       "      <td>-0.142881</td>\n",
       "      <td>0.171760</td>\n",
       "      <td>0.051145</td>\n",
       "      <td>0.146917</td>\n",
       "      <td>-0.034540</td>\n",
       "      <td>0.155963</td>\n",
       "      <td>0.084235</td>\n",
       "      <td>0.122642</td>\n",
       "      <td>-0.115169</td>\n",
       "      <td>-0.203340</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.066174</td>\n",
       "      <td>0.070993</td>\n",
       "      <td>-0.140773</td>\n",
       "      <td>0.089975</td>\n",
       "      <td>0.085778</td>\n",
       "      <td>-0.251992</td>\n",
       "      <td>-0.010128</td>\n",
       "      <td>0.120774</td>\n",
       "      <td>0.070248</td>\n",
       "      <td>-0.075598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21583</th>\n",
       "      <td>-0.052365</td>\n",
       "      <td>0.209897</td>\n",
       "      <td>0.085776</td>\n",
       "      <td>0.031980</td>\n",
       "      <td>-0.136825</td>\n",
       "      <td>0.168982</td>\n",
       "      <td>0.046745</td>\n",
       "      <td>0.182240</td>\n",
       "      <td>-0.073883</td>\n",
       "      <td>-0.236916</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.166680</td>\n",
       "      <td>0.041617</td>\n",
       "      <td>-0.164772</td>\n",
       "      <td>0.159023</td>\n",
       "      <td>0.058920</td>\n",
       "      <td>-0.195668</td>\n",
       "      <td>-0.061867</td>\n",
       "      <td>0.070360</td>\n",
       "      <td>0.059676</td>\n",
       "      <td>-0.190173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21584</th>\n",
       "      <td>-0.120541</td>\n",
       "      <td>0.102348</td>\n",
       "      <td>-0.034221</td>\n",
       "      <td>0.135927</td>\n",
       "      <td>0.011785</td>\n",
       "      <td>0.133389</td>\n",
       "      <td>0.116404</td>\n",
       "      <td>0.198307</td>\n",
       "      <td>-0.095573</td>\n",
       "      <td>-0.320705</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016061</td>\n",
       "      <td>-0.077538</td>\n",
       "      <td>-0.058656</td>\n",
       "      <td>-0.047204</td>\n",
       "      <td>0.146772</td>\n",
       "      <td>-0.225361</td>\n",
       "      <td>-0.024000</td>\n",
       "      <td>0.091415</td>\n",
       "      <td>0.055977</td>\n",
       "      <td>-0.124696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21585</th>\n",
       "      <td>-0.088767</td>\n",
       "      <td>0.183101</td>\n",
       "      <td>0.121097</td>\n",
       "      <td>0.041343</td>\n",
       "      <td>-0.066061</td>\n",
       "      <td>0.097650</td>\n",
       "      <td>0.103824</td>\n",
       "      <td>0.184291</td>\n",
       "      <td>-0.164192</td>\n",
       "      <td>-0.194438</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.176102</td>\n",
       "      <td>0.089446</td>\n",
       "      <td>-0.190146</td>\n",
       "      <td>0.144457</td>\n",
       "      <td>0.041963</td>\n",
       "      <td>-0.107956</td>\n",
       "      <td>-0.100026</td>\n",
       "      <td>0.111624</td>\n",
       "      <td>-0.057649</td>\n",
       "      <td>-0.143844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21586</th>\n",
       "      <td>-0.162002</td>\n",
       "      <td>0.230147</td>\n",
       "      <td>0.038554</td>\n",
       "      <td>0.104845</td>\n",
       "      <td>-0.021440</td>\n",
       "      <td>-0.054439</td>\n",
       "      <td>0.123186</td>\n",
       "      <td>0.117806</td>\n",
       "      <td>0.088217</td>\n",
       "      <td>-0.327175</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012391</td>\n",
       "      <td>-0.012991</td>\n",
       "      <td>-0.002100</td>\n",
       "      <td>0.056460</td>\n",
       "      <td>0.155021</td>\n",
       "      <td>-0.113956</td>\n",
       "      <td>-0.190297</td>\n",
       "      <td>0.090047</td>\n",
       "      <td>0.046523</td>\n",
       "      <td>-0.168838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21587 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6   \\\n",
       "0     -0.099359  0.229848  0.001726  0.065618 -0.077882  0.141544  0.029603   \n",
       "1     -0.148271  0.168280  0.074423 -0.116408 -0.074596  0.164001  0.001238   \n",
       "2     -0.209073  0.164122 -0.088313  0.026671 -0.099755  0.126713  0.088443   \n",
       "3     -0.177481  0.377796  0.160337 -0.150246 -0.175173  0.289743  0.090508   \n",
       "4     -0.188473  0.112612  0.000741 -0.003990 -0.086077  0.120941  0.107368   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "21582 -0.142881  0.171760  0.051145  0.146917 -0.034540  0.155963  0.084235   \n",
       "21583 -0.052365  0.209897  0.085776  0.031980 -0.136825  0.168982  0.046745   \n",
       "21584 -0.120541  0.102348 -0.034221  0.135927  0.011785  0.133389  0.116404   \n",
       "21585 -0.088767  0.183101  0.121097  0.041343 -0.066061  0.097650  0.103824   \n",
       "21586 -0.162002  0.230147  0.038554  0.104845 -0.021440 -0.054439  0.123186   \n",
       "\n",
       "             7         8         9   ...        90        91        92  \\\n",
       "0      0.131862 -0.104955 -0.206811  ... -0.121027  0.149032 -0.143528   \n",
       "1      0.089994 -0.035212 -0.181781  ... -0.045649  0.178929 -0.097523   \n",
       "2      0.121720 -0.157667 -0.372412  ... -0.067356  0.068995 -0.021621   \n",
       "3      0.314686 -0.299155 -0.010455  ... -0.216118  0.011734 -0.159354   \n",
       "4      0.057337 -0.076843 -0.222488  ... -0.045428  0.140131 -0.083319   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "21582  0.122642 -0.115169 -0.203340  ... -0.066174  0.070993 -0.140773   \n",
       "21583  0.182240 -0.073883 -0.236916  ... -0.166680  0.041617 -0.164772   \n",
       "21584  0.198307 -0.095573 -0.320705  ...  0.016061 -0.077538 -0.058656   \n",
       "21585  0.184291 -0.164192 -0.194438  ... -0.176102  0.089446 -0.190146   \n",
       "21586  0.117806  0.088217 -0.327175  ... -0.012391 -0.012991 -0.002100   \n",
       "\n",
       "             93        94        95        96        97        98        99  \n",
       "0      0.000074  0.258949 -0.123598 -0.112917  0.118791  0.072575 -0.258159  \n",
       "1      0.062516  0.158933 -0.129134 -0.052119  0.039353  0.006741 -0.270934  \n",
       "2      0.138720  0.058073 -0.133802 -0.155317 -0.030555 -0.070853 -0.257945  \n",
       "3      0.198609  0.043519 -0.381176 -0.071579 -0.004116  0.108335  0.077561  \n",
       "4     -0.001670  0.101014 -0.217243 -0.080381  0.106238  0.016119 -0.262751  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "21582  0.089975  0.085778 -0.251992 -0.010128  0.120774  0.070248 -0.075598  \n",
       "21583  0.159023  0.058920 -0.195668 -0.061867  0.070360  0.059676 -0.190173  \n",
       "21584 -0.047204  0.146772 -0.225361 -0.024000  0.091415  0.055977 -0.124696  \n",
       "21585  0.144457  0.041963 -0.107956 -0.100026  0.111624 -0.057649 -0.143844  \n",
       "21586  0.056460  0.155021 -0.113956 -0.190297  0.090047  0.046523 -0.168838  \n",
       "\n",
       "[21587 rows x 100 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mean_vector_2 = pd.DataFrame(train_mean_vector_2)\n",
    "train_mean_vector_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# income_occyp_type, income_age_type, edu_occyp_type, edu_income_type, family_income_type\n",
    "\n",
    "import random\n",
    "\n",
    "tst2=tst.reset_index().drop('index',axis=1).copy()\n",
    "tst2=tst2.reset_index()\n",
    "test_data=list(tst2.groupby('custid')['brd_nm'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 14380/14380 [00:01<00:00, 13770.61it/s]\n"
     ]
    }
   ],
   "source": [
    "test_mean_vector_2 = []\n",
    "for words in tqdm(test_data):\n",
    "    tmp = np.zeros(100)\n",
    "    cnt = 0\n",
    "    for word in words:\n",
    "        try:\n",
    "            tmp += w2v[word]\n",
    "            cnt += 1\n",
    "        except:\n",
    "            pass\n",
    "    tmp /= cnt\n",
    "    test_mean_vector_2.append(tmp)\n",
    "test_mean_vector_2 = np.array(test_mean_vector_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.122656</td>\n",
       "      <td>0.216789</td>\n",
       "      <td>0.054525</td>\n",
       "      <td>0.051357</td>\n",
       "      <td>-0.081809</td>\n",
       "      <td>0.134494</td>\n",
       "      <td>0.030452</td>\n",
       "      <td>0.119468</td>\n",
       "      <td>-0.065105</td>\n",
       "      <td>-0.209350</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.092898</td>\n",
       "      <td>0.143492</td>\n",
       "      <td>-0.094360</td>\n",
       "      <td>0.027026</td>\n",
       "      <td>0.225925</td>\n",
       "      <td>-0.144084</td>\n",
       "      <td>-0.030519</td>\n",
       "      <td>0.088379</td>\n",
       "      <td>0.003156</td>\n",
       "      <td>-0.213106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.120233</td>\n",
       "      <td>0.143215</td>\n",
       "      <td>0.045958</td>\n",
       "      <td>0.051132</td>\n",
       "      <td>-0.011490</td>\n",
       "      <td>0.125507</td>\n",
       "      <td>0.034136</td>\n",
       "      <td>0.093216</td>\n",
       "      <td>-0.063237</td>\n",
       "      <td>-0.268051</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.097375</td>\n",
       "      <td>0.058727</td>\n",
       "      <td>-0.098590</td>\n",
       "      <td>0.067234</td>\n",
       "      <td>0.089169</td>\n",
       "      <td>-0.188219</td>\n",
       "      <td>-0.049308</td>\n",
       "      <td>0.093123</td>\n",
       "      <td>0.033251</td>\n",
       "      <td>-0.252178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.196941</td>\n",
       "      <td>0.130768</td>\n",
       "      <td>0.053954</td>\n",
       "      <td>0.090406</td>\n",
       "      <td>-0.053738</td>\n",
       "      <td>0.114268</td>\n",
       "      <td>0.104093</td>\n",
       "      <td>0.104881</td>\n",
       "      <td>-0.120869</td>\n",
       "      <td>-0.174747</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.156218</td>\n",
       "      <td>0.078043</td>\n",
       "      <td>-0.105169</td>\n",
       "      <td>0.107210</td>\n",
       "      <td>0.027203</td>\n",
       "      <td>-0.090784</td>\n",
       "      <td>0.010746</td>\n",
       "      <td>0.083774</td>\n",
       "      <td>0.067833</td>\n",
       "      <td>-0.178055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.062131</td>\n",
       "      <td>0.324611</td>\n",
       "      <td>0.121338</td>\n",
       "      <td>0.211037</td>\n",
       "      <td>-0.214838</td>\n",
       "      <td>-0.007224</td>\n",
       "      <td>0.126483</td>\n",
       "      <td>0.165214</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.138296</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.283600</td>\n",
       "      <td>-0.017036</td>\n",
       "      <td>-0.234514</td>\n",
       "      <td>0.071472</td>\n",
       "      <td>0.063863</td>\n",
       "      <td>-0.240795</td>\n",
       "      <td>0.019607</td>\n",
       "      <td>0.156207</td>\n",
       "      <td>0.209697</td>\n",
       "      <td>-0.177349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.214753</td>\n",
       "      <td>0.248412</td>\n",
       "      <td>0.120212</td>\n",
       "      <td>-0.007139</td>\n",
       "      <td>-0.107194</td>\n",
       "      <td>0.100556</td>\n",
       "      <td>0.076524</td>\n",
       "      <td>0.146680</td>\n",
       "      <td>-0.092050</td>\n",
       "      <td>-0.069768</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.138405</td>\n",
       "      <td>0.007433</td>\n",
       "      <td>-0.053476</td>\n",
       "      <td>0.130685</td>\n",
       "      <td>0.045783</td>\n",
       "      <td>-0.154345</td>\n",
       "      <td>-0.162498</td>\n",
       "      <td>0.099175</td>\n",
       "      <td>0.071110</td>\n",
       "      <td>-0.236209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14375</th>\n",
       "      <td>-0.146219</td>\n",
       "      <td>0.044447</td>\n",
       "      <td>-0.010113</td>\n",
       "      <td>0.088289</td>\n",
       "      <td>-0.059829</td>\n",
       "      <td>0.152261</td>\n",
       "      <td>0.195043</td>\n",
       "      <td>0.287960</td>\n",
       "      <td>-0.014177</td>\n",
       "      <td>-0.129112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114445</td>\n",
       "      <td>0.155443</td>\n",
       "      <td>-0.003605</td>\n",
       "      <td>0.070704</td>\n",
       "      <td>-0.047171</td>\n",
       "      <td>-0.185280</td>\n",
       "      <td>-0.013389</td>\n",
       "      <td>0.139391</td>\n",
       "      <td>0.010890</td>\n",
       "      <td>-0.384596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14376</th>\n",
       "      <td>-0.159082</td>\n",
       "      <td>0.255758</td>\n",
       "      <td>0.077017</td>\n",
       "      <td>-0.064341</td>\n",
       "      <td>-0.093868</td>\n",
       "      <td>0.134684</td>\n",
       "      <td>0.076568</td>\n",
       "      <td>0.207802</td>\n",
       "      <td>-0.158940</td>\n",
       "      <td>-0.001104</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.094023</td>\n",
       "      <td>0.005866</td>\n",
       "      <td>-0.070563</td>\n",
       "      <td>0.125681</td>\n",
       "      <td>0.181137</td>\n",
       "      <td>-0.034188</td>\n",
       "      <td>-0.014389</td>\n",
       "      <td>0.055022</td>\n",
       "      <td>0.062635</td>\n",
       "      <td>-0.344001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14377</th>\n",
       "      <td>-0.145854</td>\n",
       "      <td>0.114265</td>\n",
       "      <td>-0.133513</td>\n",
       "      <td>-0.080089</td>\n",
       "      <td>-0.116625</td>\n",
       "      <td>0.173706</td>\n",
       "      <td>0.127648</td>\n",
       "      <td>0.211195</td>\n",
       "      <td>-0.023584</td>\n",
       "      <td>-0.354340</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024941</td>\n",
       "      <td>0.019279</td>\n",
       "      <td>-0.029240</td>\n",
       "      <td>0.085955</td>\n",
       "      <td>0.311522</td>\n",
       "      <td>-0.232197</td>\n",
       "      <td>-0.133312</td>\n",
       "      <td>0.107663</td>\n",
       "      <td>0.009735</td>\n",
       "      <td>-0.208561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14378</th>\n",
       "      <td>-0.109856</td>\n",
       "      <td>0.179297</td>\n",
       "      <td>-0.011043</td>\n",
       "      <td>-0.115616</td>\n",
       "      <td>-0.118027</td>\n",
       "      <td>0.107160</td>\n",
       "      <td>-0.039540</td>\n",
       "      <td>0.186736</td>\n",
       "      <td>-0.053808</td>\n",
       "      <td>-0.265585</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069942</td>\n",
       "      <td>0.077971</td>\n",
       "      <td>0.024693</td>\n",
       "      <td>0.070882</td>\n",
       "      <td>-0.030771</td>\n",
       "      <td>0.050907</td>\n",
       "      <td>-0.036859</td>\n",
       "      <td>0.067216</td>\n",
       "      <td>-0.040569</td>\n",
       "      <td>-0.325106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14379</th>\n",
       "      <td>-0.005918</td>\n",
       "      <td>0.277293</td>\n",
       "      <td>0.153069</td>\n",
       "      <td>0.135182</td>\n",
       "      <td>0.078853</td>\n",
       "      <td>0.131798</td>\n",
       "      <td>0.040581</td>\n",
       "      <td>0.157280</td>\n",
       "      <td>-0.095525</td>\n",
       "      <td>-0.091647</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.153020</td>\n",
       "      <td>-0.029306</td>\n",
       "      <td>-0.034696</td>\n",
       "      <td>0.058917</td>\n",
       "      <td>0.207587</td>\n",
       "      <td>-0.139868</td>\n",
       "      <td>-0.030032</td>\n",
       "      <td>0.021487</td>\n",
       "      <td>0.028752</td>\n",
       "      <td>-0.223584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14380 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6   \\\n",
       "0     -0.122656  0.216789  0.054525  0.051357 -0.081809  0.134494  0.030452   \n",
       "1     -0.120233  0.143215  0.045958  0.051132 -0.011490  0.125507  0.034136   \n",
       "2     -0.196941  0.130768  0.053954  0.090406 -0.053738  0.114268  0.104093   \n",
       "3     -0.062131  0.324611  0.121338  0.211037 -0.214838 -0.007224  0.126483   \n",
       "4     -0.214753  0.248412  0.120212 -0.007139 -0.107194  0.100556  0.076524   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "14375 -0.146219  0.044447 -0.010113  0.088289 -0.059829  0.152261  0.195043   \n",
       "14376 -0.159082  0.255758  0.077017 -0.064341 -0.093868  0.134684  0.076568   \n",
       "14377 -0.145854  0.114265 -0.133513 -0.080089 -0.116625  0.173706  0.127648   \n",
       "14378 -0.109856  0.179297 -0.011043 -0.115616 -0.118027  0.107160 -0.039540   \n",
       "14379 -0.005918  0.277293  0.153069  0.135182  0.078853  0.131798  0.040581   \n",
       "\n",
       "             7         8         9   ...        90        91        92  \\\n",
       "0      0.119468 -0.065105 -0.209350  ... -0.092898  0.143492 -0.094360   \n",
       "1      0.093216 -0.063237 -0.268051  ... -0.097375  0.058727 -0.098590   \n",
       "2      0.104881 -0.120869 -0.174747  ... -0.156218  0.078043 -0.105169   \n",
       "3      0.165214  0.000144 -0.138296  ... -0.283600 -0.017036 -0.234514   \n",
       "4      0.146680 -0.092050 -0.069768  ... -0.138405  0.007433 -0.053476   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "14375  0.287960 -0.014177 -0.129112  ...  0.114445  0.155443 -0.003605   \n",
       "14376  0.207802 -0.158940 -0.001104  ... -0.094023  0.005866 -0.070563   \n",
       "14377  0.211195 -0.023584 -0.354340  ... -0.024941  0.019279 -0.029240   \n",
       "14378  0.186736 -0.053808 -0.265585  ... -0.069942  0.077971  0.024693   \n",
       "14379  0.157280 -0.095525 -0.091647  ... -0.153020 -0.029306 -0.034696   \n",
       "\n",
       "             93        94        95        96        97        98        99  \n",
       "0      0.027026  0.225925 -0.144084 -0.030519  0.088379  0.003156 -0.213106  \n",
       "1      0.067234  0.089169 -0.188219 -0.049308  0.093123  0.033251 -0.252178  \n",
       "2      0.107210  0.027203 -0.090784  0.010746  0.083774  0.067833 -0.178055  \n",
       "3      0.071472  0.063863 -0.240795  0.019607  0.156207  0.209697 -0.177349  \n",
       "4      0.130685  0.045783 -0.154345 -0.162498  0.099175  0.071110 -0.236209  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "14375  0.070704 -0.047171 -0.185280 -0.013389  0.139391  0.010890 -0.384596  \n",
       "14376  0.125681  0.181137 -0.034188 -0.014389  0.055022  0.062635 -0.344001  \n",
       "14377  0.085955  0.311522 -0.232197 -0.133312  0.107663  0.009735 -0.208561  \n",
       "14378  0.070882 -0.030771  0.050907 -0.036859  0.067216 -0.040569 -0.325106  \n",
       "14379  0.058917  0.207587 -0.139868 -0.030032  0.021487  0.028752 -0.223584  \n",
       "\n",
       "[14380 rows x 100 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mean_vector_2 = pd.DataFrame(test_mean_vector_2)\n",
    "test_mean_vector_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "## save\n",
    "with open('train_mean_vector_brd.pickle', 'wb') as f:\n",
    "    pickle.dump(train_mean_vector_2, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "## save\n",
    "with open('test_mean_vector_brd.pickle', 'wb') as f:\n",
    "    pickle.dump(test_mean_vector_2, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sales time - 아침/ 낮/ 밤 구매 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sales_time'] = data['sales_time'].apply(lambda x: int(str(x)[:2]) if len(str(x)) == 4 else int(str(x)[:1]))\n",
    "data['sales_time_cat'] = data['sales_time'].apply(lambda x: 'mo_sales' if (9 <= x <= 12) else ('af_sales' if (13 <= x <= 18) else 'knt_sales'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dis_amt - 할인율 특성 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['dis_rate'] = data['dis_amt']/data['tot_amt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "퇴근전/후 구매 빈도, 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_off_work(x):\n",
    "    if 9<=x<=17:\n",
    "        return('bf_work')\n",
    "    else:\n",
    "        return('af_work')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sales_type'] = data['sales_time'].map(get_off_work)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "시간별 구매 횟수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_per_time = pd.pivot_table(index='custid',columns='sales_time',values='tot_amt',aggfunc=np.size, fill_value=0, data=data.query(\"tot_amt>0\")[['custid','sales_time','tot_amt']]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_type = pd.pivot_table(index='custid',columns='sales_type',values='tot_amt',aggfunc=np.size, fill_value=0,data = data.query('tot_amt>0')[['custid','sales_type','tot_amt']]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_per_time.columns=['custid', '1_stf', '9_stf', \"10_stf\", '11_stf', '12_stf', '13_stf', '14_stf', '15_stf', '16_stf', '17_stf', '18_stf', '19_stf', '20_stf', '21_stf', '22_stf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.merge(freq_per_time, sales_type, on = 'custid')\n",
    "features = pd.merge(features, shop_time, on = 'custid')\n",
    "features = pd.merge(features, dtdt_mean, on = 'custid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "점포별 가중치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def floor_transfer_a(x) :\n",
    "    return  {\n",
    "        \"무역점\" : 0,\n",
    "        \"본점\" : 1,\n",
    "        \"신촌점\" : 0,\n",
    "        \"천호점\" : 1\n",
    "    }.get(x)\n",
    "\n",
    "def floor_transfer_m(x) :  \n",
    "    return  {\n",
    "        \"무역점\" : 1/10*13,\n",
    "        \"본점\" : 1/6*13,\n",
    "        \"신촌점\" : 1/11*13,\n",
    "        \"천호점\" : 1\n",
    "    }.get(x)\n",
    "data['floor'] = (data.str_nm.apply(floor_transfer_a))*data.str_nm.apply(floor_transfer_m).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.merge(features, data.groupby('custid')['floor'].mean().reset_index(), on = 'custid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아침구매/ 낮구매/ 밤구매"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['af_work_rate'] = features['af_work']/(features['af_work'] + features['bf_work'])\n",
    "features['bf_work_rate'] = features['bf_work']/(features['af_work'] + features['bf_work'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_sales_cat = pd.pivot_table(index='custid',columns='sales_time_cat',values='tot_amt',aggfunc=np.size, fill_value=0, data=data.query(\"tot_amt>0\")[['custid','sales_time_cat','tot_amt']]).reset_index()\n",
    "features = pd.merge(features, freq_sales_cat,on='custid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "환불"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "refund_freq=data.query('tot_amt<0').groupby('custid')['sales_day'].count().reset_index()\n",
    "features = pd.merge(features, refund_freq, left_on = 'custid',right_on='custid',how='left')\n",
    "features.rename(columns = {'sales_day': 'refund_freq'},inplace=True)\n",
    "features.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_freq=data.query('tot_amt>0').groupby('custid')['sales_day'].count().reset_index()\n",
    "features = pd.merge(features, sales_freq, left_on = 'custid',right_on='custid',how='left')\n",
    "features.rename(columns = {'sales_day': 'sales_freq'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "구매 빈도 특성 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_freq=data.groupby('custid')['sales_day'].count().reset_index()\n",
    "features = pd.merge(features, all_freq,  left_on = 'custid',right_on='custid',how='left')\n",
    "features.rename(columns = {'sales_day': 'all_freq'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['refund_sales_rate']=features['refund_freq']/features['sales_freq']\n",
    "features['refund_all_rate'] = features['refund_freq']/features['all_freq']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "방문한 날짜의 갯수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aaa=data.groupby(['custid','sales_month','sales_day'])['sales_dayofweek'].count().reset_index()\n",
    "data_aaa.sales_dayofweek=1\n",
    "data_aaa=data_aaa.groupby('custid')['sales_dayofweek'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.merge(features, data_aaa, on = 'custid')\n",
    "\n",
    "features['sales_freq_mean']=features['sales_freq']/features['sales_dayofweek']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "평일에 방문하는 평균 시간"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "vuddlf=data[data.weekend==0].groupby('custid')['sales_time'].mean().reset_index()\n",
    "features = pd.merge(features, vuddlf, how='left',on = 'custid')\n",
    "\n",
    "wkdtjdgus=data[data.weekend==0].groupby('custid')['sales_time'].agg([('sales_time_mod', lambda x: x.mode().index[0])]).reset_index()\n",
    "features = pd.merge(features, wkdtjdgus, how='left',on = 'custid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주말에 방문하는 평균 시간"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnakf=data[data.weekend==1].groupby('custid')['sales_time'].mean().reset_index()\n",
    "features = pd.merge(features, wnakf, how='left',on = 'custid')\n",
    "\n",
    "wkdtjdgus=data[data.weekend==1].groupby('custid')['sales_time'].agg([('sales_time_mod', lambda x: x.mode().index[0])]).reset_index()\n",
    "features = pd.merge(features, wkdtjdgus, how='left',on = 'custid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수입상품 관련 열\n",
    "sales_import=data.groupby('custid')['import_flg'].agg('sum').reset_index()\n",
    "features = pd.merge(features, sales_import, on = 'custid')\n",
    "\n",
    "features.rename(columns = {'import_flg': 'sales_import'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수입상품 구매 빈도 추가\n",
    "features['import_rate']=features['sales_import']/features['sales_freq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수입상품 환불 빈도 추가\n",
    "features['import_refund_rate']=features['sales_import']/(features['refund_freq']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 계절별 구매 빈도 추가\n",
    "def f1(x):\n",
    "    if 3 <= x <= 5 :\n",
    "        return('spring_sales')\n",
    "    elif 6 <= x <= 8 :\n",
    "        return('summer_sales')\n",
    "    elif 9 <= x <= 11 :\n",
    "        return('fall_sales')\n",
    "    else :\n",
    "        return('winter_sales')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['season_sales']=data['sales_month'].apply(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "season = pd.pivot_table(data, index='custid', columns='season_sales', values='tot_amt', \n",
    "                   aggfunc=np.size, fill_value=0).reset_index()\n",
    "features = pd.merge(features, season, how='left',on = 'custid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#계절별 구매만 빈도\n",
    "season_sales_freq = pd.pivot_table(index='custid',columns='season_sales',values='tot_amt',aggfunc=np.size, fill_value=0, data=data.query(\"tot_amt>0\")[['custid','season_sales','tot_amt']]).reset_index()\n",
    "features = pd.merge(features, season_sales_freq, how='left',on = 'custid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#계절별 환불만 빈도\n",
    "season_refund_freq = pd.pivot_table(index='custid',columns='season_sales',values='tot_amt',aggfunc=np.size, fill_value=0, data=data.query(\"tot_amt<0\")[['custid','season_sales','tot_amt']]).reset_index()\n",
    "features = pd.merge(features, season_refund_freq, how='left',on = 'custid')\n",
    "features.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['sales_fall_rate']=features.fall_sales_x/(features.fall_sales_x+features.spring_sales_x+features.summer_sales_x+features.winter_sales_x)\n",
    "features['sales_spring_rate']=features.spring_sales_x/(features.fall_sales_x+features.spring_sales_x+features.summer_sales_x+features.winter_sales_x)\n",
    "features['sales_summer_rate']=features.summer_sales_x/(features.fall_sales_x+features.spring_sales_x+features.summer_sales_x+features.winter_sales_x)\n",
    "features['sales_winter_rate']=features.winter_sales_x/(features.fall_sales_x+features.spring_sales_x+features.summer_sales_x+features.winter_sales_x)\n",
    "\n",
    "features['refund_fall_rate']=features.fall_sales_y/(features.fall_sales_y+features.spring_sales_y+features.summer_sales_y+features.winter_sales_y)\n",
    "features['refund_spring_rate']=features.spring_sales_y/(features.fall_sales_y+features.spring_sales_y+features.summer_sales_y+features.winter_sales_y)\n",
    "features['refund_summer_rate']=features.summer_sales_y/(features.fall_sales_y+features.spring_sales_y+features.summer_sales_y+features.winter_sales_y)\n",
    "features['refund_winter_rate']=features.winter_sales_y/(features.fall_sales_y+features.spring_sales_y+features.summer_sales_y+features.winter_sales_y)\n",
    "\n",
    "features['all_fall_rate']=features.fall_sales/(features.fall_sales+features.spring_sales+features.summer_sales+features.winter_sales)\n",
    "features['all_spring_rate']=features.spring_sales/(features.fall_sales+features.spring_sales+features.summer_sales+features.winter_sales)\n",
    "features['all_summer_rate']=features.summer_sales/(features.fall_sales+features.spring_sales+features.summer_sales+features.winter_sales)\n",
    "features['all_winter_rate']=features.winter_sales/(features.fall_sales+features.spring_sales+features.summer_sales+features.winter_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주말 구매 비율\n",
    "features['wd_sales_rate'] = (data.groupby('custid')['weekend'].sum().reset_index()['weekend']/features['sales_freq'])*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "월별 구매액의 합, 평균"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_sales_mn=data.groupby(['custid','sales_month'])['net_amt'].mean().unstack().fillna(0)\n",
    "\n",
    "month_sales_mn.columns = ['mon_sale_mn'+ str(column) for column in month_sales_mn.columns]\n",
    "month_sales_mn = month_sales_mn.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_sales_sm=data.groupby(['custid','sales_month'])['net_amt'].sum().unstack().fillna(0)\n",
    "\n",
    "month_sales_sm.columns = ['mon_sale_sm'+ str(column) for column in month_sales_sm.columns]\n",
    "month_sales_sm = month_sales_sm.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.merge(features, month_sales_sm, how='left',on = 'custid')\n",
    "\n",
    "features = pd.merge(features, month_sales_mn, how='left',on = 'custid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구매 월 평균\n",
    "features['month_mean'] = data.groupby('custid')['sales_month'].mean().reset_index()['sales_month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자주 가는 월 - 원래 자주가는 \n",
    "#f = pd.pivot_table(data, index='custid', columns='sales_month', values='tot_amt', \n",
    "#                   aggfunc=np.size, fill_value=0).reset_index()\n",
    "f=pd.pivot_table(data[data.tot_amt>0], index='custid', columns='sales_month', values='tot_amt', \n",
    "                   aggfunc=np.size, fill_value=0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.columns=['custid', '1_mmff', '2_mmff', '3_mmff', '4_mmff', '5_mmff', '6_mmff', '7_mmff', '8_mmff', '9_mmff', '10_mmff', '11_mmff', '12_mmff']\n",
    "features = pd.merge(features, f, how='left',on = 'custid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "금액 관리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "refund_net_sum=data[data.net_amt<0].groupby('custid')['net_amt'].sum().reset_index()\n",
    "features = pd.merge(features, refund_net_sum, how='left',on = 'custid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_saels_all=data.groupby('custid')['net_amt'].agg([('tot_sales_all','sum'),('mean_amt_all','mean')])\n",
    "features = pd.merge(features, net_saels_all, how='left',on = 'custid').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_sales_0_all=data[data.net_amt>0].groupby('custid')['net_amt'].agg([('tot_sales','sum'),('max_sales_amt','max'),('min_sales_amt','min'),('mean_sales_amt','mean')])\n",
    "features = pd.merge(features, net_sales_0_all, how='left',on = 'custid').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_refund_all=data[data.net_amt<0].groupby('custid')['net_amt'].agg([('tot_refund','sum'),('max_refund_amt','max'),('min_refund_amt','min'),('mean_refund_amt','mean')])\n",
    "features = pd.merge(features, net_refund_all, how='left',on = 'custid').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[['mean_amt_all','mean_sales_amt','mean_refund_amt']] =round(features[['mean_amt_all','mean_sales_amt','mean_refund_amt']],2)\n",
    "\n",
    "features['tot_sales_freq_mean']=features['tot_sales']/features['sales_dayofweek']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 총 할인금액\n",
    "features['tot_dis_amt'] = data.groupby('custid')['dis_amt'].sum().reset_index()['dis_amt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대, 평균 할인율\n",
    "features['max_dis'] = data.groupby('custid')['dis_rate'].max().reset_index()['dis_rate']\n",
    "features['mean_dis'] = data.groupby('custid')['dis_rate'].mean().reset_index()['dis_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 할인 받은 빈도, 비율\n",
    "dis_rate=data.query(\"dis_rate>0\").groupby('custid')['dis_rate'].count().reset_index()\n",
    "features = pd.merge(features, dis_rate, how='left',on='custid').fillna(0)\n",
    "features.rename(columns = {'dis_rate': 'dis_freq'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['dis_freq_rate'] = features['dis_freq']/features['sales_freq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 할부 빈도, 비율/ 평균 할부개월\n",
    "inst_ft = data.query(\"inst_mon != 1\").groupby('custid')['inst_mon'].agg([('inst_freq','count'),('inst_avg','mean'),('inst_max','max')])\n",
    "features = pd.merge(features, inst_ft, how='left',on = 'custid').fillna(0)\n",
    "features['inst_freq_rate'] = features['inst_freq']/features['sales_freq']\n",
    "features['inst_freq_all_rate'] = features['inst_freq']/features['all_freq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 할부 총 달\n",
    "inst_ft_sum = data.groupby('custid')['inst_mon'].agg([('inst_sum','sum')])\n",
    "features = pd.merge(features, inst_ft_sum, how='left',on = 'custid').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['tot_inst_amt']=features['tot_sales']/features['inst_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 할부 총 달\n",
    "sales_noinst = data.groupby('custid')['inst_fee'].agg([('inst_fee_sum','sum')])\n",
    "features = pd.merge(features, sales_noinst, how='left',on = 'custid').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def inst_avg(x):\n",
    "#    if x >=3:\n",
    "#        return 1\n",
    "#    else:\n",
    "#        return 0\n",
    "#\n",
    "#features['inst_quan'] = features.inst_avg.apply(inst_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자주가는 지점이 어딘지 원핫인코딩 한 것\n",
    "data['str_nm'] = data['str_nm'].apply(lambda x: 'mu' if x=='무역점' else('bon' if x=='본점' else('cheon' if x=='천호점' else 'sin')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = data.groupby(['custid','str_nm'])['str_nm'].agg([('str_freq','count')]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "996f627e8fb349a4abdcf86a6a8038e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=35967.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lst = []\n",
    "for i in tqdm_notebook(f['custid'].unique()) : \n",
    "    f2 = f.query(\"custid == @i\")\n",
    "    lst.append(f2.sort_values(by='str_freq',ascending=False)['str_nm'].iloc[0])\n",
    "\n",
    "features['str_freq'] = lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.get_dummies(features, 'str_freq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아이디 별 매장별 몇번씩 갔는지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_freq_all=data.groupby(['custid','str_nm'])['str_nm'].count().unstack().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_freq_all.columns = ['store_'+ str(column) for column in str_freq_all.columns]\n",
    "str_freq_all = str_freq_all.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features = pd.merge(features, str_freq_all, how='left',on = 'custid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "매장 별로 얼마나 구매를 하였는지 총구매액"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_sale_sum=data[data.net_amt>0].groupby(['custid','str_nm'])['net_amt'].sum().unstack().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_sale_sum.columns = ['str_sales'+ str(column) for column in str_sale_sum.columns]\n",
    "str_sale_sum = str_sale_sum.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.merge(features, str_sale_sum, how='left',on = 'custid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "매장 별로 얼마나 환불을 하였는지 총환불액"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_refund_sum=data[data.net_amt<0].groupby(['custid','str_nm'])['net_amt'].sum().unstack().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_refund_sum.columns = ['str_refund'+ str(column) for column in str_refund_sum.columns]\n",
    "str_refund_sum = str_refund_sum.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.merge(features, str_refund_sum, how='left',on = 'custid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 선호 브랜드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(trn, y, on='custid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "twn_prefer_brd = df.query(\"age>=20 & age<30\").brd_nm.value_counts().index[1:11].tolist()\n",
    "thr_prefer_brd = df.query(\"age>=30 & age<40\").brd_nm.value_counts().index[1:11].tolist()\n",
    "for_prefer_brd = df.query(\"age>=40 & age<50\").brd_nm.value_counts().index[1:11].tolist()\n",
    "fiv_prefer_brd = df.query(\"age>=50 & age<60\").brd_nm.value_counts().index[1:11].tolist()\n",
    "six_prefer_brd = df.query(\"age>=60\").brd_nm.value_counts().index[1:11].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_twn_brd(x):\n",
    "    if x == twn_prefer_brd[0]:\n",
    "        x=10\n",
    "    elif x == twn_prefer_brd[1]:\n",
    "        x=9\n",
    "    elif x == twn_prefer_brd[2]:\n",
    "        x=8\n",
    "    elif x == twn_prefer_brd[3]:\n",
    "        x=7\n",
    "    elif x == twn_prefer_brd[4]:\n",
    "        x=6\n",
    "    elif x == twn_prefer_brd[5]:\n",
    "        x=5\n",
    "    elif x == twn_prefer_brd[6]:\n",
    "        x=4\n",
    "    elif x == twn_prefer_brd[7]:\n",
    "        x=3\n",
    "    elif x == twn_prefer_brd[8]:\n",
    "        x=2\n",
    "    elif x == twn_prefer_brd[9]:\n",
    "        x=1\n",
    "    else:\n",
    "        x=0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_thr_brd(x):\n",
    "    if x == thr_prefer_brd[0]:\n",
    "        x=10\n",
    "    elif x == thr_prefer_brd[1]:\n",
    "        x=9\n",
    "    elif x == thr_prefer_brd[2]:\n",
    "        x=8\n",
    "    elif x == thr_prefer_brd[3]:\n",
    "        x=7\n",
    "    elif x == thr_prefer_brd[4]:\n",
    "        x=6\n",
    "    elif x == thr_prefer_brd[5]:\n",
    "        x=5\n",
    "    elif x == thr_prefer_brd[6]:\n",
    "        x=4\n",
    "    elif x == thr_prefer_brd[7]:\n",
    "        x=3\n",
    "    elif x == thr_prefer_brd[8]:\n",
    "        x=2\n",
    "    elif x == thr_prefer_brd[9]:\n",
    "        x=1\n",
    "    else:\n",
    "        x=0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_for_brd(x):\n",
    "    if x == for_prefer_brd[0]:\n",
    "        x=10\n",
    "    elif x == for_prefer_brd[1]:\n",
    "        x=9\n",
    "    elif x == for_prefer_brd[2]:\n",
    "        x=8\n",
    "    elif x == for_prefer_brd[3]:\n",
    "        x=7\n",
    "    elif x == for_prefer_brd[4]:\n",
    "        x=6\n",
    "    elif x == for_prefer_brd[5]:\n",
    "        x=5\n",
    "    elif x == for_prefer_brd[6]:\n",
    "        x=4\n",
    "    elif x == for_prefer_brd[7]:\n",
    "        x=3\n",
    "    elif x == for_prefer_brd[8]:\n",
    "        x=2\n",
    "    elif x == for_prefer_brd[9]:\n",
    "        x=1\n",
    "    else:\n",
    "        x=0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_fiv_brd(x):\n",
    "    if x == fiv_prefer_brd[0]:\n",
    "        x=10\n",
    "    elif x == fiv_prefer_brd[1]:\n",
    "        x=9\n",
    "    elif x == fiv_prefer_brd[2]:\n",
    "        x=8\n",
    "    elif x == fiv_prefer_brd[3]:\n",
    "        x=7\n",
    "    elif x == fiv_prefer_brd[4]:\n",
    "        x=6\n",
    "    elif x == fiv_prefer_brd[5]:\n",
    "        x=5\n",
    "    elif x == fiv_prefer_brd[6]:\n",
    "        x=4\n",
    "    elif x == fiv_prefer_brd[7]:\n",
    "        x=3\n",
    "    elif x == fiv_prefer_brd[8]:\n",
    "        x=2\n",
    "    elif x == fiv_prefer_brd[9]:\n",
    "        x=1\n",
    "    else:\n",
    "        x=0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_six_brd(x):\n",
    "    if x == six_prefer_brd[0]:\n",
    "        x=10\n",
    "    elif x == six_prefer_brd[1]:\n",
    "        x=9\n",
    "    elif x == six_prefer_brd[2]:\n",
    "        x=8\n",
    "    elif x == six_prefer_brd[3]:\n",
    "        x=7\n",
    "    elif x == six_prefer_brd[4]:\n",
    "        x=6\n",
    "    elif x == six_prefer_brd[5]:\n",
    "        x=5\n",
    "    elif x == six_prefer_brd[6]:\n",
    "        x=4\n",
    "    elif x == six_prefer_brd[7]:\n",
    "        x=3\n",
    "    elif x == six_prefer_brd[8]:\n",
    "        x=2\n",
    "    elif x == six_prefer_brd[9]:\n",
    "        x=1\n",
    "    else:\n",
    "        x=0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['twn_brd_point'] = data['brd_nm'].apply(map_twn_brd)\n",
    "data['thr_brd_point'] = data['brd_nm'].apply(map_thr_brd)\n",
    "data['for_brd_point'] = data['brd_nm'].apply(map_for_brd)\n",
    "data['fiv_brd_point'] = data['brd_nm'].apply(map_fiv_brd)\n",
    "data['six_brd_point'] = data['brd_nm'].apply(map_six_brd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "twn_brd_point=data.groupby('custid')['twn_brd_point'].mean().reset_index()\n",
    "thr_brd_point=data.groupby('custid')['thr_brd_point'].mean().reset_index()\n",
    "for_brd_point=data.groupby('custid')['for_brd_point'].mean().reset_index()\n",
    "fiv_brd_point=data.groupby('custid')['fiv_brd_point'].mean().reset_index()\n",
    "six_brd_point=data.groupby('custid')['six_brd_point'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.merge(features, twn_brd_point, how='left',on='custid')\n",
    "features = pd.merge(features, thr_brd_point, how='left',on='custid')\n",
    "features = pd.merge(features, for_brd_point, how='left',on='custid')\n",
    "features = pd.merge(features, fiv_brd_point, how='left',on='custid')\n",
    "features = pd.merge(features, six_brd_point, how='left',on='custid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['buyer_nm'] = data['buyer_nm'].str.replace('행사장.*','행사장')\n",
    "data['buyer_nm'] = data['buyer_nm'].str.replace('피혁A','피혁')\n",
    "data['buyer_nm'] = data['buyer_nm'].str.replace('피혁B','피혁')\n",
    "\n",
    "data['buyer_nm'] = data['buyer_nm'].map({'화장품':'a', '수입명품':'b', '캐릭터캐주얼':'c', '트래디셔널캐주얼':'d', '유아동복':'e', '니트단품':'f', '영캐주얼':'g',\n",
    "       '엘레강스캐주얼':'h', '가전':'i', '섬유':'j', '장신구':'k', '조리욕실':'l', '스포츠':'m', '침구수예':'n', '피혁':'o', '일반식품':'p',\n",
    "       '유니캐주얼':'q', '정장셔츠':'r', '디자이너부띠끄':'s', '문화완구':'t', '타운모피':'u', '조리식품':'v', '기타바이어':'w',\n",
    "       '도자기크리스탈':'x', '가구':'y', '생활용품':'z', '행사장':'aa', '청과곡물':'bb', '점외':'cc'})\n",
    "\n",
    "buyer = pd.pivot_table(index='custid',columns='buyer_nm',values='tot_amt',aggfunc=np.size,fill_value=0,data=data.query(\"tot_amt>0\")[['custid','buyer_nm','tot_amt']]).reset_index()\n",
    "features = pd.merge(features, buyer, on='custid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#구매상품군 다양성\n",
    "n = data.part_nm.nunique()\n",
    "f = data.groupby('custid')['part_nm'].agg([('part_unique', lambda x: len(x.unique()) / n)]).reset_index()\n",
    "features = pd.merge(features, f, on='custid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#브랜드 다양성\n",
    "n=data.brd_nm.nunique()\n",
    "f = data.groupby('custid')['brd_nm'].agg([('brd_unique', lambda x: len(x.unique()) / n)]).reset_index()\n",
    "features = pd.merge(features, f, on='custid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#매장 이용 다양성\n",
    "n = 4\n",
    "f = data.groupby('custid')['str_nm'].agg([('str_unique', lambda x: len(x.unique()) / n)]).reset_index()\n",
    "features = pd.merge(features, f, on='custid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#위에서는 출근 시간 9~17시 퇴근시간 이후로 했는데 좀더 세분화\n",
    "def f2(x):\n",
    "    if 9 <= x < 12 :\n",
    "        return('9_12_sale')\n",
    "    elif 12 <= x < 14 :\n",
    "        return('12_14_sale')\n",
    "    elif 14 <= x < 16 :\n",
    "        return('14_16_sale')\n",
    "    elif 16 <= x < 18 :\n",
    "        return('16_18_sale')\n",
    "    else :\n",
    "        return('18_sale')  \n",
    "\n",
    "data['timeslot2'] = data.sales_time.apply(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sales_type = pd.pivot_table(index='custid',columns='timeslot2',values='tot_amt',aggfunc=np.size, fill_value=0,data = data.query('tot_amt>0')[['custid','timeslot2','tot_amt']]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=pd.merge(features, new_sales_type, on='custid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['12_14_sale_ratio']=features['12_14_sale']/(features['12_14_sale'] + features['14_16_sale']+features['16_18_sale']+features['18_sale']+features['9_12_sale'])\n",
    "features['14_16_sale_ratio']=features['14_16_sale']/(features['12_14_sale'] + features['14_16_sale']+features['16_18_sale']+features['18_sale']+features['9_12_sale'])\n",
    "features['16_18_sale_ratio']=features['16_18_sale']/(features['12_14_sale'] + features['14_16_sale']+features['16_18_sale']+features['18_sale']+features['9_12_sale'])\n",
    "features['18_sale_ratio']=features['18_sale']/(features['12_14_sale'] + features['14_16_sale']+features['16_18_sale']+features['18_sale']+features['9_12_sale'])\n",
    "features['9_12_sale_ratio']=features['9_12_sale']/(features['12_14_sale'] + features['14_16_sale']+features['16_18_sale']+features['18_sale']+features['9_12_sale'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#충동지수 충동구매를 하는사람들은 누구일까? \n",
    "# 여유가 있는, 나이가 많은 사람들이 충동구매를 할거같다. 곱하기대신 더하기도 생각해보자\n",
    "\n",
    "features['chungdong_sale']=(features.max_sales_amt/features.tot_sales)\n",
    "features['chungdong_sale_freq']= features.chungdong_sale*features.sales_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['mean_min_sales']=features.mean_sales_amt+features.min_sales_amt\n",
    "a=features[['custid','mean_min_sales']]\n",
    "df_LEFT_JOIN = pd.merge(data, a, left_on='custid', right_on='custid', how='left')\n",
    "df_LEFT_JOIN['chungdong']=df_LEFT_JOIN.net_amt >df_LEFT_JOIN.mean_min_sales\n",
    "chung=df_LEFT_JOIN.groupby('custid')['chungdong'].sum().reset_index()\n",
    "\n",
    "features = pd.merge(features, chung, left_on='custid', right_on='custid', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "요일별 구매 빈도/비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=data.groupby(['custid','sales_month','sales_day','sales_dayofweek'])['custid'].size().unstack().reset_index()\n",
    "a.iloc[:,3:]=a.iloc[:,3:]/a.iloc[:,3:]\n",
    "a=a.groupby('custid')[['금','목','수','월','일','토','화']].sum()\n",
    "\n",
    "features = pd.merge(features, a, on='custid')\n",
    "\n",
    "features = features.rename(columns={'월':'mon_opn','화':'tue_opn','수':'wen_opn','목':'thr_opn','금':'fri_opn','토':'sat_opn','일':'sun_opn'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['mon_opn_rate']=features.mon_opn/features.sales_dayofweek\n",
    "features['tue_opn_rate']=features.tue_opn/features.sales_dayofweek\n",
    "features['wen_opn_rate']=features.wen_opn/features.sales_dayofweek\n",
    "features['thr_opn_rate']=features.thr_opn/features.sales_dayofweek\n",
    "features['fri_opn_rate']=features.fri_opn/features.sales_dayofweek\n",
    "features['sat_opn_rate']=features.sat_opn/features.sales_dayofweek\n",
    "features['sun_opn_rate']=features.sun_opn/features.sales_dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=data[data.net_amt>0].groupby(['custid','sales_month','sales_day','sales_dayofweek'])['net_amt'].sum().unstack().reset_index()\n",
    "a=a.groupby('custid')[['금','목','수','월','일','토','화']].sum()\n",
    "\n",
    "features = pd.merge(features, a, on='custid')\n",
    "\n",
    "features = features.rename(columns={'월':'mon_pri','화':'tue_pri','수':'wen_pri','목':'thr_pri','금':'fri_pri','토':'sat_pri','일':'sun_pri'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['mon_pri_rate']=features.mon_pri/features.tot_sales\n",
    "features['tue_pri_rate']=features.tue_pri/features.tot_sales\n",
    "features['wen_pri_rate']=features.wen_pri/features.tot_sales\n",
    "features['thr_pri_rate']=features.thr_pri/features.tot_sales\n",
    "features['fri_pri_rate']=features.fri_pri/features.tot_sales\n",
    "features['sat_pri_rate']=features.sat_pri/features.tot_sales\n",
    "features['sun_pri_rate']=features.sun_pri/features.tot_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "충동구매를 하고 환불을 한 횟수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['max_min_sales']=features['max_sales_amt']-features['min_sales_amt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shopfnc(m):\n",
    "    if m < 5:\n",
    "        return 1\n",
    "    elif m < 12.94 :\n",
    "        return 2 \n",
    "    elif m < 21.538 :\n",
    "        return 3 \n",
    "    elif m < 33.909 :\n",
    "        return 4\n",
    "    else:\n",
    "        return 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['shop_time_mean']= features.shop_time.apply(lambda x : shopfnc(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249406.0\n",
      "498646.0000000001\n",
      "795650.0\n",
      "1165370.0000000002\n",
      "1622598.0\n",
      "2254104.0\n",
      "3107333.5999999996\n",
      "4541631.800000001\n",
      "7485838.200000001\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(0.1,1,0.1):\n",
    "    print(features.tot_sales.quantile(q=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 10분위별로 (1 ~ 10 범위)\n",
    "def fnc(m) : \n",
    "    if m < 249406.0:\n",
    "        return 1\n",
    "    elif m <498646 :\n",
    "        return 2 \n",
    "    elif m < 795650.0 :\n",
    "        return 3 \n",
    "    elif m < 1165370:\n",
    "        return 4 \n",
    "    elif m < 1622598.0 :\n",
    "        return 5 \n",
    "    elif m < 2254104 :\n",
    "        return 6 \n",
    "    elif m < 3107333:\n",
    "        return 7 \n",
    "    elif m < 4541631 :\n",
    "        return 8\n",
    "    elif m < 7485838 :\n",
    "        return 9 \n",
    "    else:\n",
    "        return 10\n",
    "\n",
    "features['tot_sales_amt_quan']= features.tot_sales.apply(lambda x : fnc(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98400.0\n",
      "149000.0\n",
      "199000.0\n",
      "261250.0\n",
      "330600.0\n",
      "416000.0\n",
      "534850.0\n",
      "736250.0\n",
      "1180000.0\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(0.1,1,0.1):\n",
    "    print(features.max_sales_amt.quantile(q=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10분위별로 가격선호도 조사 (1 ~ 10 범위)\n",
    "def fnc2(m) : \n",
    "    if m < 98400.0 :\n",
    "        return 1\n",
    "    elif m < 149000.0 :\n",
    "        return 2 \n",
    "    elif m < 199000.0 :\n",
    "        return 3 \n",
    "    elif m < 261250.0 :\n",
    "        return 4 \n",
    "    elif m < 330600.0 :\n",
    "        return 5 \n",
    "    elif m < 416000.0 :\n",
    "        return 6 \n",
    "    elif m < 534850.0 :\n",
    "        return 7 \n",
    "    elif m < 736250.0 :\n",
    "        return 8\n",
    "    elif m < 1180000.0 :\n",
    "        return 9 \n",
    "    else:\n",
    "        return 10\n",
    "\n",
    "features['max_amt_quan']= features.max_sales_amt.apply(lambda x : fnc2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#아동상품 구매건수\n",
    "f = data[data.tot_amt > 0].groupby('custid')['part_nm'].agg([('baby_sales', lambda x: list(x).count('아동')+list(x).count('케주얼,구두,아동')+list(x).count('아동문화')+list(x).count('아동,스포츠'))]).reset_index()\n",
    "features = pd.merge(features, f, on='custid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['baby_sales_freq']=features['baby_sales']/features.sales_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#할부 총합\n",
    "sales_installment=data.groupby('custid')['inst_mon'].agg('sum').reset_index()\n",
    "features = pd.merge(features, sales_installment, on='custid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "팀 별 구매 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['team_nm'] = data['team_nm'].map({'잡화가용팀':'aa', '의류패션팀':'bb', '식품팀':'cb', '인터넷백화점':'dd'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = data.groupby(['custid', 'team_nm'])['custid'].count().unstack()\n",
    "f = f.fillna(0)\n",
    "f.columns = ['Team_'+ str(column) for column in f.columns]\n",
    "f = f.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.merge(features, f, on = 'custid')\n",
    "\n",
    "features['food_rate']=features.Team_cb/(features.Team_aa+features.Team_bb+features.Team_cb+features.Team_dd)\n",
    "features['cloth_rate']=features.Team_bb/(features.Team_aa+features.Team_bb+features.Team_cb+features.Team_dd)\n",
    "features['stuff_rate']=features.Team_aa/(features.Team_aa+features.Team_bb+features.Team_cb+features.Team_dd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "품목 별 구매 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_list=['조리식품','축산물','농산물','수산물']\n",
    "t1 = data[data.net_amt>0].query(\"pc_nm == @food_list\").groupby('custid')['net_amt'].agg([('food_sales', np.sum)])\n",
    "features= pd.merge(features,t1, on = 'custid', how='left').fillna(0)\n",
    "features['food_sales_rate'] = features['food_sales'] / features['tot_sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "st=f\"(net_amt>0) and (pc_nm=='화장품')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = data.query(st).groupby('custid')['net_amt'].agg([('face_goods_sales', np.sum)])\n",
    "features= pd.merge(features,t2, on = 'custid', how='left').fillna(0)\n",
    "features['face_sales_rate'] = features['face_goods_sales'] / features['tot_sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = data.query(st).groupby('custid')['net_amt'].agg([('face_goods_freq', np.size)])\n",
    "features= pd.merge(features,t2, on = 'custid', how='left').fillna(0)\n",
    "features['face_goods_rate'] = features['face_goods_freq'] / features['sales_freq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pc_nm'] = data['pc_nm'].str.replace('디자이너부띠끄','디자이너부띠크')\n",
    "data['pc_nm'] = data['pc_nm'].str.replace('디자이너부틱','디자이너부띠크')\n",
    "data['pc_nm'] = data['pc_nm'].str.replace('트래디셔널','트레디셔널')\n",
    "data['pc_nm'] = data['pc_nm'].str.replace('침구/수예','침구수예')\n",
    "data['pc_nm'] = data['pc_nm'].str.replace('침구,수예','침구수예')\n",
    "data['pc_nm'] = data['pc_nm'].str.replace('피혁A','피혁')\n",
    "data['pc_nm'] = data['pc_nm'].str.replace('피혁B','피혁')\n",
    "data['pc_nm'] = data['pc_nm'].str.replace('캐릭터케쥬얼','캐릭터캐쥬얼')\n",
    "data['pc_nm'] = data['pc_nm'].str.replace('캐릭터캐주얼','캐릭터캐쥬얼')\n",
    "data['pc_nm'] = data['pc_nm'].str.replace('진캐주얼','진캐쥬얼')\n",
    "data['pc_nm'] = data['pc_nm'].str.replace('진케주얼','진캐쥬얼')\n",
    "data['pc_nm'] = data['pc_nm'].str.replace('로얄부틱','로얄부띠크')\n",
    "data['pc_nm'] = data['pc_nm'].str.replace('로얄부띠끄','로얄부띠크')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "선호 브랜드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "twn_trend_brd = df.query(\"age>=20 & age<30\").pc_nm.value_counts().index[1:11].tolist()\n",
    "thr_trend_brd = df.query(\"age>=30 & age<40\").pc_nm.value_counts().index[1:11].tolist()\n",
    "for_trend_brd = df.query(\"age>=40 & age<50\").pc_nm.value_counts().index[1:11].tolist()\n",
    "fiv_trend_brd = df.query(\"age>=50 & age<60\").pc_nm.value_counts().index[1:11].tolist()\n",
    "six_trend_brd = ['미확인pc', '농산물', '식기', '일반식품', '골프', '아동', '수산물', '로얄부띠끄', '스포츠', '수입명품']\n",
    "\n",
    "def map_twn_pc(x):\n",
    "    if x == twn_trend_brd[0]:\n",
    "        x=10\n",
    "    elif x == twn_trend_brd[1]:\n",
    "        x=9\n",
    "    elif x == twn_trend_brd[2]:\n",
    "        x=8\n",
    "    elif x == twn_trend_brd[3]:\n",
    "        x=7\n",
    "    elif x == twn_trend_brd[4]:\n",
    "        x=6\n",
    "    elif x == twn_trend_brd[5]:\n",
    "        x=5\n",
    "    elif x == twn_trend_brd[6]:\n",
    "        x=4\n",
    "    elif x == twn_trend_brd[7]:\n",
    "        x=3\n",
    "    elif x == twn_trend_brd[8]:\n",
    "        x=2\n",
    "    elif x == twn_trend_brd[9]:\n",
    "        x=1\n",
    "    else:\n",
    "        x=0\n",
    "    return x\n",
    "\n",
    "def map_thr_pc(x):\n",
    "    if x == thr_trend_brd[0]:\n",
    "        x=10\n",
    "    elif x == thr_trend_brd[1]:\n",
    "        x=9\n",
    "    elif x == thr_trend_brd[2]:\n",
    "        x=8\n",
    "    elif x == thr_trend_brd[3]:\n",
    "        x=7\n",
    "    elif x == thr_trend_brd[4]:\n",
    "        x=6\n",
    "    elif x == thr_trend_brd[5]:\n",
    "        x=5\n",
    "    elif x == thr_trend_brd[6]:\n",
    "        x=4\n",
    "    elif x == thr_trend_brd[7]:\n",
    "        x=3\n",
    "    elif x == thr_trend_brd[8]:\n",
    "        x=2\n",
    "    elif x == thr_trend_brd[9]:\n",
    "        x=1\n",
    "    else:\n",
    "        x=0\n",
    "    return x\n",
    "\n",
    "def map_for_pc(x):\n",
    "    if x == for_trend_brd[0]:\n",
    "        x=10\n",
    "    elif x == for_trend_brd[1]:\n",
    "        x=9\n",
    "    elif x == for_trend_brd[2]:\n",
    "        x=8\n",
    "    elif x == for_trend_brd[3]:\n",
    "        x=7\n",
    "    elif x == for_trend_brd[4]:\n",
    "        x=6\n",
    "    elif x == for_trend_brd[5]:\n",
    "        x=5\n",
    "    elif x == for_trend_brd[6]:\n",
    "        x=4\n",
    "    elif x == for_trend_brd[7]:\n",
    "        x=3\n",
    "    elif x == for_trend_brd[8]:\n",
    "        x=2\n",
    "    elif x == for_trend_brd[9]:\n",
    "        x=1\n",
    "    else:\n",
    "        x=0\n",
    "    return x\n",
    "\n",
    "def map_fiv_pc(x):\n",
    "    if x == fiv_trend_brd[0]:\n",
    "        x=10\n",
    "    elif x == fiv_trend_brd[1]:\n",
    "        x=9\n",
    "    elif x == fiv_trend_brd[2]:\n",
    "        x=8\n",
    "    elif x == fiv_trend_brd[3]:\n",
    "        x=7\n",
    "    elif x == fiv_trend_brd[4]:\n",
    "        x=6\n",
    "    elif x == fiv_trend_brd[5]:\n",
    "        x=5\n",
    "    elif x == fiv_trend_brd[6]:\n",
    "        x=4\n",
    "    elif x == fiv_trend_brd[7]:\n",
    "        x=3\n",
    "    elif x == fiv_trend_brd[8]:\n",
    "        x=2\n",
    "    elif x == fiv_trend_brd[9]:\n",
    "        x=1\n",
    "    else:\n",
    "        x=0\n",
    "    return x\n",
    "\n",
    "def map_six_pc(x):\n",
    "    if x == six_trend_brd[0]:\n",
    "        x=10\n",
    "    elif x == six_trend_brd[1]:\n",
    "        x=9\n",
    "    elif x == six_trend_brd[2]:\n",
    "        x=8\n",
    "    elif x == six_trend_brd[3]:\n",
    "        x=7\n",
    "    elif x == six_trend_brd[4]:\n",
    "        x=6\n",
    "    elif x == six_trend_brd[5]:\n",
    "        x=5\n",
    "    elif x == six_trend_brd[6]:\n",
    "        x=4\n",
    "    elif x == six_trend_brd[7]:\n",
    "        x=3\n",
    "    elif x == six_trend_brd[8]:\n",
    "        x=2\n",
    "    elif x == six_trend_brd[9]:\n",
    "        x=1\n",
    "    else:\n",
    "        x=0\n",
    "    return x\n",
    "\n",
    "data['twn_pc_point'] = data['pc_nm'].apply(map_twn_pc)\n",
    "data['thr_pc_point'] = data['pc_nm'].apply(map_thr_pc)\n",
    "data['for_pc_point'] = data['pc_nm'].apply(map_for_pc)\n",
    "data['fiv_pc_point'] = data['pc_nm'].apply(map_fiv_pc)\n",
    "data['six_pc_point'] = data['pc_nm'].apply(map_six_pc)\n",
    "\n",
    "twn_pcnm_point=data.groupby('custid')['twn_pc_point'].mean().reset_index()\n",
    "thr_pcnm_point=data.groupby('custid')['thr_pc_point'].mean().reset_index()\n",
    "for_pcnm_point=data.groupby('custid')['for_pc_point'].mean().reset_index()\n",
    "fiv_pcnm_point=data.groupby('custid')['fiv_pc_point'].mean().reset_index()\n",
    "six_pcnm_point=data.groupby('custid')['six_pc_point'].mean().reset_index()\n",
    "\n",
    "features = pd.merge(features, twn_pcnm_point, how='left',on='custid')\n",
    "features = pd.merge(features, thr_pcnm_point, how='left',on='custid')\n",
    "features = pd.merge(features, for_pcnm_point, how='left',on='custid')\n",
    "features = pd.merge(features, fiv_pcnm_point, how='left',on='custid')\n",
    "features = pd.merge(features, six_pcnm_point, how='left',on='custid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = data[data.tot_amt > 0].groupby('custid')['pc_nm'].agg([('young_pc', lambda x: list(x).count('캐릭터정장')+list(x).count('영트랜디')+list(x).count('영캐릭터캐쥬얼')+list(x).count('트랜디 케쥬얼')+list(x).count('캐릭터캐쥬얼')+list(x).count('장신구')+list(x).count('피혁')+list(x).count('섬유')+list(x).count('소품')+list(x).count('핸드백')+list(x).count('(주)현스포츠아쌤'))]).reset_index()\n",
    "features = pd.merge(features, f, on='custid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['20goods_rate']=features['young_pc']/features.sales_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#50대선호\n",
    "\n",
    "f = data[data.tot_amt > 0].groupby('custid')['pc_nm'].agg([('butique_freq', lambda x: list(x).count('로얄부띠크')+list(x).count('디자이너부띠크'))]).reset_index()\n",
    "features = pd.merge(features, f, on='custid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['50goods_rate']=features['butique_freq']/features.sales_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#트랜드 명품 구매비율\n",
    "f = data[data.tot_amt > 0].groupby('custid')['pc_nm'].agg([('expensive_freq', lambda x: list(x).count('명품토탈')+list(x).count('명품시즌'))]).reset_index()\n",
    "features = pd.merge(features, f, on='custid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['20~30expensive_rate']=features['expensive_freq']/features.sales_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한글 오류 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pc_nm'] = data['pc_nm'].map({'화장품':'face_goods', '명품토탈':'expensive_total', '캐릭터정장':'chr_suit',  '수예':'suae', '유아/완구':'baby_wangu', '패션슈즈':'fashoin_shoes',\n",
    "       '가전':'electronic', '영트랜디':'young_trend', '조리식품':'instant_food', '스포츠':'sports', '아동':'baby', '트래디셔널캐쥬얼':'trady_ca', '가전/문화':'ele_cultur', '셔츠':'shirts',\n",
    "       '란제리':'rangeri', '식기':'food_case', '영캐릭터캐쥬얼':'young_chr', '골프':'golf', '캐릭터캐쥬얼':'chr_cha', '엘레강스캐쥬얼':'elegangs', '트레디셔널':'tradi',\n",
    "       '정장':'suit', '일반식품':'normal_food', '아동복':'baby_cloth', '축산물':'pig', '미확인pc':'unknown_pc','농산물':'vegetable','트랜디 케쥬얼':'trendi cha',\n",
    "        '영트렌디':'young_trendi','여성캐주얼':'girl_cha','문화용품':'culture_goods','생활용품':'lsang','문화':'culture','니트,단품,모피':'neat_mofi','섬유잡화':'sumyou_goods','내의란제리':'inrangeri',\n",
    "        '진캐쥬얼':'jean_ca3','여성구두':'girl_shoes','명품시즌':'expensive_season','남여구두':'man_girl','장신구':'jangsingu','수산물':'susanmul','악세사리':'accesory','섬유':'sumyou',\n",
    "        '어덜트캐주얼':'adult_cha','소품':'sopeum','캐주얼':'cha','미씨캐주얼':'messi_cha','영캐주얼':'young_cha','타운란제리':'townrangeri',\n",
    "        '핸드백':'handbag','수입명품':'suip_goods','디자이너부띠크':'designier_beatuty','쇼핑보증':'shooping_bo','미씨캐릭터':'messi_cha5',\n",
    "        '가구':'gagu','캐쥬얼':'cha2','니트/단품':'neat_one','남성잡화':'man_jobs','(주)현스포츠아쌤':'sportsam','침구수예':'chimgu4','패션시즌':'fashion_season',\n",
    "        '수입의류':'suip_cloth','용기보증':'youngi','사이버쇼핑':'cyber_shopping'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#주구매상품분류\n",
    "f = data.groupby('custid')['pc_nm'].agg([\n",
    "    ('main_goods', lambda x: x.mode()[0])\n",
    "]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.merge(f, how='left')\n",
    "features = pd.get_dummies(features, 'main_goods')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_p = data[data.tot_amt>0].groupby('pc_nm')['tot_amt'].agg([\n",
    "    ('main_goods_price', 'mean')\n",
    "]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pd.merge(f, f_p,  left_on = 'main_goods',right_on='pc_nm',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.merge(features, f[['custid','main_goods_price']],  left_on = 'custid',right_on='custid',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_count=data[data.tot_amt>0].groupby(['custid','pc_nm'])['pc_nm'].count().unstack().fillna(0)\n",
    "\n",
    "pc_count.columns = ['pc_size_'+ str(column) for column in pc_count.columns]\n",
    "pc_count = pc_count.reset_index()\n",
    "\n",
    "features = pd.merge(features, pc_count, how='left',on = 'custid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한글 오류 수정2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data['part_nm'] = data['part_nm'].map({'명품잡화':'aaa', '잡화파트':'bbb', '남성의류':'ccc', '골프/유니캐쥬얼':'ddd', '가정용품':'eee', '아동':'fff', '잡화':'ggg',\n",
    "       '영어덜트캐쥬얼':'hhh', '영라이브':'iii', '공산품':'jjj', '스포츠캐쥬얼':'kkk', '여성캐주얼':'lll', '여성정장':'mmm', '케주얼,구두,아동':'nnn', '남성정장스포츠':'ooo', '아동문화':'ppp',\n",
    "       '로얄부띠끄':'qqq', '공산품파트':'rrr', '생식품파트':'sss', '여성의류파트':'ttt', '가정용품파트':'uuu', '아동,스포츠':'vvv', '패션잡화':'www',\n",
    "       '영캐릭터':'abbb', '영플라자':'abbc', '생식품':'abbd', '스포츠캐주얼':'abbe', '여성캐쥬얼':'abbf', '로얄부틱':'abbg','인터넷백화점':'abbh'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#주구매상품분류\n",
    "f = data.groupby('custid')['part_nm'].agg([\n",
    "    ('main_part', lambda x: x.mode()[0])\n",
    "]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.merge(f, how='left')\n",
    "features = pd.get_dummies(features, 'main_part')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_p = data[data.tot_amt>0].groupby('part_nm')['tot_amt'].agg([\n",
    "    ('main_part_price', 'mean')\n",
    "]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pd.merge(f, f_p,  left_on = 'main_part',right_on='part_nm',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.merge(features, f[['custid','main_part_price']],  left_on = 'custid',right_on='custid',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_count=data[data.tot_amt>0].groupby(['custid','part_nm'])['part_nm'].count().unstack().fillna(0)\n",
    "\n",
    "part_count.columns = ['part_size_'+ str(column) for column in part_count.columns]\n",
    "part_count = part_count.reset_index()\n",
    "\n",
    "features = pd.merge(features, part_count, how='left',on = 'custid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#내점 당 구매건수 / 내점당 구매액\n",
    "features['one_sales_freq']=features.sales_freq/features.sales_dayofweek\n",
    "features['one_tot_amt']=features.tot_sales/features.sales_dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "#주구매상품분류\n",
    "data['part_pc'] = data['part_nm'].astype(str) + \"_\" + data['pc_nm'].astype(str)\n",
    "\n",
    "f = data.groupby('custid')['part_pc'].agg([\n",
    "    ('main_part_pc', lambda x: x.mode()[0])\n",
    "]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.merge(f, how='left')\n",
    "features = pd.get_dummies(features, 'main_part_pc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_p = data[data.tot_amt>0].groupby('part_pc')['tot_amt'].agg([\n",
    "    ('main_part_pc_price', 'mean')\n",
    "]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pd.merge(f, f_p,  left_on = 'main_part_pc',right_on='part_pc',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.merge(features, f[['custid','main_part_pc_price']],  left_on = 'custid',right_on='custid',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_count=data[data.tot_amt>0].groupby(['custid','part_pc'])['part_pc'].count().unstack().fillna(0)\n",
    "\n",
    "part_count.columns = ['part_pc_size_'+ str(column) for column in part_count.columns]\n",
    "part_count = part_count.reset_index()\n",
    "\n",
    "features = pd.merge(features, part_count, how='left',on = 'custid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-f6a70e1b91b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m## save\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data.pickle'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'features' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "## save\n",
    "with open('data1.pickle', 'wb') as f:\n",
    "    pickle.dump(features, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame({'custid':trn['custid'].unique()})\n",
    "X_train = pd.merge(X_train, features, how='left',on='custid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>custid</th>\n",
       "      <th>1_stf</th>\n",
       "      <th>9_stf</th>\n",
       "      <th>10_stf</th>\n",
       "      <th>11_stf</th>\n",
       "      <th>12_stf</th>\n",
       "      <th>13_stf</th>\n",
       "      <th>14_stf</th>\n",
       "      <th>15_stf</th>\n",
       "      <th>16_stf</th>\n",
       "      <th>...</th>\n",
       "      <th>part_pc_size_www_nan</th>\n",
       "      <th>part_pc_size_www_rangeri</th>\n",
       "      <th>part_pc_size_www_shooping_bo</th>\n",
       "      <th>part_pc_size_www_suip_cloth</th>\n",
       "      <th>part_pc_size_www_sumyou</th>\n",
       "      <th>part_pc_size_www_unknown_pc</th>\n",
       "      <th>str_nm_tar_mean</th>\n",
       "      <th>pc_nm_tar_mean</th>\n",
       "      <th>part_nm_tar_mean</th>\n",
       "      <th>buyer_nm_tar_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.241627</td>\n",
       "      <td>36.444439</td>\n",
       "      <td>37.824731</td>\n",
       "      <td>36.575116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.192970</td>\n",
       "      <td>37.502349</td>\n",
       "      <td>38.022331</td>\n",
       "      <td>37.687634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.048983</td>\n",
       "      <td>37.968566</td>\n",
       "      <td>37.952269</td>\n",
       "      <td>38.078728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.259086</td>\n",
       "      <td>37.189765</td>\n",
       "      <td>38.056806</td>\n",
       "      <td>36.520959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.220395</td>\n",
       "      <td>38.718581</td>\n",
       "      <td>39.415123</td>\n",
       "      <td>38.202271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21582</th>\n",
       "      <td>29995</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.064942</td>\n",
       "      <td>38.162031</td>\n",
       "      <td>38.552659</td>\n",
       "      <td>37.600597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21583</th>\n",
       "      <td>29996</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.259086</td>\n",
       "      <td>36.297488</td>\n",
       "      <td>35.711526</td>\n",
       "      <td>35.712502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21584</th>\n",
       "      <td>29997</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.979616</td>\n",
       "      <td>38.538867</td>\n",
       "      <td>38.811788</td>\n",
       "      <td>38.745738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21585</th>\n",
       "      <td>29998</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.259086</td>\n",
       "      <td>37.181207</td>\n",
       "      <td>37.415945</td>\n",
       "      <td>37.259960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21586</th>\n",
       "      <td>29999</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.016660</td>\n",
       "      <td>36.639988</td>\n",
       "      <td>36.633405</td>\n",
       "      <td>35.774931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21587 rows × 762 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       custid  1_stf  9_stf  10_stf  11_stf  12_stf  13_stf  14_stf  15_stf  \\\n",
       "0           0      0      0       0       0       2       0       0       0   \n",
       "1           2      0      0       1       1       0       1       1       0   \n",
       "2           3      0      0       0       2       3       2       3       0   \n",
       "3           4      0      0       0       0       0       0       4       0   \n",
       "4           5      0      0       0       8       1       2       0       1   \n",
       "...       ...    ...    ...     ...     ...     ...     ...     ...     ...   \n",
       "21582   29995      0      0       2       1       8       8      12       3   \n",
       "21583   29996      0      0       0       0       4       4       0       0   \n",
       "21584   29997      0      0       0       0       1       3       4       7   \n",
       "21585   29998      0      0       0       0       0       5       2       2   \n",
       "21586   29999      0      0       0       0       0       1       0       0   \n",
       "\n",
       "       16_stf  ...  part_pc_size_www_nan  part_pc_size_www_rangeri  \\\n",
       "0           0  ...                   0.0                       0.0   \n",
       "1           1  ...                   0.0                       0.0   \n",
       "2           0  ...                   0.0                       0.0   \n",
       "3           0  ...                   0.0                       0.0   \n",
       "4           3  ...                   0.0                       0.0   \n",
       "...       ...  ...                   ...                       ...   \n",
       "21582      10  ...                   0.0                       0.0   \n",
       "21583       3  ...                   0.0                       0.0   \n",
       "21584       3  ...                   0.0                       0.0   \n",
       "21585       2  ...                   0.0                       0.0   \n",
       "21586       0  ...                   0.0                       0.0   \n",
       "\n",
       "       part_pc_size_www_shooping_bo  part_pc_size_www_suip_cloth  \\\n",
       "0                               0.0                          0.0   \n",
       "1                               0.0                          0.0   \n",
       "2                               0.0                          0.0   \n",
       "3                               0.0                          0.0   \n",
       "4                               0.0                          0.0   \n",
       "...                             ...                          ...   \n",
       "21582                           0.0                          0.0   \n",
       "21583                           0.0                          0.0   \n",
       "21584                           0.0                          0.0   \n",
       "21585                           0.0                          0.0   \n",
       "21586                           0.0                          0.0   \n",
       "\n",
       "       part_pc_size_www_sumyou  part_pc_size_www_unknown_pc  str_nm_tar_mean  \\\n",
       "0                          0.0                          0.0        39.241627   \n",
       "1                          0.0                          0.0        38.192970   \n",
       "2                          0.0                          0.0        38.048983   \n",
       "3                          0.0                          0.0        38.259086   \n",
       "4                          0.0                          0.0        39.220395   \n",
       "...                        ...                          ...              ...   \n",
       "21582                      0.0                          0.0        39.064942   \n",
       "21583                      0.0                          0.0        38.259086   \n",
       "21584                      0.0                          0.0        38.979616   \n",
       "21585                      0.0                          0.0        38.259086   \n",
       "21586                      0.0                          0.0        38.016660   \n",
       "\n",
       "       pc_nm_tar_mean  part_nm_tar_mean  buyer_nm_tar_mean  \n",
       "0           36.444439         37.824731          36.575116  \n",
       "1           37.502349         38.022331          37.687634  \n",
       "2           37.968566         37.952269          38.078728  \n",
       "3           37.189765         38.056806          36.520959  \n",
       "4           38.718581         39.415123          38.202271  \n",
       "...               ...               ...                ...  \n",
       "21582       38.162031         38.552659          37.600597  \n",
       "21583       36.297488         35.711526          35.712502  \n",
       "21584       38.538867         38.811788          38.745738  \n",
       "21585       37.181207         37.415945          37.259960  \n",
       "21586       36.639988         36.633405          35.774931  \n",
       "\n",
       "[21587 rows x 762 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 만든 데이터 부르기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "with open('train_mean_vector.pickle', 'rb') as f:\n",
    "    train_mean_vector = pickle.load(f)\n",
    "\n",
    "# load\n",
    "with open('test_mean_vector.pickle', 'rb') as f:\n",
    "    test_mean_vector = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean_vector.columns = ['custif'+f'{c+1:03d}' for c in train_mean_vector.columns]\n",
    "test_mean_vector.columns = ['custif'+f'{c+1:03d}' for c in test_mean_vector.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "with open('train_mean_vector1.pickle', 'rb') as f:\n",
    "    train_mean_vector1 = pickle.load(f)\n",
    "\n",
    "# load\n",
    "with open('test_mean_vector1.pickle', 'rb') as f:\n",
    "    test_mean_vector1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean_vector1.columns = ['date'+f'{c+1:03d}' for c in train_mean_vector1.columns]\n",
    "test_mean_vector1.columns = ['date'+f'{c+1:03d}' for c in test_mean_vector1.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "with open('train_mean_vector_good.pickle', 'rb') as f:\n",
    "    train_mean_vector_good = pickle.load(f)\n",
    "\n",
    "# load\n",
    "with open('test_mean_vector_good.pickle', 'rb') as f:\n",
    "    test_mean_vector_good = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean_vector_good.columns = ['good'+f'{c+1:03d}' for c in train_mean_vector_good.columns]\n",
    "test_mean_vector_good.columns = ['good'+f'{c+1:03d}' for c in test_mean_vector_good.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "with open('train_mean_vector_brd.pickle', 'rb') as f:\n",
    "    train_mean_vector_brd = pickle.load(f)\n",
    "\n",
    "# load\n",
    "with open('test_mean_vector_brd.pickle', 'rb') as f:\n",
    "    test_mean_vector_brd = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean_vector_brd.columns = ['brd'+f'{c+1:03d}' for c in train_mean_vector_brd.columns]\n",
    "test_mean_vector_brd.columns = ['brd'+f'{c+1:03d}' for c in test_mean_vector_brd.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "with open('data.pickle', 'rb') as f:\n",
    "    pkl1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=pkl1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_max=[]\n",
    "for i in list(features.columns):\n",
    "    if features[i].max()>50000:\n",
    "        i_max.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_max_min=[]\n",
    "for i in i_max:\n",
    "    if features[i].min()>0:\n",
    "        i_max_min.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[i_max_min]=np.log1p(features[i_max_min])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분할\n",
    "X_train = pd.DataFrame({'custid':trn['custid'].unique()})\n",
    "X_train = pd.merge(X_train, features, how='left',on='custid')\n",
    "\n",
    "y_train = y['age']\n",
    "\n",
    "X_test = pd.DataFrame({'custid':tst['custid'].unique()})\n",
    "X_test = pd.merge(X_test, features, how='left',on='custid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop('custid',axis=1,inplace=True)\n",
    "X_test.drop('custid',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_columns=X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled=pd.DataFrame(X_train_scaled,columns=X_columns)\n",
    "X_test_scaled=pd.DataFrame(X_test_scaled,columns=X_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = pd.concat([X_train_scaled, train_mean_vector], axis=1)\n",
    "\n",
    "X_test_scaled = pd.concat([X_test_scaled, test_mean_vector], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = pd.concat([X_train_scaled, train_mean_vector1], axis=1)\n",
    "\n",
    "X_test_scaled = pd.concat([X_test_scaled, test_mean_vector1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = pd.concat([X_train_scaled, train_mean_vector_good], axis=1)\n",
    "\n",
    "X_test_scaled = pd.concat([X_test_scaled, test_mean_vector_good], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = pd.concat([X_train_scaled, train_mean_vector_brd], axis=1)\n",
    "\n",
    "X_test_scaled = pd.concat([X_test_scaled, test_mean_vector_brd], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21587, 1161), (14380, 1161), (21587,))"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled.shape, X_test_scaled.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "#with open('1_2_3st_merge_train.pickle', 'rb') as f:\n",
    "#    first_merge_train = pickle.load(f)\n",
    "\n",
    "# load\n",
    "#with open('1_2_3st_merge_test.pickle', 'rb') as f:\n",
    "#    first_merge_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first_merge_train=first_merge_train.iloc[:,:-1]\n",
    "#\n",
    "#first_merge_test.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### log1p를 해볼 생각도 해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Select Features*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [02:56<00:00,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38, -8.441933329014343)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 사용할 모델 설정 (속도가 빠른 모델 사용 권장)\n",
    "model = Ridge(random_state=0)\n",
    "\n",
    "# 각 특성과 타깃(class) 사이에 유의한 통계적 관계가 있는지 계산하여 특성을 선택하는 방법 \n",
    "cv_scores = []\n",
    "for p in tqdm(range(5,100,1)):\n",
    "    X_new = SelectPercentile(percentile=p).fit_transform(X_train_scaled, y_train)    #SelectPercentile: 지정된 비율만큼 특성을 선택한다.\n",
    "    cv_score = cross_val_score(model, X_new, y_train, scoring='neg_root_mean_squared_error', cv=5).mean()\n",
    "    cv_scores.append((p,cv_score))\n",
    "\n",
    "# Print the best percentile\n",
    "best_score = cv_scores[np.argmax([score for _, score in cv_scores])]\n",
    "print(best_score)\n",
    "\n",
    "# Plot the performance change with p\n",
    "plt.plot([k for k, _ in cv_scores], [score for _, score in cv_scores])\n",
    "plt.xlabel('Percent of features')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 과적합을 피하기 위해 최적의 p값 주변의 값을 선택하는게 더 나은 결과를 얻을 수 있다. \n",
    "fs = SelectPercentile(percentile=best_score[0]).fit(X_train_scaled, y_train)\n",
    "X_train_select = fs.transform(X_train_scaled)\n",
    "X_test_select = fs.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21587, 440)\n",
      "['10_stf', '11_stf', '12_stf', '13_stf', '14_stf', '15_stf', '16_stf', '17_stf', 'bf_work', 'shop_time', 'floor', 'af_work_rate', 'bf_work_rate', 'af_sales', 'mo_sales', 'refund_freq', 'sales_freq', 'all_freq', 'sales_dayofweek', 'sales_freq_mean', 'sales_import', 'fall_sales_x', 'spring_sales_x', 'summer_sales_x', 'winter_sales_x', 'fall_sales_y', 'spring_sales_y', 'summer_sales_y', 'winter_sales_y', 'summer_sales', 'sales_summer_rate', 'refund_summer_rate', 'wd_sales_rate', 'mon_sale_sm1', 'mon_sale_sm2', 'mon_sale_sm4', 'mon_sale_sm5', 'mon_sale_sm6', 'mon_sale_sm7', 'mon_sale_sm8', 'mon_sale_sm10', 'mon_sale_sm11', 'mon_sale_sm12', 'mon_sale_mn5', 'mon_sale_mn7', '1_mmff', '2_mmff', '4_mmff', '5_mmff', '6_mmff', '7_mmff', '8_mmff', '9_mmff', '10_mmff', '11_mmff', '12_mmff', 'net_amt', 'tot_sales_all', 'mean_amt_all', 'tot_sales', 'max_sales_amt', 'mean_sales_amt', 'tot_refund', 'min_refund_amt', 'mean_refund_amt', 'tot_sales_freq_mean', 'tot_dis_amt', 'mean_dis', 'dis_freq', 'dis_freq_rate', 'inst_freq', 'inst_freq_rate', 'inst_freq_all_rate', 'inst_sum', 'tot_inst_amt', 'str_freq_sin', 'store_bon', 'store_cheon', 'store_mu', 'str_salesbon', 'str_salescheon', 'str_salesmu', 'twn_brd_point', 'thr_brd_point', 'for_brd_point', 'fiv_brd_point', 'b', 'd', 'e', 'f', 'h', 'i', 'l', 'm', 'n', 'p', 'q', 'r', 's', 't', 'x', 'part_unique', 'brd_unique', '12_14_sale', '14_16_sale', '16_18_sale', '9_12_sale', '12_14_sale_ratio', '14_16_sale_ratio', '18_sale_ratio', '9_12_sale_ratio', 'chungdong_sale_freq', 'mean_min_sales', 'chungdong', 'fri_opn', 'thr_opn', 'wen_opn', 'mon_opn', 'sun_opn', 'tue_opn', 'sat_opn_rate', 'fri_pri', 'thr_pri', 'wen_pri', 'mon_pri', 'sun_pri', 'sat_pri', 'tue_pri', 'sat_pri_rate', 'max_min_sales', 'shop_time_mean', 'tot_sales_amt_quan', 'max_amt_quan', 'baby_sales', 'baby_sales_freq', 'inst_mon', 'Team_aa', 'Team_bb', 'Team_cb', 'food_rate', 'cloth_rate', 'stuff_rate', 'food_sales', 'food_sales_rate', 'face_goods_sales', 'face_sales_rate', 'face_goods_rate', 'twn_pc_point', 'thr_pc_point', 'for_pc_point', 'fiv_pc_point', 'six_pc_point', '20goods_rate', 'butique_freq', '50goods_rate', 'main_goods_baby', 'main_goods_face_goods', 'main_goods_golf', 'main_goods_unknown_pc', 'main_goods_vegetable', 'pc_size_baby', 'pc_size_baby_cloth', 'pc_size_culture', 'pc_size_designier_beatuty', 'pc_size_elegangs', 'pc_size_food_case', 'pc_size_golf', 'pc_size_jean_ca3', 'pc_size_lsang', 'pc_size_normal_food', 'pc_size_pig', 'pc_size_shooping_bo', 'pc_size_sports', 'pc_size_suip_goods', 'pc_size_suit', 'pc_size_susanmul', 'pc_size_unknown_pc', 'pc_size_vegetable', 'main_part_abbc', 'main_part_abbd', 'main_part_fff', 'main_part_ppp', 'main_part_www', 'part_size_abbd', 'part_size_ccc', 'part_size_ddd', 'part_size_eee', 'part_size_fff', 'part_size_jjj', 'part_size_kkk', 'part_size_mmm', 'part_size_ooo', 'part_size_ppp', 'part_size_qqq', 'part_size_rrr', 'part_size_sss', 'part_size_ttt', 'part_size_uuu', 'part_size_vvv', 'one_sales_freq', 'one_tot_amt', 'main_part_pc_ppp_baby', 'main_part_pc_vvv_baby', 'part_pc_size_abbd_normal_food', 'part_pc_size_abbd_pig', 'part_pc_size_abbd_susanmul', 'part_pc_size_abbd_unknown_pc', 'part_pc_size_abbd_vegetable', 'part_pc_size_ccc_suit', 'part_pc_size_ddd_golf', 'part_pc_size_ddd_jean_ca3', 'part_pc_size_eee_food_case', 'part_pc_size_fff_baby', 'part_pc_size_jjj_unknown_pc', 'part_pc_size_jjj_vegetable', 'part_pc_size_kkk_golf', 'part_pc_size_kkk_sports', 'part_pc_size_mmm_designier_beatuty', 'part_pc_size_nnn_baby_cloth', 'part_pc_size_ooo_golf', 'part_pc_size_ooo_shirts', 'part_pc_size_ppp_baby', 'part_pc_size_ppp_culture', 'part_pc_size_qqq_nan', 'part_pc_size_rrr_normal_food', 'part_pc_size_rrr_vegetable', 'part_pc_size_sss_susanmul', 'part_pc_size_sss_unknown_pc', 'part_pc_size_sss_vegetable', 'part_pc_size_ttt_designier_beatuty', 'part_pc_size_ttt_nan', 'part_pc_size_uuu_food_case', 'part_pc_size_vvv_baby', 'part_pc_size_vvv_culture', 'bcpp004', 'bcpp008', 'bcpp010', 'bcpp013', 'bcpp014', 'bcpp015', 'bcpp019', 'bcpp020', 'bcpp025', 'bcpp026', 'bcpp027', 'bcpp031', 'bcpp032', 'bcpp033', 'bcpp034', 'bcpp037', 'bcpp038', 'bcpp041', 'bcpp042', 'bcpp047', 'bcpp051', 'bcpp052', 'bcpp053', 'bcpp055', 'bcpp057', 'bcpp059', 'bcpp062', 'bcpp063', 'bcpp064', 'bcpp067', 'bcpp069', 'bcpp070', 'bcpp072', 'bcpp073', 'bcpp074', 'bcpp077', 'bcpp082', 'bcpp084', 'bcpp085', 'bcpp086', 'bcpp087', 'bcpp089', 'bcpp092', 'bcpp093', 'bcpp095', 'bcpp097', 'bcpp100', 'dst001', 'dst006', 'dst008', 'dst011', 'dst014', 'dst024', 'dst025', 'dst027', 'dst037', 'dst039', 'dst042', 'dst043', 'dst046', 'dst047', 'dst052', 'dst053', 'dst057', 'dst058', 'dst062', 'dst064', 'dst065', 'dst068', 'dst074', 'dst076', 'dst082', 'dst084', 'dst086', 'dst088', 'dst089', 'dst092', 'dst093', 'dst095', 'dst097', 'dst099', 'dst100', 'gg001', 'gg002', 'gg004', 'gg007', 'gg008', 'gg009', 'gg013', 'gg014', 'gg015', 'gg016', 'gg020', 'gg021', 'gg023', 'gg024', 'gg027', 'gg029', 'gg030', 'gg031', 'gg032', 'gg039', 'gg040', 'gg043', 'gg046', 'gg047', 'gg048', 'gg052', 'gg053', 'gg054', 'gg055', 'gg057', 'gg058', 'gg059', 'gg060', 'gg061', 'gg062', 'gg063', 'gg064', 'gg065', 'gg067', 'gg069', 'gg071', 'gg072', 'gg074', 'gg076', 'gg077', 'gg078', 'gg079', 'gg081', 'gg082', 'gg083', 'gg087', 'gg088', 'gg091', 'gg095', 'gg096', 'v001', 'v002', 'v003', 'v006', 'v007', 'v008', 'v010', 'v013', 'v014', 'v018', 'v019', 'v020', 'v021', 'v025', 'v026', 'v028', 'v029', 'v031', 'v033', 'v034', 'v035', 'v037', 'v039', 'v040', 'v041', 'v042', 'v043', 'v044', 'v046', 'v047', 'v049', 'v050', 'v051', 'v053', 'v054', 'v055', 'v056', 'v058', 'v059', 'v060', 'v061', 'v065', 'v067', 'v068', 'v069', 'v070', 'v072', 'v073', 'v075', 'v076', 'v077', 'v078', 'v079', 'v080', 'v082', 'v084', 'v086', 'v087', 'v088', 'v089', 'v090', 'v091', 'v092', 'v093', 'v095', 'v096', 'v097', 'v099', 'v100']\n"
     ]
    }
   ],
   "source": [
    "print(X_train_select.shape)\n",
    "print(X_train_nothuman.columns[fs.get_support()].tolist()) #get_support: 선택한 특성을 불린값으로 보여줘서 어떤 특성을 선택했는지 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 앙상블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from tensorflow import keras\n",
    "from scipy.stats.mstats import gmean\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import sys, warnings\n",
    "if not sys.warnoptions: warnings.simplefilter(\"ignore\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM HP tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr = X_train_select\n",
    "target =  y['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15110, 1239), (6477, 1239))"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(ftr, target, test_size=0.3, random_state=0)\n",
    "train_x.shape, valid_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_params = {\n",
    "    'max_depth':(8, 16),\n",
    "    'num_leaves':(24, 64),\n",
    "    'min_child_samples':(10, 200),\n",
    "    'min_child_weight':(1, 50),\n",
    "    'subsample':(0.5, 1),\n",
    "    'colsample_bytree':(0.5, 1),\n",
    "    'max_bin':(10, 500),\n",
    "    'reg_lambda':(0.001, 10),\n",
    "    'reg_alpha':(0.01, 50)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_rmse_eval(max_depth, num_leaves, min_child_samples, min_child_weight, subsample, \n",
    "                colsample_bytree, max_bin, reg_lambda, reg_alpha):\n",
    "    \n",
    "    params = {\n",
    "        \"n_estimators\":2000, \n",
    "        \"learning_rate\":0.02,\n",
    "        'max_depth':int(round(max_depth)),\n",
    "        'num_leaves':int(round(num_leaves)),\n",
    "        'min_child_samples': int(round(min_child_samples)),\n",
    "        'min_child_weight': int(round(min_child_weight)),\n",
    "        'subsample':max(min(subsample, 1), 0),\n",
    "        'colsample_bytree':max(min(colsample_bytree, 1), 0),\n",
    "        'reg_lambda': max(reg_lambda,0),\n",
    "        'reg_alpha': max(reg_alpha, 0)\n",
    "    }\n",
    "    \n",
    "    lgb_model = LGBMRegressor(**params)\n",
    "    lgb_model.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], eval_metric= 'RMSE', verbose= 100, \n",
    "                early_stopping_rounds= 100)\n",
    "    valid_pred = lgb_model.predict(valid_x)\n",
    "    RMSE = np.sqrt(mean_squared_error(valid_y, valid_pred))\n",
    "    \n",
    "    return RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... |  max_bin  | max_depth | min_ch... | min_ch... | num_le... | reg_alpha | reg_la... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.51938\ttraining's l2: 56.5411\tvalid_1's rmse: 8.2567\tvalid_1's l2: 68.1731\n",
      "[200]\ttraining's rmse: 6.67524\ttraining's l2: 44.5588\tvalid_1's rmse: 8.05351\tvalid_1's l2: 64.859\n",
      "[300]\ttraining's rmse: 6.0802\ttraining's l2: 36.9688\tvalid_1's rmse: 8.00891\tvalid_1's l2: 64.1426\n",
      "[400]\ttraining's rmse: 5.57891\ttraining's l2: 31.1243\tvalid_1's rmse: 7.99358\tvalid_1's l2: 63.8973\n",
      "[500]\ttraining's rmse: 5.14195\ttraining's l2: 26.4396\tvalid_1's rmse: 7.98879\tvalid_1's l2: 63.8208\n",
      "Early stopping, best iteration is:\n",
      "[475]\ttraining's rmse: 5.24707\ttraining's l2: 27.5317\tvalid_1's rmse: 7.98717\tvalid_1's l2: 63.7948\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 7.987   \u001b[0m | \u001b[0m 0.7744  \u001b[0m | \u001b[0m 360.4   \u001b[0m | \u001b[0m 12.82   \u001b[0m | \u001b[0m 113.5   \u001b[0m | \u001b[0m 21.76   \u001b[0m | \u001b[0m 49.84   \u001b[0m | \u001b[0m 21.88   \u001b[0m | \u001b[0m 8.918   \u001b[0m | \u001b[0m 0.9818  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.8477\ttraining's l2: 61.5864\tvalid_1's rmse: 8.30424\tvalid_1's l2: 68.9604\n",
      "[200]\ttraining's rmse: 7.21611\ttraining's l2: 52.0723\tvalid_1's rmse: 8.07844\tvalid_1's l2: 65.2612\n",
      "[300]\ttraining's rmse: 6.79891\ttraining's l2: 46.2252\tvalid_1's rmse: 8.02076\tvalid_1's l2: 64.3327\n",
      "[400]\ttraining's rmse: 6.45809\ttraining's l2: 41.7069\tvalid_1's rmse: 7.99608\tvalid_1's l2: 63.9373\n",
      "[500]\ttraining's rmse: 6.15611\ttraining's l2: 37.8977\tvalid_1's rmse: 7.98999\tvalid_1's l2: 63.8399\n",
      "[600]\ttraining's rmse: 5.88122\ttraining's l2: 34.5888\tvalid_1's rmse: 7.98583\tvalid_1's l2: 63.7735\n",
      "[700]\ttraining's rmse: 5.6286\ttraining's l2: 31.6812\tvalid_1's rmse: 7.97972\tvalid_1's l2: 63.6759\n",
      "[800]\ttraining's rmse: 5.3912\ttraining's l2: 29.0651\tvalid_1's rmse: 7.98269\tvalid_1's l2: 63.7233\n",
      "Early stopping, best iteration is:\n",
      "[715]\ttraining's rmse: 5.5918\ttraining's l2: 31.2682\tvalid_1's rmse: 7.97949\tvalid_1's l2: 63.6723\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 7.979   \u001b[0m | \u001b[0m 0.6917  \u001b[0m | \u001b[0m 397.9   \u001b[0m | \u001b[0m 12.23   \u001b[0m | \u001b[0m 117.9   \u001b[0m | \u001b[0m 46.35   \u001b[0m | \u001b[0m 26.84   \u001b[0m | \u001b[0m 4.366   \u001b[0m | \u001b[0m 0.2032  \u001b[0m | \u001b[0m 0.9163  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.48017\ttraining's l2: 55.953\tvalid_1's rmse: 8.26214\tvalid_1's l2: 68.263\n",
      "[200]\ttraining's rmse: 6.61345\ttraining's l2: 43.7377\tvalid_1's rmse: 8.07176\tvalid_1's l2: 65.1533\n",
      "[300]\ttraining's rmse: 5.99383\ttraining's l2: 35.926\tvalid_1's rmse: 8.03104\tvalid_1's l2: 64.4975\n",
      "[400]\ttraining's rmse: 5.48991\ttraining's l2: 30.1391\tvalid_1's rmse: 8.01386\tvalid_1's l2: 64.2219\n",
      "[500]\ttraining's rmse: 5.03938\ttraining's l2: 25.3954\tvalid_1's rmse: 8.008\tvalid_1's l2: 64.1281\n",
      "[600]\ttraining's rmse: 4.64205\ttraining's l2: 21.5487\tvalid_1's rmse: 8.00706\tvalid_1's l2: 64.113\n",
      "Early stopping, best iteration is:\n",
      "[534]\ttraining's rmse: 4.90096\ttraining's l2: 24.0194\tvalid_1's rmse: 8.00516\tvalid_1's l2: 64.0826\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 8.005   \u001b[0m | \u001b[95m 0.8891  \u001b[0m | \u001b[95m 436.3   \u001b[0m | \u001b[95m 15.83   \u001b[0m | \u001b[95m 161.8   \u001b[0m | \u001b[95m 23.61   \u001b[0m | \u001b[95m 55.22   \u001b[0m | \u001b[95m 5.923   \u001b[0m | \u001b[95m 6.4     \u001b[0m | \u001b[95m 0.5717  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.54103\ttraining's l2: 56.8671\tvalid_1's rmse: 8.27046\tvalid_1's l2: 68.4005\n",
      "[200]\ttraining's rmse: 6.71948\ttraining's l2: 45.1514\tvalid_1's rmse: 8.07251\tvalid_1's l2: 65.1654\n",
      "[300]\ttraining's rmse: 6.14975\ttraining's l2: 37.8194\tvalid_1's rmse: 8.02779\tvalid_1's l2: 64.4453\n",
      "[400]\ttraining's rmse: 5.68125\ttraining's l2: 32.2766\tvalid_1's rmse: 8.00997\tvalid_1's l2: 64.1596\n",
      "[500]\ttraining's rmse: 5.27592\ttraining's l2: 27.8353\tvalid_1's rmse: 8.00819\tvalid_1's l2: 64.1312\n",
      "[600]\ttraining's rmse: 4.91072\ttraining's l2: 24.1151\tvalid_1's rmse: 8.00309\tvalid_1's l2: 64.0495\n",
      "[700]\ttraining's rmse: 4.58465\ttraining's l2: 21.019\tvalid_1's rmse: 8.00446\tvalid_1's l2: 64.0714\n",
      "Early stopping, best iteration is:\n",
      "[618]\ttraining's rmse: 4.84853\ttraining's l2: 23.5083\tvalid_1's rmse: 8.00103\tvalid_1's l2: 64.0165\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 8.001   \u001b[0m | \u001b[0m 0.9723  \u001b[0m | \u001b[0m 265.7   \u001b[0m | \u001b[0m 11.32   \u001b[0m | \u001b[0m 60.27   \u001b[0m | \u001b[0m 38.94   \u001b[0m | \u001b[0m 42.25   \u001b[0m | \u001b[0m 28.43   \u001b[0m | \u001b[0m 0.1889  \u001b[0m | \u001b[0m 0.8088  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.63698\ttraining's l2: 58.3234\tvalid_1's rmse: 8.26514\tvalid_1's l2: 68.3126\n",
      "[200]\ttraining's rmse: 6.8813\ttraining's l2: 47.3522\tvalid_1's rmse: 8.0607\tvalid_1's l2: 64.9748\n",
      "[300]\ttraining's rmse: 6.35083\ttraining's l2: 40.333\tvalid_1's rmse: 8.01706\tvalid_1's l2: 64.2732\n",
      "[400]\ttraining's rmse: 5.9037\ttraining's l2: 34.8537\tvalid_1's rmse: 7.99914\tvalid_1's l2: 63.9862\n",
      "[500]\ttraining's rmse: 5.50789\ttraining's l2: 30.3368\tvalid_1's rmse: 7.99045\tvalid_1's l2: 63.8473\n",
      "[600]\ttraining's rmse: 5.15702\ttraining's l2: 26.5948\tvalid_1's rmse: 7.98652\tvalid_1's l2: 63.7845\n",
      "[700]\ttraining's rmse: 4.83746\ttraining's l2: 23.401\tvalid_1's rmse: 7.98505\tvalid_1's l2: 63.761\n",
      "[800]\ttraining's rmse: 4.54502\ttraining's l2: 20.6572\tvalid_1's rmse: 7.98784\tvalid_1's l2: 63.8055\n",
      "Early stopping, best iteration is:\n",
      "[742]\ttraining's rmse: 4.71038\ttraining's l2: 22.1877\tvalid_1's rmse: 7.98413\tvalid_1's l2: 63.7463\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 7.984   \u001b[0m | \u001b[0m 0.806   \u001b[0m | \u001b[0m 312.3   \u001b[0m | \u001b[0m 15.55   \u001b[0m | \u001b[0m 139.5   \u001b[0m | \u001b[0m 18.62   \u001b[0m | \u001b[0m 41.48   \u001b[0m | \u001b[0m 34.88   \u001b[0m | \u001b[0m 0.6032  \u001b[0m | \u001b[0m 0.8334  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.59896\ttraining's l2: 57.7442\tvalid_1's rmse: 8.26344\tvalid_1's l2: 68.2844\n",
      "[200]\ttraining's rmse: 6.86079\ttraining's l2: 47.0704\tvalid_1's rmse: 8.05778\tvalid_1's l2: 64.9279\n",
      "[300]\ttraining's rmse: 6.44227\ttraining's l2: 41.5029\tvalid_1's rmse: 8.00917\tvalid_1's l2: 64.1469\n",
      "[400]\ttraining's rmse: 6.11223\ttraining's l2: 37.3593\tvalid_1's rmse: 7.99498\tvalid_1's l2: 63.9197\n",
      "[500]\ttraining's rmse: 5.81522\ttraining's l2: 33.8168\tvalid_1's rmse: 7.98759\tvalid_1's l2: 63.8015\n",
      "[600]\ttraining's rmse: 5.54113\ttraining's l2: 30.7041\tvalid_1's rmse: 7.98559\tvalid_1's l2: 63.7696\n",
      "[700]\ttraining's rmse: 5.28656\ttraining's l2: 27.9477\tvalid_1's rmse: 7.98403\tvalid_1's l2: 63.7447\n",
      "[800]\ttraining's rmse: 5.05882\ttraining's l2: 25.5916\tvalid_1's rmse: 7.98121\tvalid_1's l2: 63.6998\n",
      "Early stopping, best iteration is:\n",
      "[776]\ttraining's rmse: 5.10609\ttraining's l2: 26.0721\tvalid_1's rmse: 7.97937\tvalid_1's l2: 63.6704\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 7.979   \u001b[0m | \u001b[0m 0.6919  \u001b[0m | \u001b[0m 434.4   \u001b[0m | \u001b[0m 8.987   \u001b[0m | \u001b[0m 179.9   \u001b[0m | \u001b[0m 31.78   \u001b[0m | \u001b[0m 48.95   \u001b[0m | \u001b[0m 6.625   \u001b[0m | \u001b[0m 8.67    \u001b[0m | \u001b[0m 0.9017  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.60772\ttraining's l2: 57.8774\tvalid_1's rmse: 8.27395\tvalid_1's l2: 68.4582\n",
      "[200]\ttraining's rmse: 6.82538\ttraining's l2: 46.5858\tvalid_1's rmse: 8.06312\tvalid_1's l2: 65.0139\n",
      "[300]\ttraining's rmse: 6.27759\ttraining's l2: 39.4081\tvalid_1's rmse: 8.01911\tvalid_1's l2: 64.3062\n",
      "[400]\ttraining's rmse: 5.82422\ttraining's l2: 33.9216\tvalid_1's rmse: 8.00164\tvalid_1's l2: 64.0262\n",
      "[500]\ttraining's rmse: 5.43022\ttraining's l2: 29.4873\tvalid_1's rmse: 7.99985\tvalid_1's l2: 63.9976\n",
      "Early stopping, best iteration is:\n",
      "[448]\ttraining's rmse: 5.62897\ttraining's l2: 31.6853\tvalid_1's rmse: 7.99656\tvalid_1's l2: 63.945\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 7.997   \u001b[0m | \u001b[0m 0.9752  \u001b[0m | \u001b[0m 260.9   \u001b[0m | \u001b[0m 13.76   \u001b[0m | \u001b[0m 63.06   \u001b[0m | \u001b[0m 39.62   \u001b[0m | \u001b[0m 39.29   \u001b[0m | \u001b[0m 31.04   \u001b[0m | \u001b[0m 2.351   \u001b[0m | \u001b[0m 0.6457  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.41928\ttraining's l2: 55.0457\tvalid_1's rmse: 8.25712\tvalid_1's l2: 68.18\n",
      "[200]\ttraining's rmse: 6.51777\ttraining's l2: 42.4813\tvalid_1's rmse: 8.06798\tvalid_1's l2: 65.0923\n",
      "[300]\ttraining's rmse: 5.87519\ttraining's l2: 34.5178\tvalid_1's rmse: 8.02239\tvalid_1's l2: 64.3587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[400]\ttraining's rmse: 5.33858\ttraining's l2: 28.5005\tvalid_1's rmse: 8.00867\tvalid_1's l2: 64.1388\n",
      "[500]\ttraining's rmse: 4.8745\ttraining's l2: 23.7608\tvalid_1's rmse: 8.0054\tvalid_1's l2: 64.0865\n",
      "Early stopping, best iteration is:\n",
      "[496]\ttraining's rmse: 4.89223\ttraining's l2: 23.9339\tvalid_1's rmse: 8.00436\tvalid_1's l2: 64.0698\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 8.004   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 439.4   \u001b[0m | \u001b[0m 16.0    \u001b[0m | \u001b[0m 155.8   \u001b[0m | \u001b[0m 19.67   \u001b[0m | \u001b[0m 59.17   \u001b[0m | \u001b[0m 6.849   \u001b[0m | \u001b[0m 5.148   \u001b[0m | \u001b[0m 0.852   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.48316\ttraining's l2: 55.9976\tvalid_1's rmse: 8.24482\tvalid_1's l2: 67.9771\n",
      "[200]\ttraining's rmse: 6.73863\ttraining's l2: 45.4092\tvalid_1's rmse: 8.0602\tvalid_1's l2: 64.9668\n",
      "[300]\ttraining's rmse: 6.35401\ttraining's l2: 40.3734\tvalid_1's rmse: 8.02152\tvalid_1's l2: 64.3447\n",
      "[400]\ttraining's rmse: 6.05321\ttraining's l2: 36.6414\tvalid_1's rmse: 8.00311\tvalid_1's l2: 64.0498\n",
      "[500]\ttraining's rmse: 5.78167\ttraining's l2: 33.4277\tvalid_1's rmse: 7.99112\tvalid_1's l2: 63.858\n",
      "[600]\ttraining's rmse: 5.5306\ttraining's l2: 30.5876\tvalid_1's rmse: 7.98813\tvalid_1's l2: 63.8102\n",
      "[700]\ttraining's rmse: 5.29573\ttraining's l2: 28.0448\tvalid_1's rmse: 7.98877\tvalid_1's l2: 63.8204\n",
      "Early stopping, best iteration is:\n",
      "[642]\ttraining's rmse: 5.43492\ttraining's l2: 29.5383\tvalid_1's rmse: 7.98637\tvalid_1's l2: 63.7821\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 7.986   \u001b[0m | \u001b[0m 0.7029  \u001b[0m | \u001b[0m 431.2   \u001b[0m | \u001b[0m 8.39    \u001b[0m | \u001b[0m 150.4   \u001b[0m | \u001b[0m 24.14   \u001b[0m | \u001b[0m 54.39   \u001b[0m | \u001b[0m 0.711   \u001b[0m | \u001b[0m 1.682   \u001b[0m | \u001b[0m 0.5028  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.44989\ttraining's l2: 55.5009\tvalid_1's rmse: 8.24449\tvalid_1's l2: 67.9717\n",
      "[200]\ttraining's rmse: 6.58879\ttraining's l2: 43.4122\tvalid_1's rmse: 8.04626\tvalid_1's l2: 64.7423\n",
      "[300]\ttraining's rmse: 6.03749\ttraining's l2: 36.4513\tvalid_1's rmse: 8.00901\tvalid_1's l2: 64.1443\n",
      "[400]\ttraining's rmse: 5.594\ttraining's l2: 31.2929\tvalid_1's rmse: 7.98508\tvalid_1's l2: 63.7614\n",
      "[500]\ttraining's rmse: 5.23325\ttraining's l2: 27.3869\tvalid_1's rmse: 7.98005\tvalid_1's l2: 63.6812\n",
      "[600]\ttraining's rmse: 4.91545\ttraining's l2: 24.1617\tvalid_1's rmse: 7.9775\tvalid_1's l2: 63.6405\n",
      "[700]\ttraining's rmse: 4.62293\ttraining's l2: 21.3715\tvalid_1's rmse: 7.97748\tvalid_1's l2: 63.6402\n",
      "Early stopping, best iteration is:\n",
      "[641]\ttraining's rmse: 4.78911\ttraining's l2: 22.9356\tvalid_1's rmse: 7.97453\tvalid_1's l2: 63.5931\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 7.975   \u001b[0m | \u001b[0m 0.5438  \u001b[0m | \u001b[0m 435.0   \u001b[0m | \u001b[0m 12.15   \u001b[0m | \u001b[0m 164.9   \u001b[0m | \u001b[0m 10.62   \u001b[0m | \u001b[0m 60.77   \u001b[0m | \u001b[0m 5.544   \u001b[0m | \u001b[0m 2.032   \u001b[0m | \u001b[0m 0.6578  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.51728\ttraining's l2: 56.5096\tvalid_1's rmse: 8.26174\tvalid_1's l2: 68.2564\n",
      "[200]\ttraining's rmse: 6.67085\ttraining's l2: 44.5002\tvalid_1's rmse: 8.05934\tvalid_1's l2: 64.9529\n",
      "[300]\ttraining's rmse: 6.07866\ttraining's l2: 36.9502\tvalid_1's rmse: 8.00922\tvalid_1's l2: 64.1476\n",
      "[400]\ttraining's rmse: 5.59085\ttraining's l2: 31.2576\tvalid_1's rmse: 7.99275\tvalid_1's l2: 63.884\n",
      "[500]\ttraining's rmse: 5.16743\ttraining's l2: 26.7023\tvalid_1's rmse: 7.98355\tvalid_1's l2: 63.7371\n",
      "[600]\ttraining's rmse: 4.79038\ttraining's l2: 22.9478\tvalid_1's rmse: 7.97417\tvalid_1's l2: 63.5873\n",
      "[700]\ttraining's rmse: 4.45392\ttraining's l2: 19.8374\tvalid_1's rmse: 7.97457\tvalid_1's l2: 63.5938\n",
      "Early stopping, best iteration is:\n",
      "[614]\ttraining's rmse: 4.74145\ttraining's l2: 22.4814\tvalid_1's rmse: 7.97319\tvalid_1's l2: 63.5717\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 7.973   \u001b[0m | \u001b[0m 0.5925  \u001b[0m | \u001b[0m 265.0   \u001b[0m | \u001b[0m 14.29   \u001b[0m | \u001b[0m 64.35   \u001b[0m | \u001b[0m 39.22   \u001b[0m | \u001b[0m 45.55   \u001b[0m | \u001b[0m 19.14   \u001b[0m | \u001b[0m 3.45    \u001b[0m | \u001b[0m 0.7425  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.66831\ttraining's l2: 58.803\tvalid_1's rmse: 8.2901\tvalid_1's l2: 68.7258\n",
      "[200]\ttraining's rmse: 6.91777\ttraining's l2: 47.8556\tvalid_1's rmse: 8.07965\tvalid_1's l2: 65.2808\n",
      "[300]\ttraining's rmse: 6.39752\ttraining's l2: 40.9282\tvalid_1's rmse: 8.03032\tvalid_1's l2: 64.4861\n",
      "[400]\ttraining's rmse: 5.96825\ttraining's l2: 35.62\tvalid_1's rmse: 8.01171\tvalid_1's l2: 64.1875\n",
      "[500]\ttraining's rmse: 5.59627\ttraining's l2: 31.3182\tvalid_1's rmse: 7.99818\tvalid_1's l2: 63.9709\n",
      "[600]\ttraining's rmse: 5.2633\ttraining's l2: 27.7023\tvalid_1's rmse: 7.9943\tvalid_1's l2: 63.9088\n",
      "[700]\ttraining's rmse: 4.96093\ttraining's l2: 24.6108\tvalid_1's rmse: 7.99298\tvalid_1's l2: 63.8877\n",
      "Early stopping, best iteration is:\n",
      "[695]\ttraining's rmse: 4.97552\ttraining's l2: 24.7558\tvalid_1's rmse: 7.99204\tvalid_1's l2: 63.8727\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 7.992   \u001b[0m | \u001b[0m 0.6289  \u001b[0m | \u001b[0m 267.2   \u001b[0m | \u001b[0m 14.41   \u001b[0m | \u001b[0m 54.27   \u001b[0m | \u001b[0m 37.85   \u001b[0m | \u001b[0m 38.34   \u001b[0m | \u001b[0m 32.07   \u001b[0m | \u001b[0m 6.707   \u001b[0m | \u001b[0m 0.5215  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.653\ttraining's l2: 58.5684\tvalid_1's rmse: 8.28607\tvalid_1's l2: 68.659\n",
      "[200]\ttraining's rmse: 6.8903\ttraining's l2: 47.4762\tvalid_1's rmse: 8.07688\tvalid_1's l2: 65.236\n",
      "[300]\ttraining's rmse: 6.36494\ttraining's l2: 40.5124\tvalid_1's rmse: 8.02839\tvalid_1's l2: 64.4551\n",
      "[400]\ttraining's rmse: 5.9309\ttraining's l2: 35.1756\tvalid_1's rmse: 8.00714\tvalid_1's l2: 64.1142\n",
      "[500]\ttraining's rmse: 5.55238\ttraining's l2: 30.829\tvalid_1's rmse: 7.99719\tvalid_1's l2: 63.9551\n",
      "[600]\ttraining's rmse: 5.21847\ttraining's l2: 27.2324\tvalid_1's rmse: 7.99867\tvalid_1's l2: 63.9787\n",
      "Early stopping, best iteration is:\n",
      "[524]\ttraining's rmse: 5.46991\ttraining's l2: 29.92\tvalid_1's rmse: 7.99551\tvalid_1's l2: 63.9282\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 7.996   \u001b[0m | \u001b[0m 0.9785  \u001b[0m | \u001b[0m 263.2   \u001b[0m | \u001b[0m 12.97   \u001b[0m | \u001b[0m 59.1    \u001b[0m | \u001b[0m 48.8    \u001b[0m | \u001b[0m 36.75   \u001b[0m | \u001b[0m 28.75   \u001b[0m | \u001b[0m 4.737   \u001b[0m | \u001b[0m 0.7619  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.64002\ttraining's l2: 58.3699\tvalid_1's rmse: 8.2794\tvalid_1's l2: 68.5484\n",
      "[200]\ttraining's rmse: 6.88332\ttraining's l2: 47.3801\tvalid_1's rmse: 8.07212\tvalid_1's l2: 65.1591\n",
      "[300]\ttraining's rmse: 6.3534\ttraining's l2: 40.3657\tvalid_1's rmse: 8.02639\tvalid_1's l2: 64.4229\n",
      "[400]\ttraining's rmse: 5.9185\ttraining's l2: 35.0286\tvalid_1's rmse: 8.01065\tvalid_1's l2: 64.1704\n",
      "[500]\ttraining's rmse: 5.53771\ttraining's l2: 30.6662\tvalid_1's rmse: 8.00105\tvalid_1's l2: 64.0169\n",
      "[600]\ttraining's rmse: 5.19986\ttraining's l2: 27.0386\tvalid_1's rmse: 7.99684\tvalid_1's l2: 63.9494\n",
      "Early stopping, best iteration is:\n",
      "[584]\ttraining's rmse: 5.25152\ttraining's l2: 27.5784\tvalid_1's rmse: 7.99629\tvalid_1's l2: 63.9406\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 7.996   \u001b[0m | \u001b[0m 0.9835  \u001b[0m | \u001b[0m 263.9   \u001b[0m | \u001b[0m 15.77   \u001b[0m | \u001b[0m 64.85   \u001b[0m | \u001b[0m 40.3    \u001b[0m | \u001b[0m 37.32   \u001b[0m | \u001b[0m 34.37   \u001b[0m | \u001b[0m 1.043   \u001b[0m | \u001b[0m 0.932   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.44148\ttraining's l2: 55.3756\tvalid_1's rmse: 8.25304\tvalid_1's l2: 68.1126\n",
      "[200]\ttraining's rmse: 6.63844\ttraining's l2: 44.0689\tvalid_1's rmse: 8.06118\tvalid_1's l2: 64.9827\n",
      "[300]\ttraining's rmse: 6.21278\ttraining's l2: 38.5987\tvalid_1's rmse: 8.02604\tvalid_1's l2: 64.4173\n",
      "[400]\ttraining's rmse: 5.87252\ttraining's l2: 34.4865\tvalid_1's rmse: 8.00706\tvalid_1's l2: 64.1131\n",
      "[500]\ttraining's rmse: 5.53107\ttraining's l2: 30.5928\tvalid_1's rmse: 7.9979\tvalid_1's l2: 63.9663\n",
      "[600]\ttraining's rmse: 5.23467\ttraining's l2: 27.4017\tvalid_1's rmse: 7.99617\tvalid_1's l2: 63.9387\n",
      "Early stopping, best iteration is:\n",
      "[538]\ttraining's rmse: 5.41818\ttraining's l2: 29.3566\tvalid_1's rmse: 7.99441\tvalid_1's l2: 63.9107\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 7.994   \u001b[0m | \u001b[0m 0.8776  \u001b[0m | \u001b[0m 443.6   \u001b[0m | \u001b[0m 8.955   \u001b[0m | \u001b[0m 153.7   \u001b[0m | \u001b[0m 21.51   \u001b[0m | \u001b[0m 61.16   \u001b[0m | \u001b[0m 7.411   \u001b[0m | \u001b[0m 7.877   \u001b[0m | \u001b[0m 0.9169  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.44139\ttraining's l2: 55.3743\tvalid_1's rmse: 8.242\tvalid_1's l2: 67.9305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\ttraining's rmse: 6.55208\ttraining's l2: 42.9298\tvalid_1's rmse: 8.04675\tvalid_1's l2: 64.7501\n",
      "[300]\ttraining's rmse: 5.92231\ttraining's l2: 35.0737\tvalid_1's rmse: 8.00053\tvalid_1's l2: 64.0085\n",
      "[400]\ttraining's rmse: 5.40092\ttraining's l2: 29.17\tvalid_1's rmse: 7.98885\tvalid_1's l2: 63.8217\n",
      "[500]\ttraining's rmse: 4.95548\ttraining's l2: 24.5568\tvalid_1's rmse: 7.98353\tvalid_1's l2: 63.7367\n",
      "[600]\ttraining's rmse: 4.57504\ttraining's l2: 20.931\tvalid_1's rmse: 7.98452\tvalid_1's l2: 63.7525\n",
      "[700]\ttraining's rmse: 4.22719\ttraining's l2: 17.8692\tvalid_1's rmse: 7.98236\tvalid_1's l2: 63.7181\n",
      "[800]\ttraining's rmse: 3.92203\ttraining's l2: 15.3823\tvalid_1's rmse: 7.98375\tvalid_1's l2: 63.7402\n",
      "Early stopping, best iteration is:\n",
      "[707]\ttraining's rmse: 4.2043\ttraining's l2: 17.6761\tvalid_1's rmse: 7.98105\tvalid_1's l2: 63.6971\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 7.981   \u001b[0m | \u001b[0m 0.6926  \u001b[0m | \u001b[0m 443.6   \u001b[0m | \u001b[0m 15.97   \u001b[0m | \u001b[0m 156.2   \u001b[0m | \u001b[0m 22.23   \u001b[0m | \u001b[0m 56.56   \u001b[0m | \u001b[0m 0.1986  \u001b[0m | \u001b[0m 2.602   \u001b[0m | \u001b[0m 0.6324  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.51205\ttraining's l2: 56.4309\tvalid_1's rmse: 8.2599\tvalid_1's l2: 68.226\n",
      "[200]\ttraining's rmse: 6.66501\ttraining's l2: 44.4223\tvalid_1's rmse: 8.06879\tvalid_1's l2: 65.1053\n",
      "[300]\ttraining's rmse: 6.06793\ttraining's l2: 36.8198\tvalid_1's rmse: 8.02009\tvalid_1's l2: 64.3218\n",
      "[400]\ttraining's rmse: 5.58724\ttraining's l2: 31.2173\tvalid_1's rmse: 8.00203\tvalid_1's l2: 64.0325\n",
      "[500]\ttraining's rmse: 5.15013\ttraining's l2: 26.5239\tvalid_1's rmse: 8.00175\tvalid_1's l2: 64.0281\n",
      "Early stopping, best iteration is:\n",
      "[430]\ttraining's rmse: 5.44776\ttraining's l2: 29.678\tvalid_1's rmse: 7.9986\tvalid_1's l2: 63.9777\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 7.999   \u001b[0m | \u001b[0m 0.8049  \u001b[0m | \u001b[0m 433.0   \u001b[0m | \u001b[0m 13.74   \u001b[0m | \u001b[0m 160.0   \u001b[0m | \u001b[0m 20.51   \u001b[0m | \u001b[0m 52.54   \u001b[0m | \u001b[0m 13.09   \u001b[0m | \u001b[0m 6.148   \u001b[0m | \u001b[0m 0.901   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.48846\ttraining's l2: 56.0771\tvalid_1's rmse: 8.26199\tvalid_1's l2: 68.2605\n",
      "[200]\ttraining's rmse: 6.62552\ttraining's l2: 43.8975\tvalid_1's rmse: 8.05921\tvalid_1's l2: 64.9509\n",
      "[300]\ttraining's rmse: 6.01534\ttraining's l2: 36.1843\tvalid_1's rmse: 8.0163\tvalid_1's l2: 64.261\n",
      "[400]\ttraining's rmse: 5.50988\ttraining's l2: 30.3587\tvalid_1's rmse: 7.99724\tvalid_1's l2: 63.9558\n",
      "[500]\ttraining's rmse: 5.07216\ttraining's l2: 25.7268\tvalid_1's rmse: 7.99045\tvalid_1's l2: 63.8473\n",
      "[600]\ttraining's rmse: 4.68394\ttraining's l2: 21.9393\tvalid_1's rmse: 7.98806\tvalid_1's l2: 63.8091\n",
      "[700]\ttraining's rmse: 4.34346\ttraining's l2: 18.8656\tvalid_1's rmse: 7.98635\tvalid_1's l2: 63.7817\n",
      "Early stopping, best iteration is:\n",
      "[649]\ttraining's rmse: 4.51201\ttraining's l2: 20.3582\tvalid_1's rmse: 7.98491\tvalid_1's l2: 63.7587\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 7.985   \u001b[0m | \u001b[0m 0.8571  \u001b[0m | \u001b[0m 266.7   \u001b[0m | \u001b[0m 14.59   \u001b[0m | \u001b[0m 59.39   \u001b[0m | \u001b[0m 36.4    \u001b[0m | \u001b[0m 47.86   \u001b[0m | \u001b[0m 33.67   \u001b[0m | \u001b[0m 5.514   \u001b[0m | \u001b[0m 0.7402  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.67798\ttraining's l2: 58.9514\tvalid_1's rmse: 8.28356\tvalid_1's l2: 68.6174\n",
      "[200]\ttraining's rmse: 6.93824\ttraining's l2: 48.1391\tvalid_1's rmse: 8.07067\tvalid_1's l2: 65.1357\n",
      "[300]\ttraining's rmse: 6.43775\ttraining's l2: 41.4446\tvalid_1's rmse: 8.01975\tvalid_1's l2: 64.3164\n",
      "[400]\ttraining's rmse: 6.02435\ttraining's l2: 36.2928\tvalid_1's rmse: 8.00141\tvalid_1's l2: 64.0226\n",
      "[500]\ttraining's rmse: 5.65873\ttraining's l2: 32.0212\tvalid_1's rmse: 7.98543\tvalid_1's l2: 63.7672\n",
      "[600]\ttraining's rmse: 5.32916\ttraining's l2: 28.4\tvalid_1's rmse: 7.97882\tvalid_1's l2: 63.6615\n",
      "[700]\ttraining's rmse: 5.03273\ttraining's l2: 25.3284\tvalid_1's rmse: 7.97876\tvalid_1's l2: 63.6607\n",
      "Early stopping, best iteration is:\n",
      "[645]\ttraining's rmse: 5.19146\ttraining's l2: 26.9512\tvalid_1's rmse: 7.97737\tvalid_1's l2: 63.6384\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 7.977   \u001b[0m | \u001b[0m 0.783   \u001b[0m | \u001b[0m 269.4   \u001b[0m | \u001b[0m 10.38   \u001b[0m | \u001b[0m 61.73   \u001b[0m | \u001b[0m 42.31   \u001b[0m | \u001b[0m 36.2    \u001b[0m | \u001b[0m 30.57   \u001b[0m | \u001b[0m 4.447   \u001b[0m | \u001b[0m 0.9378  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.53879\ttraining's l2: 56.8334\tvalid_1's rmse: 8.26354\tvalid_1's l2: 68.2861\n",
      "[200]\ttraining's rmse: 6.75431\ttraining's l2: 45.6207\tvalid_1's rmse: 8.06406\tvalid_1's l2: 65.0291\n",
      "[300]\ttraining's rmse: 6.31431\ttraining's l2: 39.8706\tvalid_1's rmse: 8.01931\tvalid_1's l2: 64.3093\n",
      "[400]\ttraining's rmse: 5.9697\ttraining's l2: 35.6374\tvalid_1's rmse: 7.99965\tvalid_1's l2: 63.9943\n",
      "[500]\ttraining's rmse: 5.64484\ttraining's l2: 31.8642\tvalid_1's rmse: 7.98984\tvalid_1's l2: 63.8375\n",
      "[600]\ttraining's rmse: 5.34516\ttraining's l2: 28.5708\tvalid_1's rmse: 7.98913\tvalid_1's l2: 63.8263\n",
      "[700]\ttraining's rmse: 5.0676\ttraining's l2: 25.6805\tvalid_1's rmse: 7.98381\tvalid_1's l2: 63.7412\n",
      "Early stopping, best iteration is:\n",
      "[699]\ttraining's rmse: 5.07033\ttraining's l2: 25.7082\tvalid_1's rmse: 7.98349\tvalid_1's l2: 63.7361\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 7.983   \u001b[0m | \u001b[0m 0.6791  \u001b[0m | \u001b[0m 437.5   \u001b[0m | \u001b[0m 9.159   \u001b[0m | \u001b[0m 154.1   \u001b[0m | \u001b[0m 21.2    \u001b[0m | \u001b[0m 52.63   \u001b[0m | \u001b[0m 14.1    \u001b[0m | \u001b[0m 9.155   \u001b[0m | \u001b[0m 0.9433  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.50027\ttraining's l2: 56.2541\tvalid_1's rmse: 8.26929\tvalid_1's l2: 68.3812\n",
      "[200]\ttraining's rmse: 6.64756\ttraining's l2: 44.1901\tvalid_1's rmse: 8.06828\tvalid_1's l2: 65.0971\n",
      "[300]\ttraining's rmse: 6.05863\ttraining's l2: 36.707\tvalid_1's rmse: 8.01894\tvalid_1's l2: 64.3035\n",
      "[400]\ttraining's rmse: 5.56704\ttraining's l2: 30.9919\tvalid_1's rmse: 8.00625\tvalid_1's l2: 64.1\n",
      "[500]\ttraining's rmse: 5.12682\ttraining's l2: 26.2843\tvalid_1's rmse: 8.0058\tvalid_1's l2: 64.0929\n",
      "[600]\ttraining's rmse: 4.74968\ttraining's l2: 22.5594\tvalid_1's rmse: 8.00479\tvalid_1's l2: 64.0767\n",
      "Early stopping, best iteration is:\n",
      "[557]\ttraining's rmse: 4.89779\ttraining's l2: 23.9883\tvalid_1's rmse: 8.00254\tvalid_1's l2: 64.0406\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 8.003   \u001b[0m | \u001b[0m 0.9695  \u001b[0m | \u001b[0m 444.2   \u001b[0m | \u001b[0m 14.4    \u001b[0m | \u001b[0m 159.9   \u001b[0m | \u001b[0m 20.68   \u001b[0m | \u001b[0m 52.76   \u001b[0m | \u001b[0m 8.617   \u001b[0m | \u001b[0m 7.89    \u001b[0m | \u001b[0m 0.9558  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.46266\ttraining's l2: 55.6912\tvalid_1's rmse: 8.259\tvalid_1's l2: 68.2111\n",
      "[200]\ttraining's rmse: 6.59382\ttraining's l2: 43.4785\tvalid_1's rmse: 8.06901\tvalid_1's l2: 65.109\n",
      "[300]\ttraining's rmse: 5.99179\ttraining's l2: 35.9015\tvalid_1's rmse: 8.02204\tvalid_1's l2: 64.3532\n",
      "[400]\ttraining's rmse: 5.51191\ttraining's l2: 30.3812\tvalid_1's rmse: 8.00849\tvalid_1's l2: 64.136\n",
      "[500]\ttraining's rmse: 5.08305\ttraining's l2: 25.8374\tvalid_1's rmse: 8.00004\tvalid_1's l2: 64.0007\n",
      "[600]\ttraining's rmse: 4.72803\ttraining's l2: 22.3543\tvalid_1's rmse: 7.9961\tvalid_1's l2: 63.9376\n",
      "[700]\ttraining's rmse: 4.39663\ttraining's l2: 19.3304\tvalid_1's rmse: 8.00009\tvalid_1's l2: 64.0014\n",
      "Early stopping, best iteration is:\n",
      "[604]\ttraining's rmse: 4.71381\ttraining's l2: 22.22\tvalid_1's rmse: 7.99565\tvalid_1's l2: 63.9304\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 7.996   \u001b[0m | \u001b[0m 0.9423  \u001b[0m | \u001b[0m 432.9   \u001b[0m | \u001b[0m 13.85   \u001b[0m | \u001b[0m 166.8   \u001b[0m | \u001b[0m 23.29   \u001b[0m | \u001b[0m 55.58   \u001b[0m | \u001b[0m 5.256   \u001b[0m | \u001b[0m 4.266   \u001b[0m | \u001b[0m 0.67    \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.55146\ttraining's l2: 57.0246\tvalid_1's rmse: 8.2652\tvalid_1's l2: 68.3135\n",
      "[200]\ttraining's rmse: 6.735\ttraining's l2: 45.3603\tvalid_1's rmse: 8.05949\tvalid_1's l2: 64.9554\n",
      "[300]\ttraining's rmse: 6.16113\ttraining's l2: 37.9595\tvalid_1's rmse: 8.00658\tvalid_1's l2: 64.1053\n",
      "[400]\ttraining's rmse: 5.69662\ttraining's l2: 32.4515\tvalid_1's rmse: 7.9915\tvalid_1's l2: 63.8641\n",
      "[500]\ttraining's rmse: 5.29211\ttraining's l2: 28.0064\tvalid_1's rmse: 7.98363\tvalid_1's l2: 63.7384\n",
      "[600]\ttraining's rmse: 4.93586\ttraining's l2: 24.3627\tvalid_1's rmse: 7.97823\tvalid_1's l2: 63.6522\n",
      "[700]\ttraining's rmse: 4.61343\ttraining's l2: 21.2838\tvalid_1's rmse: 7.97686\tvalid_1's l2: 63.6302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[800]\ttraining's rmse: 4.33513\ttraining's l2: 18.7934\tvalid_1's rmse: 7.97809\tvalid_1's l2: 63.6499\n",
      "Early stopping, best iteration is:\n",
      "[731]\ttraining's rmse: 4.52072\ttraining's l2: 20.4369\tvalid_1's rmse: 7.97543\tvalid_1's l2: 63.6074\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 7.975   \u001b[0m | \u001b[0m 0.5971  \u001b[0m | \u001b[0m 447.9   \u001b[0m | \u001b[0m 13.87   \u001b[0m | \u001b[0m 168.5   \u001b[0m | \u001b[0m 19.35   \u001b[0m | \u001b[0m 51.06   \u001b[0m | \u001b[0m 4.679   \u001b[0m | \u001b[0m 6.172   \u001b[0m | \u001b[0m 0.6255  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.59359\ttraining's l2: 57.6626\tvalid_1's rmse: 8.27085\tvalid_1's l2: 68.4069\n",
      "[200]\ttraining's rmse: 6.80535\ttraining's l2: 46.3128\tvalid_1's rmse: 8.07609\tvalid_1's l2: 65.2233\n",
      "[300]\ttraining's rmse: 6.25342\ttraining's l2: 39.1053\tvalid_1's rmse: 8.0246\tvalid_1's l2: 64.3941\n",
      "[400]\ttraining's rmse: 5.79807\ttraining's l2: 33.6177\tvalid_1's rmse: 8.00542\tvalid_1's l2: 64.0867\n",
      "[500]\ttraining's rmse: 5.40227\ttraining's l2: 29.1845\tvalid_1's rmse: 7.99738\tvalid_1's l2: 63.9582\n",
      "[600]\ttraining's rmse: 5.05297\ttraining's l2: 25.5325\tvalid_1's rmse: 7.99684\tvalid_1's l2: 63.9494\n",
      "[700]\ttraining's rmse: 4.73703\ttraining's l2: 22.4394\tvalid_1's rmse: 7.99594\tvalid_1's l2: 63.935\n",
      "[800]\ttraining's rmse: 4.44575\ttraining's l2: 19.7647\tvalid_1's rmse: 7.99484\tvalid_1's l2: 63.9175\n",
      "Early stopping, best iteration is:\n",
      "[758]\ttraining's rmse: 4.56531\ttraining's l2: 20.8421\tvalid_1's rmse: 7.99326\tvalid_1's l2: 63.8921\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m 7.993   \u001b[0m | \u001b[0m 0.8864  \u001b[0m | \u001b[0m 257.0   \u001b[0m | \u001b[0m 11.86   \u001b[0m | \u001b[0m 56.6    \u001b[0m | \u001b[0m 39.88   \u001b[0m | \u001b[0m 39.99   \u001b[0m | \u001b[0m 30.69   \u001b[0m | \u001b[0m 3.682   \u001b[0m | \u001b[0m 0.8725  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.45943\ttraining's l2: 55.6431\tvalid_1's rmse: 8.25513\tvalid_1's l2: 68.1472\n",
      "[200]\ttraining's rmse: 6.57305\ttraining's l2: 43.205\tvalid_1's rmse: 8.07126\tvalid_1's l2: 65.1453\n",
      "[300]\ttraining's rmse: 5.95178\ttraining's l2: 35.4237\tvalid_1's rmse: 8.02826\tvalid_1's l2: 64.453\n",
      "[400]\ttraining's rmse: 5.4309\ttraining's l2: 29.4946\tvalid_1's rmse: 8.01646\tvalid_1's l2: 64.2636\n",
      "[500]\ttraining's rmse: 4.9809\ttraining's l2: 24.8094\tvalid_1's rmse: 8.01251\tvalid_1's l2: 64.2003\n",
      "[600]\ttraining's rmse: 4.58232\ttraining's l2: 20.9977\tvalid_1's rmse: 8.00832\tvalid_1's l2: 64.1332\n",
      "[700]\ttraining's rmse: 4.24599\ttraining's l2: 18.0284\tvalid_1's rmse: 8.00652\tvalid_1's l2: 64.1044\n",
      "Early stopping, best iteration is:\n",
      "[628]\ttraining's rmse: 4.47866\ttraining's l2: 20.0584\tvalid_1's rmse: 8.00495\tvalid_1's l2: 64.0792\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 8.005   \u001b[0m | \u001b[0m 0.8504  \u001b[0m | \u001b[0m 433.1   \u001b[0m | \u001b[0m 15.39   \u001b[0m | \u001b[0m 154.8   \u001b[0m | \u001b[0m 16.52   \u001b[0m | \u001b[0m 57.73   \u001b[0m | \u001b[0m 8.9     \u001b[0m | \u001b[0m 9.122   \u001b[0m | \u001b[0m 0.8634  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.48035\ttraining's l2: 55.9556\tvalid_1's rmse: 8.24332\tvalid_1's l2: 67.9523\n",
      "[200]\ttraining's rmse: 6.6695\ttraining's l2: 44.4822\tvalid_1's rmse: 8.05555\tvalid_1's l2: 64.8919\n",
      "[300]\ttraining's rmse: 6.18384\ttraining's l2: 38.2399\tvalid_1's rmse: 8.01884\tvalid_1's l2: 64.3017\n",
      "[400]\ttraining's rmse: 5.79401\ttraining's l2: 33.5705\tvalid_1's rmse: 8.00178\tvalid_1's l2: 64.0284\n",
      "[500]\ttraining's rmse: 5.42741\ttraining's l2: 29.4568\tvalid_1's rmse: 7.99261\tvalid_1's l2: 63.8818\n",
      "[600]\ttraining's rmse: 5.11126\ttraining's l2: 26.1249\tvalid_1's rmse: 7.9874\tvalid_1's l2: 63.7986\n",
      "[700]\ttraining's rmse: 4.81226\ttraining's l2: 23.1578\tvalid_1's rmse: 7.98671\tvalid_1's l2: 63.7876\n",
      "[800]\ttraining's rmse: 4.55676\ttraining's l2: 20.7641\tvalid_1's rmse: 7.98295\tvalid_1's l2: 63.7275\n",
      "Early stopping, best iteration is:\n",
      "[785]\ttraining's rmse: 4.59743\ttraining's l2: 21.1364\tvalid_1's rmse: 7.98248\tvalid_1's l2: 63.72\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m 7.982   \u001b[0m | \u001b[0m 0.8482  \u001b[0m | \u001b[0m 437.6   \u001b[0m | \u001b[0m 10.24   \u001b[0m | \u001b[0m 159.4   \u001b[0m | \u001b[0m 24.54   \u001b[0m | \u001b[0m 52.7    \u001b[0m | \u001b[0m 13.4    \u001b[0m | \u001b[0m 0.5057  \u001b[0m | \u001b[0m 0.8552  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.44368\ttraining's l2: 55.4084\tvalid_1's rmse: 8.2508\tvalid_1's l2: 68.0757\n",
      "[200]\ttraining's rmse: 6.55101\ttraining's l2: 42.9158\tvalid_1's rmse: 8.05397\tvalid_1's l2: 64.8665\n",
      "[300]\ttraining's rmse: 5.9427\ttraining's l2: 35.3157\tvalid_1's rmse: 8.00934\tvalid_1's l2: 64.1496\n",
      "[400]\ttraining's rmse: 5.46256\ttraining's l2: 29.8396\tvalid_1's rmse: 7.99895\tvalid_1's l2: 63.9832\n",
      "[500]\ttraining's rmse: 5.03138\ttraining's l2: 25.3148\tvalid_1's rmse: 7.99317\tvalid_1's l2: 63.8908\n",
      "[600]\ttraining's rmse: 4.65207\ttraining's l2: 21.6418\tvalid_1's rmse: 7.99268\tvalid_1's l2: 63.8829\n",
      "Early stopping, best iteration is:\n",
      "[570]\ttraining's rmse: 4.75679\ttraining's l2: 22.6271\tvalid_1's rmse: 7.99032\tvalid_1's l2: 63.8452\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 7.99    \u001b[0m | \u001b[0m 0.8734  \u001b[0m | \u001b[0m 439.0   \u001b[0m | \u001b[0m 13.95   \u001b[0m | \u001b[0m 160.8   \u001b[0m | \u001b[0m 27.01   \u001b[0m | \u001b[0m 59.56   \u001b[0m | \u001b[0m 5.822   \u001b[0m | \u001b[0m 8.695   \u001b[0m | \u001b[0m 0.6129  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.615\ttraining's l2: 57.9882\tvalid_1's rmse: 8.27402\tvalid_1's l2: 68.4593\n",
      "[200]\ttraining's rmse: 6.84397\ttraining's l2: 46.8399\tvalid_1's rmse: 8.0589\tvalid_1's l2: 64.9458\n",
      "[300]\ttraining's rmse: 6.32143\ttraining's l2: 39.9605\tvalid_1's rmse: 8.00386\tvalid_1's l2: 64.0618\n",
      "[400]\ttraining's rmse: 5.89137\ttraining's l2: 34.7082\tvalid_1's rmse: 7.99092\tvalid_1's l2: 63.8547\n",
      "[500]\ttraining's rmse: 5.52674\ttraining's l2: 30.5449\tvalid_1's rmse: 7.98425\tvalid_1's l2: 63.7483\n",
      "[600]\ttraining's rmse: 5.1862\ttraining's l2: 26.8967\tvalid_1's rmse: 7.98247\tvalid_1's l2: 63.7199\n",
      "[700]\ttraining's rmse: 4.8824\ttraining's l2: 23.8379\tvalid_1's rmse: 7.98185\tvalid_1's l2: 63.71\n",
      "[800]\ttraining's rmse: 4.60437\ttraining's l2: 21.2002\tvalid_1's rmse: 7.98241\tvalid_1's l2: 63.7188\n",
      "Early stopping, best iteration is:\n",
      "[725]\ttraining's rmse: 4.81119\ttraining's l2: 23.1476\tvalid_1's rmse: 7.97891\tvalid_1's l2: 63.6631\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m 7.979   \u001b[0m | \u001b[0m 0.5203  \u001b[0m | \u001b[0m 438.9   \u001b[0m | \u001b[0m 11.62   \u001b[0m | \u001b[0m 154.9   \u001b[0m | \u001b[0m 24.46   \u001b[0m | \u001b[0m 44.62   \u001b[0m | \u001b[0m 6.032   \u001b[0m | \u001b[0m 5.831   \u001b[0m | \u001b[0m 0.8662  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.68735\ttraining's l2: 59.0953\tvalid_1's rmse: 8.2809\tvalid_1's l2: 68.5733\n",
      "[200]\ttraining's rmse: 6.96033\ttraining's l2: 48.4461\tvalid_1's rmse: 8.06831\tvalid_1's l2: 65.0977\n",
      "[300]\ttraining's rmse: 6.46377\ttraining's l2: 41.7803\tvalid_1's rmse: 8.00655\tvalid_1's l2: 64.1049\n",
      "[400]\ttraining's rmse: 6.05219\ttraining's l2: 36.629\tvalid_1's rmse: 7.98661\tvalid_1's l2: 63.7859\n",
      "[500]\ttraining's rmse: 5.69081\ttraining's l2: 32.3853\tvalid_1's rmse: 7.97902\tvalid_1's l2: 63.6648\n",
      "[600]\ttraining's rmse: 5.36827\ttraining's l2: 28.8183\tvalid_1's rmse: 7.97343\tvalid_1's l2: 63.5756\n",
      "[700]\ttraining's rmse: 5.07228\ttraining's l2: 25.7281\tvalid_1's rmse: 7.97419\tvalid_1's l2: 63.5878\n",
      "Early stopping, best iteration is:\n",
      "[633]\ttraining's rmse: 5.26811\ttraining's l2: 27.753\tvalid_1's rmse: 7.97056\tvalid_1's l2: 63.5298\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m 7.971   \u001b[0m | \u001b[0m 0.6901  \u001b[0m | \u001b[0m 259.8   \u001b[0m | \u001b[0m 10.86   \u001b[0m | \u001b[0m 67.27   \u001b[0m | \u001b[0m 43.75   \u001b[0m | \u001b[0m 34.92   \u001b[0m | \u001b[0m 27.13   \u001b[0m | \u001b[0m 1.324   \u001b[0m | \u001b[0m 0.8103  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.54226\ttraining's l2: 56.8857\tvalid_1's rmse: 8.25416\tvalid_1's l2: 68.1312\n",
      "[200]\ttraining's rmse: 6.77283\ttraining's l2: 45.8712\tvalid_1's rmse: 8.04373\tvalid_1's l2: 64.7017\n",
      "[300]\ttraining's rmse: 6.35078\ttraining's l2: 40.3324\tvalid_1's rmse: 7.99601\tvalid_1's l2: 63.9362\n",
      "[400]\ttraining's rmse: 6.0206\ttraining's l2: 36.2477\tvalid_1's rmse: 7.97685\tvalid_1's l2: 63.6302\n",
      "[500]\ttraining's rmse: 5.71341\ttraining's l2: 32.643\tvalid_1's rmse: 7.97071\tvalid_1's l2: 63.5322\n",
      "[600]\ttraining's rmse: 5.429\ttraining's l2: 29.4741\tvalid_1's rmse: 7.96598\tvalid_1's l2: 63.4568\n",
      "[700]\ttraining's rmse: 5.1687\ttraining's l2: 26.7154\tvalid_1's rmse: 7.9629\tvalid_1's l2: 63.4077\n",
      "[800]\ttraining's rmse: 4.94557\ttraining's l2: 24.4586\tvalid_1's rmse: 7.9637\tvalid_1's l2: 63.4205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[747]\ttraining's rmse: 5.06072\ttraining's l2: 25.6109\tvalid_1's rmse: 7.96223\tvalid_1's l2: 63.3971\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m 7.962   \u001b[0m | \u001b[0m 0.6136  \u001b[0m | \u001b[0m 431.9   \u001b[0m | \u001b[0m 9.192   \u001b[0m | \u001b[0m 165.5   \u001b[0m | \u001b[0m 19.27   \u001b[0m | \u001b[0m 52.59   \u001b[0m | \u001b[0m 7.53    \u001b[0m | \u001b[0m 7.167   \u001b[0m | \u001b[0m 0.8966  \u001b[0m |\n",
      "=====================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "lgbBO = BayesianOptimization(f=lgb_rmse_eval, pbounds=bayesian_params, random_state=0)\n",
    "lgbBO.maximize(init_points=5, n_iter=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'target': 7.987165508847268,\n",
       "  'params': {'colsample_bytree': 0.7744067519636624,\n",
       "   'max_bin': 360.44278952248555,\n",
       "   'max_depth': 12.822107008573152,\n",
       "   'min_child_samples': 113.52780476941041,\n",
       "   'min_child_weight': 21.75908516760633,\n",
       "   'num_leaves': 49.835764522666246,\n",
       "   'reg_alpha': 21.884984691022,\n",
       "   'reg_lambda': 8.917838234820016,\n",
       "   'subsample': 0.9818313802505146}},\n",
       " {'target': 7.979492120448154,\n",
       "  'params': {'colsample_bytree': 0.6917207594128889,\n",
       "   'max_bin': 397.94526866050563,\n",
       "   'max_depth': 12.231159358023236,\n",
       "   'min_child_samples': 117.92846660784714,\n",
       "   'min_child_weight': 46.35423527634039,\n",
       "   'num_leaves': 26.841442327915477,\n",
       "   'reg_alpha': 4.36559369208002,\n",
       "   'reg_lambda': 0.20316375600581688,\n",
       "   'subsample': 0.916309922773969}},\n",
       " {'target': 8.005161819920833,\n",
       "  'params': {'colsample_bytree': 0.8890783754749252,\n",
       "   'max_bin': 436.30595264094137,\n",
       "   'max_depth': 15.828946737862111,\n",
       "   'min_child_samples': 161.8401272011775,\n",
       "   'min_child_weight': 23.61248875039366,\n",
       "   'num_leaves': 55.22116705145822,\n",
       "   'reg_alpha': 5.922538549187972,\n",
       "   'reg_lambda': 6.3995702922539115,\n",
       "   'subsample': 0.5716766437045232}},\n",
       " {'target': 8.001034158959296,\n",
       "  'params': {'colsample_bytree': 0.972334458524792,\n",
       "   'max_bin': 265.70567765753515,\n",
       "   'max_depth': 11.317295519924189,\n",
       "   'min_child_samples': 60.265566299879126,\n",
       "   'min_child_weight': 38.93745078227661,\n",
       "   'num_leaves': 42.24601328866194,\n",
       "   'reg_alpha': 28.426013103943742,\n",
       "   'reg_lambda': 0.18887921456311507,\n",
       "   'subsample': 0.8088177485379385}},\n",
       " {'target': 7.984131121469482,\n",
       "  'params': {'colsample_bytree': 0.8060478613612108,\n",
       "   'max_bin': 312.2976584686309,\n",
       "   'max_depth': 15.549984628116993,\n",
       "   'min_child_samples': 139.54585682966186,\n",
       "   'min_child_weight': 18.615887128115514,\n",
       "   'num_leaves': 41.481278151973655,\n",
       "   'reg_alpha': 34.88458348440397,\n",
       "   'reg_lambda': 0.6031944908210691,\n",
       "   'subsample': 0.8333833577228338}},\n",
       " {'target': 7.979372983288738,\n",
       "  'params': {'colsample_bytree': 0.6919352847204874,\n",
       "   'max_bin': 434.3628450175187,\n",
       "   'max_depth': 8.987444696464362,\n",
       "   'min_child_samples': 179.90035021893075,\n",
       "   'min_child_weight': 31.78136139817396,\n",
       "   'num_leaves': 48.94805290370239,\n",
       "   'reg_alpha': 6.625037072639985,\n",
       "   'reg_lambda': 8.669549733231406,\n",
       "   'subsample': 0.9016942556058185}},\n",
       " {'target': 7.996563996319494,\n",
       "  'params': {'colsample_bytree': 0.9751700808028696,\n",
       "   'max_bin': 260.9260418938709,\n",
       "   'max_depth': 13.755904263766023,\n",
       "   'min_child_samples': 63.0554140405143,\n",
       "   'min_child_weight': 39.62368934393238,\n",
       "   'num_leaves': 39.29041452592574,\n",
       "   'reg_alpha': 31.036201567054785,\n",
       "   'reg_lambda': 2.3512502971131224,\n",
       "   'subsample': 0.6456656081716027}},\n",
       " {'target': 8.004364055621693,\n",
       "  'params': {'colsample_bytree': 1.0,\n",
       "   'max_bin': 439.37405679929134,\n",
       "   'max_depth': 16.0,\n",
       "   'min_child_samples': 155.80956919146092,\n",
       "   'min_child_weight': 19.667710693160906,\n",
       "   'num_leaves': 59.16990050368063,\n",
       "   'reg_alpha': 6.849158652302749,\n",
       "   'reg_lambda': 5.1475499890643075,\n",
       "   'subsample': 0.8519646157542057}},\n",
       " {'target': 7.986371279904021,\n",
       "  'params': {'colsample_bytree': 0.7028985461799415,\n",
       "   'max_bin': 431.23450539197967,\n",
       "   'max_depth': 8.389882324789724,\n",
       "   'min_child_samples': 150.44968310470477,\n",
       "   'min_child_weight': 24.14039685972133,\n",
       "   'num_leaves': 54.39163847664063,\n",
       "   'reg_alpha': 0.7110431733054295,\n",
       "   'reg_lambda': 1.6822336685637058,\n",
       "   'subsample': 0.5027546292761405}},\n",
       " {'target': 7.974529789982075,\n",
       "  'params': {'colsample_bytree': 0.5437579372795863,\n",
       "   'max_bin': 434.9512484032127,\n",
       "   'max_depth': 12.149648702336364,\n",
       "   'min_child_samples': 164.9140085645037,\n",
       "   'min_child_weight': 10.62326179340681,\n",
       "   'num_leaves': 60.76578856928439,\n",
       "   'reg_alpha': 5.544298406373573,\n",
       "   'reg_lambda': 2.0321346672218463,\n",
       "   'subsample': 0.6577758468928738}},\n",
       " {'target': 7.973188262204542,\n",
       "  'params': {'colsample_bytree': 0.5925209137066674,\n",
       "   'max_bin': 264.9694919138617,\n",
       "   'max_depth': 14.288080416925759,\n",
       "   'min_child_samples': 64.35453598964475,\n",
       "   'min_child_weight': 39.21862410208353,\n",
       "   'num_leaves': 45.550114189442766,\n",
       "   'reg_alpha': 19.14062300483427,\n",
       "   'reg_lambda': 3.4496173592690638,\n",
       "   'subsample': 0.742492131973743}},\n",
       " {'target': 7.992036975456488,\n",
       "  'params': {'colsample_bytree': 0.6288946157166544,\n",
       "   'max_bin': 267.2499597798998,\n",
       "   'max_depth': 14.410408673124728,\n",
       "   'min_child_samples': 54.27473210373281,\n",
       "   'min_child_weight': 37.85256920024338,\n",
       "   'num_leaves': 38.33779677013426,\n",
       "   'reg_alpha': 32.07465520828383,\n",
       "   'reg_lambda': 6.707088648804531,\n",
       "   'subsample': 0.5215149573015059}},\n",
       " {'target': 7.995508391479316,\n",
       "  'params': {'colsample_bytree': 0.9784911734048759,\n",
       "   'max_bin': 263.22459446601664,\n",
       "   'max_depth': 12.967086975282488,\n",
       "   'min_child_samples': 59.1026093857436,\n",
       "   'min_child_weight': 48.80342230604281,\n",
       "   'num_leaves': 36.74526460499274,\n",
       "   'reg_alpha': 28.7453900722824,\n",
       "   'reg_lambda': 4.737256103637732,\n",
       "   'subsample': 0.7618691031151543}},\n",
       " {'target': 7.996287337381436,\n",
       "  'params': {'colsample_bytree': 0.9834752202693975,\n",
       "   'max_bin': 263.86658844996333,\n",
       "   'max_depth': 15.76991625658298,\n",
       "   'min_child_samples': 64.85059288892577,\n",
       "   'min_child_weight': 40.300529211713744,\n",
       "   'num_leaves': 37.31700307163347,\n",
       "   'reg_alpha': 34.37019942904567,\n",
       "   'reg_lambda': 1.0431164994907334,\n",
       "   'subsample': 0.9320029771290941}},\n",
       " {'target': 7.994414305309304,\n",
       "  'params': {'colsample_bytree': 0.8776431000592861,\n",
       "   'max_bin': 443.6301788754557,\n",
       "   'max_depth': 8.955184482949797,\n",
       "   'min_child_samples': 153.69893110256677,\n",
       "   'min_child_weight': 21.511311130929727,\n",
       "   'num_leaves': 61.16250185559939,\n",
       "   'reg_alpha': 7.41143010455637,\n",
       "   'reg_lambda': 7.876597475283019,\n",
       "   'subsample': 0.9169482827586055}},\n",
       " {'target': 7.9810490082537955,\n",
       "  'params': {'colsample_bytree': 0.692646090725441,\n",
       "   'max_bin': 443.6326098297924,\n",
       "   'max_depth': 15.97432547480863,\n",
       "   'min_child_samples': 156.1957113657251,\n",
       "   'min_child_weight': 22.233562224624905,\n",
       "   'num_leaves': 56.55737507365588,\n",
       "   'reg_alpha': 0.19858069609263748,\n",
       "   'reg_lambda': 2.60236333472367,\n",
       "   'subsample': 0.6323945127120649}},\n",
       " {'target': 7.998604429084676,\n",
       "  'params': {'colsample_bytree': 0.8049127708123405,\n",
       "   'max_bin': 432.9794450102791,\n",
       "   'max_depth': 13.736631284449217,\n",
       "   'min_child_samples': 159.9998794806329,\n",
       "   'min_child_weight': 20.508130223447203,\n",
       "   'num_leaves': 52.54259117873046,\n",
       "   'reg_alpha': 13.09037483603729,\n",
       "   'reg_lambda': 6.148289870077169,\n",
       "   'subsample': 0.9010369266915803}},\n",
       " {'target': 7.984906536868514,\n",
       "  'params': {'colsample_bytree': 0.8571457907047135,\n",
       "   'max_bin': 266.6926577274409,\n",
       "   'max_depth': 14.594050721313067,\n",
       "   'min_child_samples': 59.39068228505766,\n",
       "   'min_child_weight': 36.39679083406056,\n",
       "   'num_leaves': 47.858372567572445,\n",
       "   'reg_alpha': 33.6666888502765,\n",
       "   'reg_lambda': 5.513691049952486,\n",
       "   'subsample': 0.7401814552852192}},\n",
       " {'target': 7.977370559867514,\n",
       "  'params': {'colsample_bytree': 0.7830404599337359,\n",
       "   'max_bin': 269.40141149185723,\n",
       "   'max_depth': 10.377655220290276,\n",
       "   'min_child_samples': 61.73239127857961,\n",
       "   'min_child_weight': 42.31039044764997,\n",
       "   'num_leaves': 36.19781162040525,\n",
       "   'reg_alpha': 30.56541989469859,\n",
       "   'reg_lambda': 4.44732817324411,\n",
       "   'subsample': 0.9377757682255993}},\n",
       " {'target': 7.983486085076981,\n",
       "  'params': {'colsample_bytree': 0.679134350048815,\n",
       "   'max_bin': 437.5234583488567,\n",
       "   'max_depth': 9.159038467591222,\n",
       "   'min_child_samples': 154.09407127857654,\n",
       "   'min_child_weight': 21.203288148654863,\n",
       "   'num_leaves': 52.62542937215405,\n",
       "   'reg_alpha': 14.095328333563332,\n",
       "   'reg_lambda': 9.15466237741231,\n",
       "   'subsample': 0.9432691279711745}},\n",
       " {'target': 8.00253667772943,\n",
       "  'params': {'colsample_bytree': 0.9694572631554397,\n",
       "   'max_bin': 444.2303414756819,\n",
       "   'max_depth': 14.400185396770869,\n",
       "   'min_child_samples': 159.91299562993436,\n",
       "   'min_child_weight': 20.681429069560544,\n",
       "   'num_leaves': 52.76009475818188,\n",
       "   'reg_alpha': 8.617013772077534,\n",
       "   'reg_lambda': 7.890145195250425,\n",
       "   'subsample': 0.9558339992559453}},\n",
       " {'target': 7.9956457491130735,\n",
       "  'params': {'colsample_bytree': 0.9422774565965977,\n",
       "   'max_bin': 432.901127494436,\n",
       "   'max_depth': 13.848431551983415,\n",
       "   'min_child_samples': 166.77905663554327,\n",
       "   'min_child_weight': 23.28553184405287,\n",
       "   'num_leaves': 55.575924732862816,\n",
       "   'reg_alpha': 5.25580377631261,\n",
       "   'reg_lambda': 4.266373836256442,\n",
       "   'subsample': 0.6700437819134917}},\n",
       " {'target': 7.975425732419744,\n",
       "  'params': {'colsample_bytree': 0.5970820953708277,\n",
       "   'max_bin': 447.8734934436748,\n",
       "   'max_depth': 13.874369943806112,\n",
       "   'min_child_samples': 168.52801339820587,\n",
       "   'min_child_weight': 19.35135899048112,\n",
       "   'num_leaves': 51.06203969649676,\n",
       "   'reg_alpha': 4.6788095952313755,\n",
       "   'reg_lambda': 6.1723042631335785,\n",
       "   'subsample': 0.6255334634400487}},\n",
       " {'target': 7.993256473785115,\n",
       "  'params': {'colsample_bytree': 0.886409204366879,\n",
       "   'max_bin': 257.0472391100314,\n",
       "   'max_depth': 11.864092685444747,\n",
       "   'min_child_samples': 56.60023000639587,\n",
       "   'min_child_weight': 39.88017411815558,\n",
       "   'num_leaves': 39.98823787143672,\n",
       "   'reg_alpha': 30.69026248770009,\n",
       "   'reg_lambda': 3.6820349286191933,\n",
       "   'subsample': 0.8724705010662281}},\n",
       " {'target': 8.004949738371579,\n",
       "  'params': {'colsample_bytree': 0.8503986056496755,\n",
       "   'max_bin': 433.09500496883135,\n",
       "   'max_depth': 15.386301203705505,\n",
       "   'min_child_samples': 154.75341650407097,\n",
       "   'min_child_weight': 16.517850206857524,\n",
       "   'num_leaves': 57.73495106870535,\n",
       "   'reg_alpha': 8.899791866040571,\n",
       "   'reg_lambda': 9.121542000354047,\n",
       "   'subsample': 0.863411921956049}},\n",
       " {'target': 7.982482845492641,\n",
       "  'params': {'colsample_bytree': 0.8482462475254497,\n",
       "   'max_bin': 437.6443824917015,\n",
       "   'max_depth': 10.235621259386324,\n",
       "   'min_child_samples': 159.4363804024643,\n",
       "   'min_child_weight': 24.53969163586457,\n",
       "   'num_leaves': 52.70318528202723,\n",
       "   'reg_alpha': 13.402129685734392,\n",
       "   'reg_lambda': 0.5056821626625813,\n",
       "   'subsample': 0.8551784268057525}},\n",
       " {'target': 7.990317183933471,\n",
       "  'params': {'colsample_bytree': 0.8733712361563151,\n",
       "   'max_bin': 439.02038421166884,\n",
       "   'max_depth': 13.951131353744849,\n",
       "   'min_child_samples': 160.84635703876205,\n",
       "   'min_child_weight': 27.01065654384857,\n",
       "   'num_leaves': 59.564266546408,\n",
       "   'reg_alpha': 5.8217607896034504,\n",
       "   'reg_lambda': 8.695126802714116,\n",
       "   'subsample': 0.6128922179225391}},\n",
       " {'target': 7.978914043426273,\n",
       "  'params': {'colsample_bytree': 0.5203297857891499,\n",
       "   'max_bin': 438.93433645412875,\n",
       "   'max_depth': 11.615815730017253,\n",
       "   'min_child_samples': 154.88532120800298,\n",
       "   'min_child_weight': 24.462388040101544,\n",
       "   'num_leaves': 44.61767886945886,\n",
       "   'reg_alpha': 6.032494262361121,\n",
       "   'reg_lambda': 5.83052863697451,\n",
       "   'subsample': 0.8661700966714905}},\n",
       " {'target': 7.97055606469109,\n",
       "  'params': {'colsample_bytree': 0.6901484444424856,\n",
       "   'max_bin': 259.7789107195877,\n",
       "   'max_depth': 10.856161203287082,\n",
       "   'min_child_samples': 67.27155398384778,\n",
       "   'min_child_weight': 43.746960803989225,\n",
       "   'num_leaves': 34.922156184410085,\n",
       "   'reg_alpha': 27.13449438343015,\n",
       "   'reg_lambda': 1.3239913107414665,\n",
       "   'subsample': 0.810349434282837}},\n",
       " {'target': 7.962228403815961,\n",
       "  'params': {'colsample_bytree': 0.6135677219430125,\n",
       "   'max_bin': 431.8568693529485,\n",
       "   'max_depth': 9.192474991886744,\n",
       "   'min_child_samples': 165.51271062603092,\n",
       "   'min_child_weight': 19.273181779562222,\n",
       "   'num_leaves': 52.585047034434965,\n",
       "   'reg_alpha': 7.529604471620047,\n",
       "   'reg_lambda': 7.167466400758902,\n",
       "   'subsample': 0.89656117567388}}]"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbBO.res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.987165508847268, 7.979492120448154, 8.005161819920833, 8.001034158959296, 7.984131121469482, 7.979372983288738, 7.996563996319494, 8.004364055621693, 7.986371279904021, 7.974529789982075, 7.973188262204542, 7.992036975456488, 7.995508391479316, 7.996287337381436, 7.994414305309304, 7.9810490082537955, 7.998604429084676, 7.984906536868514, 7.977370559867514, 7.983486085076981, 8.00253667772943, 7.9956457491130735, 7.975425732419744, 7.993256473785115, 8.004949738371579, 7.982482845492641, 7.990317183933471, 7.978914043426273, 7.97055606469109, 7.962228403815961]\n",
      "maximum target index: 29\n"
     ]
    }
   ],
   "source": [
    "# dictionary에 있는 target값을 모두 추출\n",
    "target_list = []\n",
    "for result in lgbBO.res:\n",
    "    target = result['target']\n",
    "    target_list.append(target)\n",
    "print(target_list)\n",
    "# 가장 큰 target 값을 가지는 순번(index)를 추출\n",
    "print('maximum target index:', np.argmin(np.array(target_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'target': 7.962228403815961, 'params': {'colsample_bytree': 0.6135677219430125, 'max_bin': 431.8568693529485, 'max_depth': 9.192474991886744, 'min_child_samples': 165.51271062603092, 'min_child_weight': 19.273181779562222, 'num_leaves': 52.585047034434965, 'reg_alpha': 7.529604471620047, 'reg_lambda': 7.167466400758902, 'subsample': 0.89656117567388}}\n"
     ]
    }
   ],
   "source": [
    "# 가장 큰 target값을 가지는 index값을 기준으로 res에서 해당 parameter 추출. \n",
    "max_dict = lgbBO.res[np.argmin(np.array(target_list))]\n",
    "print(max_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr = X_train_select\n",
    "target =  y['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def train_apps_all_with_oof(ftr, target, nfolds=5):\n",
    "    ftr = ftr\n",
    "    target = target\n",
    "\n",
    "    # nfolds 개의 cross validatin fold set을 가지는 KFold 생성 \n",
    "    folds = KFold(n_splits=nfolds, shuffle=True, random_state=0)\n",
    "    \n",
    "    # Out of Folds로 학습된 모델의 validation set을 예측하여 결과 확률을 담을 array 생성.\n",
    "    # validation set가 n_split갯수만큼 있으므로 크기는 ftr_app의 크기가 되어야 함. \n",
    "    oof_preds = np.zeros((ftr.shape[0],))  \n",
    "    \n",
    "    # Ouf of Folds로 학습된 모델의 test dataset을 예측하여 결과 확률을 담을 array 생성. \n",
    "    test_preds = np.zeros(((X_test_select.shape[0],)))\n",
    "    \n",
    "    # n_estimators를 4000까지 확대. \n",
    "    clf = LGBMRegressor(\n",
    "                nthread=4,\n",
    "                n_estimators=4000,\n",
    "                learning_rate=0.01,\n",
    "                max_depth=14,\n",
    "                num_leaves=43,\n",
    "                colsample_bytree=0.6243940572960339,\n",
    "                subsample=0.9456256393571061,\n",
    "                max_bin=308,\n",
    "                reg_alpha=40.75532541032781,\n",
    "                reg_lambda=1.3974457528318784,\n",
    "                min_child_weight=10.700102581559948,\n",
    "                min_child_samples=140,\n",
    "                silent=-1,\n",
    "                verbose=-1,\n",
    "                )\n",
    "\n",
    "\n",
    "    # nfolds 번 cross validation Iteration 반복하면서 OOF 방식으로 학습 및 테스트 데이터 예측\n",
    "    for fold_idx, (train_idx, valid_idx) in enumerate(folds.split(ftr)):\n",
    "        print('##### iteration ', fold_idx, ' 시작')\n",
    "        # 학습용 데이터 세트의 인덱스와 검증용 데이터 세트의 인덱스 추출하여 이를 기반으로 학습/검증 데이터 추출\n",
    "        train_x = ftr.iloc[train_idx, :]\n",
    "        train_y = target.iloc[train_idx]\n",
    "        valid_x = ftr.iloc[valid_idx, :]\n",
    "        valid_y = target.iloc[valid_idx]\n",
    "        \n",
    "        # 추출된 학습/검증 데이터 세트로 모델 학습. early_stopping은 200으로 증가. \n",
    "        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], eval_metric= 'RMSE', verbose= 200, \n",
    "                early_stopping_rounds= 200)\n",
    "        # 검증 데이터 세트로 예측된 확률 저장. 사용되지는 않음. \n",
    "        #oof_preds[valid_idx] = clf.predict(valid_x, num_iteration=clf.best_iteration_)       \n",
    "        # 학습된 모델로 테스트 데이터 세트에 예측 확률 계산. \n",
    "        # nfolds 번 반복 실행하므로 평균 확률을 구하기 위해 개별 수행시 마다 수행 횟수로 나눈 확률을 추후에 더해서 최종 평균 확률 계산. \n",
    "        test_preds += clf.predict(X_test_select, num_iteration=clf.best_iteration_)/folds.n_splits\n",
    "        \n",
    "        \n",
    "    return clf, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### iteration  0  시작\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's rmse: 7.5716\ttraining's l2: 57.3292\tvalid_1's rmse: 8.28327\tvalid_1's l2: 68.6125\n",
      "[400]\ttraining's rmse: 6.8237\ttraining's l2: 46.5629\tvalid_1's rmse: 8.0948\tvalid_1's l2: 65.5258\n",
      "[600]\ttraining's rmse: 6.42053\ttraining's l2: 41.2232\tvalid_1's rmse: 8.05453\tvalid_1's l2: 64.8754\n",
      "[800]\ttraining's rmse: 6.10941\ttraining's l2: 37.3249\tvalid_1's rmse: 8.03856\tvalid_1's l2: 64.6184\n",
      "[1000]\ttraining's rmse: 5.81739\ttraining's l2: 33.8421\tvalid_1's rmse: 8.02758\tvalid_1's l2: 64.4421\n",
      "[1200]\ttraining's rmse: 5.55747\ttraining's l2: 30.8854\tvalid_1's rmse: 8.02152\tvalid_1's l2: 64.3447\n",
      "[1400]\ttraining's rmse: 5.3196\ttraining's l2: 28.2981\tvalid_1's rmse: 8.01917\tvalid_1's l2: 64.3071\n",
      "[1600]\ttraining's rmse: 5.10682\ttraining's l2: 26.0796\tvalid_1's rmse: 8.01767\tvalid_1's l2: 64.283\n",
      "Early stopping, best iteration is:\n",
      "[1497]\ttraining's rmse: 5.21024\ttraining's l2: 27.1466\tvalid_1's rmse: 8.01526\tvalid_1's l2: 64.2444\n",
      "##### iteration  1  시작\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's rmse: 7.60807\ttraining's l2: 57.8828\tvalid_1's rmse: 8.17774\tvalid_1's l2: 66.8755\n",
      "[400]\ttraining's rmse: 6.85349\ttraining's l2: 46.9703\tvalid_1's rmse: 7.94302\tvalid_1's l2: 63.0916\n",
      "[600]\ttraining's rmse: 6.44998\ttraining's l2: 41.6023\tvalid_1's rmse: 7.88878\tvalid_1's l2: 62.2329\n",
      "[800]\ttraining's rmse: 6.14665\ttraining's l2: 37.7813\tvalid_1's rmse: 7.86372\tvalid_1's l2: 61.838\n",
      "[1000]\ttraining's rmse: 5.87255\ttraining's l2: 34.4868\tvalid_1's rmse: 7.84828\tvalid_1's l2: 61.5956\n",
      "[1200]\ttraining's rmse: 5.61169\ttraining's l2: 31.4911\tvalid_1's rmse: 7.84151\tvalid_1's l2: 61.4893\n",
      "[1400]\ttraining's rmse: 5.36437\ttraining's l2: 28.7765\tvalid_1's rmse: 7.8377\tvalid_1's l2: 61.4295\n",
      "[1600]\ttraining's rmse: 5.13337\ttraining's l2: 26.3515\tvalid_1's rmse: 7.83326\tvalid_1's l2: 61.36\n",
      "[1800]\ttraining's rmse: 4.91952\ttraining's l2: 24.2017\tvalid_1's rmse: 7.83122\tvalid_1's l2: 61.328\n",
      "[2000]\ttraining's rmse: 4.7075\ttraining's l2: 22.1605\tvalid_1's rmse: 7.82913\tvalid_1's l2: 61.2952\n",
      "[2200]\ttraining's rmse: 4.50807\ttraining's l2: 20.3227\tvalid_1's rmse: 7.82938\tvalid_1's l2: 61.2992\n",
      "Early stopping, best iteration is:\n",
      "[2104]\ttraining's rmse: 4.60316\ttraining's l2: 21.1891\tvalid_1's rmse: 7.828\tvalid_1's l2: 61.2776\n",
      "##### iteration  2  시작\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's rmse: 7.58395\ttraining's l2: 57.5163\tvalid_1's rmse: 8.24541\tvalid_1's l2: 67.9867\n",
      "[400]\ttraining's rmse: 6.82135\ttraining's l2: 46.5309\tvalid_1's rmse: 8.04836\tvalid_1's l2: 64.7761\n",
      "[600]\ttraining's rmse: 6.40676\ttraining's l2: 41.0466\tvalid_1's rmse: 8.00599\tvalid_1's l2: 64.0959\n",
      "[800]\ttraining's rmse: 6.09429\ttraining's l2: 37.1404\tvalid_1's rmse: 7.9908\tvalid_1's l2: 63.8529\n",
      "[1000]\ttraining's rmse: 5.7996\ttraining's l2: 33.6353\tvalid_1's rmse: 7.98303\tvalid_1's l2: 63.7287\n",
      "[1200]\ttraining's rmse: 5.53849\ttraining's l2: 30.6748\tvalid_1's rmse: 7.97751\tvalid_1's l2: 63.6406\n",
      "[1400]\ttraining's rmse: 5.2896\ttraining's l2: 27.9799\tvalid_1's rmse: 7.9762\tvalid_1's l2: 63.6198\n",
      "Early stopping, best iteration is:\n",
      "[1368]\ttraining's rmse: 5.32704\ttraining's l2: 28.3774\tvalid_1's rmse: 7.97505\tvalid_1's l2: 63.6015\n",
      "##### iteration  3  시작\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's rmse: 7.5559\ttraining's l2: 57.0916\tvalid_1's rmse: 8.39041\tvalid_1's l2: 70.399\n",
      "[400]\ttraining's rmse: 6.79462\ttraining's l2: 46.1669\tvalid_1's rmse: 8.16135\tvalid_1's l2: 66.6076\n",
      "[600]\ttraining's rmse: 6.34309\ttraining's l2: 40.2348\tvalid_1's rmse: 8.11348\tvalid_1's l2: 65.8285\n",
      "[800]\ttraining's rmse: 6.02747\ttraining's l2: 36.3304\tvalid_1's rmse: 8.09488\tvalid_1's l2: 65.5271\n",
      "[1000]\ttraining's rmse: 5.74532\ttraining's l2: 33.0087\tvalid_1's rmse: 8.08591\tvalid_1's l2: 65.382\n",
      "[1200]\ttraining's rmse: 5.48459\ttraining's l2: 30.0807\tvalid_1's rmse: 8.08078\tvalid_1's l2: 65.299\n",
      "[1400]\ttraining's rmse: 5.23742\ttraining's l2: 27.4306\tvalid_1's rmse: 8.07911\tvalid_1's l2: 65.2719\n",
      "[1600]\ttraining's rmse: 5.01789\ttraining's l2: 25.1793\tvalid_1's rmse: 8.08011\tvalid_1's l2: 65.2882\n",
      "Early stopping, best iteration is:\n",
      "[1452]\ttraining's rmse: 5.17703\ttraining's l2: 26.8016\tvalid_1's rmse: 8.07836\tvalid_1's l2: 65.2598\n",
      "##### iteration  4  시작\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's rmse: 7.60005\ttraining's l2: 57.7608\tvalid_1's rmse: 8.21293\tvalid_1's l2: 67.4523\n",
      "[400]\ttraining's rmse: 6.84185\ttraining's l2: 46.8109\tvalid_1's rmse: 7.98671\tvalid_1's l2: 63.7876\n",
      "[600]\ttraining's rmse: 6.42436\ttraining's l2: 41.2724\tvalid_1's rmse: 7.93054\tvalid_1's l2: 62.8935\n",
      "[800]\ttraining's rmse: 6.12786\ttraining's l2: 37.5507\tvalid_1's rmse: 7.91262\tvalid_1's l2: 62.6096\n",
      "[1000]\ttraining's rmse: 5.86418\ttraining's l2: 34.3886\tvalid_1's rmse: 7.90016\tvalid_1's l2: 62.4125\n",
      "[1200]\ttraining's rmse: 5.62021\ttraining's l2: 31.5868\tvalid_1's rmse: 7.89438\tvalid_1's l2: 62.3213\n",
      "[1400]\ttraining's rmse: 5.38191\ttraining's l2: 28.9649\tvalid_1's rmse: 7.88793\tvalid_1's l2: 62.2195\n",
      "[1600]\ttraining's rmse: 5.16295\ttraining's l2: 26.656\tvalid_1's rmse: 7.88876\tvalid_1's l2: 62.2326\n",
      "Early stopping, best iteration is:\n",
      "[1502]\ttraining's rmse: 5.27381\ttraining's l2: 27.8131\tvalid_1's rmse: 7.8871\tvalid_1's l2: 62.2064\n"
     ]
    }
   ],
   "source": [
    "clf, test_preds = train_apps_all_with_oof(ftr, target, nfolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([40.73266126, 44.74888879, 27.71824792, ..., 36.95008037,\n",
       "       33.93506517, 26.72884366])"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'submission_0615_0327.csv' is ready to submit.\n"
     ]
    }
   ],
   "source": [
    "t = pd.Timestamp.now()\n",
    "fname = f\"submission_{t.month:02}{t.day:02}_{t.hour:02}{t.minute:02}.csv\"\n",
    "submissions = pd.concat([pd.Series(IDtest, name=\"custid\"), pd.Series(test_preds, name=\"age\")] ,axis=1)\n",
    "submissions.to_csv(fname, index=False)\n",
    "print(\"'{}' is ready to submit.\" .format(fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgboost HP tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr = X_train_select\n",
    "target =  y['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15110, 1239), (6477, 1239))"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(ftr, target, test_size=0.3, random_state=0)\n",
    "train_x.shape, valid_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_params = {\n",
    "    'n_estimators': (1804, 1806),\n",
    "     'gamma': (0, 100),\n",
    "     'max_depth' : (8, 16),\n",
    "     'min_child_weight': (1, 50),\n",
    "     'max_delta_step': (1, 50),\n",
    "     'subsample': (0.5, 1),\n",
    "     'colsample_bytree': (0.5, 1),\n",
    "     'colsample_bylevel': (0.5, 1),\n",
    "     'reg_lambda': (0.001, 10),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_rmse_eval(n_estimators, gamma, max_depth, min_child_weight, max_delta_step, \n",
    "                subsample, colsample_bytree, colsample_bylevel, reg_lambda):\n",
    "    \n",
    "    params = {\n",
    "        \"n_estimators\":int(round(n_estimators)), \n",
    "        \"learning_rate\":0.02,\n",
    "        'gamma':int(round(gamma)),\n",
    "        'max_depth':int(round(max_depth)),\n",
    "        'min_child_weight': int(round(min_child_weight)),\n",
    "        'max_delta_step': int(round(max_delta_step)),\n",
    "        'subsample':max(min(subsample, 1), 0),\n",
    "        'colsample_bytree':max(min(colsample_bytree, 1), 0),\n",
    "        'reg_lambda': max(reg_lambda,0),\n",
    "        'colsample_bylevel':max(min(colsample_bylevel, 1), 0)\n",
    "    }\n",
    "    \n",
    "    xgb_model = XGBRegressor(**params)\n",
    "    xgb_model.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], eval_metric= 'rmse', verbose= 100, \n",
    "                early_stopping_rounds= 100)\n",
    "    valid_pred = xgb_model.predict(valid_x)\n",
    "    RMSE = np.sqrt(mean_squared_error(valid_y, valid_pred))\n",
    "    \n",
    "    return RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... | colsam... |   gamma   | max_de... | max_depth | min_ch... | n_esti... | reg_la... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "[0]\tvalidation_0-rmse:36.06340\tvalidation_1-rmse:35.83922\n",
      "[100]\tvalidation_0-rmse:8.87136\tvalidation_1-rmse:9.70564\n",
      "[200]\tvalidation_0-rmse:5.29508\tvalidation_1-rmse:8.09067\n",
      "[300]\tvalidation_0-rmse:4.33195\tvalidation_1-rmse:8.01332\n",
      "[400]\tvalidation_0-rmse:3.80262\tvalidation_1-rmse:7.99972\n",
      "[500]\tvalidation_0-rmse:3.31632\tvalidation_1-rmse:7.99312\n",
      "[600]\tvalidation_0-rmse:2.93917\tvalidation_1-rmse:7.98979\n",
      "[700]\tvalidation_0-rmse:2.64273\tvalidation_1-rmse:7.98760\n",
      "[800]\tvalidation_0-rmse:2.39303\tvalidation_1-rmse:7.98654\n",
      "[860]\tvalidation_0-rmse:2.28410\tvalidation_1-rmse:7.98673\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 7.985   \u001b[0m | \u001b[0m 0.7744  \u001b[0m | \u001b[0m 0.8576  \u001b[0m | \u001b[0m 60.28   \u001b[0m | \u001b[0m 27.7    \u001b[0m | \u001b[0m 11.39   \u001b[0m | \u001b[0m 32.65   \u001b[0m | \u001b[0m 1.805e+0\u001b[0m | \u001b[0m 8.918   \u001b[0m | \u001b[0m 0.9818  \u001b[0m |\n",
      "[0]\tvalidation_0-rmse:36.04584\tvalidation_1-rmse:35.82173\n",
      "[100]\tvalidation_0-rmse:6.12694\tvalidation_1-rmse:9.49956\n",
      "[200]\tvalidation_0-rmse:1.97226\tvalidation_1-rmse:8.08506\n",
      "[300]\tvalidation_0-rmse:1.63097\tvalidation_1-rmse:8.05501\n",
      "[400]\tvalidation_0-rmse:1.57138\tvalidation_1-rmse:8.05066\n",
      "[500]\tvalidation_0-rmse:1.53824\tvalidation_1-rmse:8.04747\n",
      "[600]\tvalidation_0-rmse:1.51864\tvalidation_1-rmse:8.04506\n",
      "[700]\tvalidation_0-rmse:1.50746\tvalidation_1-rmse:8.04424\n",
      "[800]\tvalidation_0-rmse:1.49755\tvalidation_1-rmse:8.04320\n",
      "[900]\tvalidation_0-rmse:1.48761\tvalidation_1-rmse:8.04274\n",
      "[1000]\tvalidation_0-rmse:1.47911\tvalidation_1-rmse:8.04221\n",
      "[1100]\tvalidation_0-rmse:1.47064\tvalidation_1-rmse:8.04200\n",
      "[1200]\tvalidation_0-rmse:1.46403\tvalidation_1-rmse:8.04128\n",
      "[1300]\tvalidation_0-rmse:1.45815\tvalidation_1-rmse:8.04086\n",
      "[1400]\tvalidation_0-rmse:1.45183\tvalidation_1-rmse:8.04018\n",
      "[1500]\tvalidation_0-rmse:1.44707\tvalidation_1-rmse:8.03972\n",
      "[1600]\tvalidation_0-rmse:1.44206\tvalidation_1-rmse:8.03953\n",
      "[1670]\tvalidation_0-rmse:1.43868\tvalidation_1-rmse:8.03955\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 8.039   \u001b[0m | \u001b[95m 0.6917  \u001b[0m | \u001b[95m 0.8959  \u001b[0m | \u001b[95m 52.89   \u001b[0m | \u001b[95m 28.83   \u001b[0m | \u001b[95m 15.4    \u001b[0m | \u001b[95m 4.481   \u001b[0m | \u001b[95m 1.804e+0\u001b[0m | \u001b[95m 0.2032  \u001b[0m | \u001b[95m 0.9163  \u001b[0m |\n",
      "[0]\tvalidation_0-rmse:35.93086\tvalidation_1-rmse:35.70524\n",
      "[100]\tvalidation_0-rmse:8.89455\tvalidation_1-rmse:9.51623\n",
      "[200]\tvalidation_0-rmse:5.95200\tvalidation_1-rmse:8.05949\n",
      "[300]\tvalidation_0-rmse:5.10369\tvalidation_1-rmse:8.00714\n",
      "[400]\tvalidation_0-rmse:4.51960\tvalidation_1-rmse:7.99859\n",
      "[500]\tvalidation_0-rmse:4.01750\tvalidation_1-rmse:7.99463\n",
      "[600]\tvalidation_0-rmse:3.57985\tvalidation_1-rmse:7.98965\n",
      "[700]\tvalidation_0-rmse:3.21813\tvalidation_1-rmse:7.99225\n",
      "[732]\tvalidation_0-rmse:3.12531\tvalidation_1-rmse:7.99256\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 7.988   \u001b[0m | \u001b[0m 0.8891  \u001b[0m | \u001b[0m 0.935   \u001b[0m | \u001b[0m 97.86   \u001b[0m | \u001b[0m 40.16   \u001b[0m | \u001b[0m 11.69   \u001b[0m | \u001b[0m 39.25   \u001b[0m | \u001b[0m 1.804e+0\u001b[0m | \u001b[0m 6.4     \u001b[0m | \u001b[0m 0.5717  \u001b[0m |\n",
      "[0]\tvalidation_0-rmse:36.33155\tvalidation_1-rmse:36.10720\n",
      "[100]\tvalidation_0-rmse:12.99329\tvalidation_1-rmse:13.15971\n",
      "[200]\tvalidation_0-rmse:5.19564\tvalidation_1-rmse:8.20155\n",
      "[300]\tvalidation_0-rmse:3.49850\tvalidation_1-rmse:8.02349\n",
      "[400]\tvalidation_0-rmse:2.77263\tvalidation_1-rmse:8.00982\n",
      "[500]\tvalidation_0-rmse:2.28578\tvalidation_1-rmse:8.00573\n",
      "[600]\tvalidation_0-rmse:1.92855\tvalidation_1-rmse:8.00663\n",
      "[662]\tvalidation_0-rmse:1.77188\tvalidation_1-rmse:8.00655\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 8.005   \u001b[0m | \u001b[0m 0.9723  \u001b[0m | \u001b[0m 0.7609  \u001b[0m | \u001b[0m 41.47   \u001b[0m | \u001b[0m 13.96   \u001b[0m | \u001b[0m 14.19   \u001b[0m | \u001b[0m 23.35   \u001b[0m | \u001b[0m 1.805e+0\u001b[0m | \u001b[0m 0.1889  \u001b[0m | \u001b[0m 0.8088  \u001b[0m |\n",
      "[0]\tvalidation_0-rmse:35.97424\tvalidation_1-rmse:35.75192\n",
      "[100]\tvalidation_0-rmse:7.33869\tvalidation_1-rmse:9.35332\n",
      "[200]\tvalidation_0-rmse:4.12623\tvalidation_1-rmse:8.01512\n",
      "[300]\tvalidation_0-rmse:3.45023\tvalidation_1-rmse:7.98482\n",
      "[400]\tvalidation_0-rmse:3.01493\tvalidation_1-rmse:7.97778\n",
      "[500]\tvalidation_0-rmse:2.69428\tvalidation_1-rmse:7.97375\n",
      "[600]\tvalidation_0-rmse:2.47877\tvalidation_1-rmse:7.96939\n",
      "[700]\tvalidation_0-rmse:2.33247\tvalidation_1-rmse:7.96732\n",
      "[800]\tvalidation_0-rmse:2.24207\tvalidation_1-rmse:7.96690\n",
      "[878]\tvalidation_0-rmse:2.18560\tvalidation_1-rmse:7.96718\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 7.967   \u001b[0m | \u001b[0m 0.806   \u001b[0m | \u001b[0m 0.8085  \u001b[0m | \u001b[0m 94.37   \u001b[0m | \u001b[0m 34.41   \u001b[0m | \u001b[0m 10.88   \u001b[0m | \u001b[0m 22.41   \u001b[0m | \u001b[0m 1.805e+0\u001b[0m | \u001b[0m 0.6032  \u001b[0m | \u001b[0m 0.8334  \u001b[0m |\n",
      "[0]\tvalidation_0-rmse:36.02845\tvalidation_1-rmse:35.80493\n",
      "[100]\tvalidation_0-rmse:5.98056\tvalidation_1-rmse:9.40998\n",
      "[200]\tvalidation_0-rmse:1.80244\tvalidation_1-rmse:8.10162\n",
      "[300]\tvalidation_0-rmse:1.46763\tvalidation_1-rmse:8.07967\n",
      "[400]\tvalidation_0-rmse:1.40498\tvalidation_1-rmse:8.07567\n",
      "[500]\tvalidation_0-rmse:1.37627\tvalidation_1-rmse:8.07384\n",
      "[600]\tvalidation_0-rmse:1.35435\tvalidation_1-rmse:8.07207\n",
      "[700]\tvalidation_0-rmse:1.34223\tvalidation_1-rmse:8.07103\n",
      "[800]\tvalidation_0-rmse:1.33019\tvalidation_1-rmse:8.07015\n",
      "[900]\tvalidation_0-rmse:1.31884\tvalidation_1-rmse:8.06933\n",
      "[1000]\tvalidation_0-rmse:1.31238\tvalidation_1-rmse:8.06900\n",
      "[1100]\tvalidation_0-rmse:1.30581\tvalidation_1-rmse:8.06821\n",
      "[1200]\tvalidation_0-rmse:1.30021\tvalidation_1-rmse:8.06813\n",
      "[1300]\tvalidation_0-rmse:1.29609\tvalidation_1-rmse:8.06803\n",
      "[1400]\tvalidation_0-rmse:1.29101\tvalidation_1-rmse:8.06758\n",
      "[1500]\tvalidation_0-rmse:1.28612\tvalidation_1-rmse:8.06709\n",
      "[1600]\tvalidation_0-rmse:1.28281\tvalidation_1-rmse:8.06715\n",
      "[1633]\tvalidation_0-rmse:1.28092\tvalidation_1-rmse:8.06715\n",
      "| \u001b[95m 6       \u001b[0m | \u001b[95m 8.067   \u001b[0m | \u001b[95m 0.6987  \u001b[0m | \u001b[95m 0.8956  \u001b[0m | \u001b[95m 40.99   \u001b[0m | \u001b[95m 30.37   \u001b[0m | \u001b[95m 16.0    \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 1.804e+0\u001b[0m | \u001b[95m 0.001   \u001b[0m | \u001b[95m 0.8122  \u001b[0m |\n",
      "[0]\tvalidation_0-rmse:35.89781\tvalidation_1-rmse:35.69005\n",
      "[100]\tvalidation_0-rmse:5.31932\tvalidation_1-rmse:9.21550\n",
      "[200]\tvalidation_0-rmse:1.50952\tvalidation_1-rmse:8.10958\n",
      "[300]\tvalidation_0-rmse:1.19428\tvalidation_1-rmse:8.09650\n",
      "[400]\tvalidation_0-rmse:1.13906\tvalidation_1-rmse:8.09397\n",
      "[500]\tvalidation_0-rmse:1.11230\tvalidation_1-rmse:8.09187\n",
      "[600]\tvalidation_0-rmse:1.09378\tvalidation_1-rmse:8.09026\n",
      "[700]\tvalidation_0-rmse:1.08215\tvalidation_1-rmse:8.08941\n",
      "[800]\tvalidation_0-rmse:1.07214\tvalidation_1-rmse:8.08890\n",
      "[900]\tvalidation_0-rmse:1.06359\tvalidation_1-rmse:8.08870\n",
      "[1000]\tvalidation_0-rmse:1.05669\tvalidation_1-rmse:8.08845\n",
      "[1100]\tvalidation_0-rmse:1.05009\tvalidation_1-rmse:8.08770\n",
      "[1200]\tvalidation_0-rmse:1.04239\tvalidation_1-rmse:8.08739\n",
      "[1300]\tvalidation_0-rmse:1.03957\tvalidation_1-rmse:8.08723\n",
      "[1400]\tvalidation_0-rmse:1.03560\tvalidation_1-rmse:8.08719\n",
      "[1440]\tvalidation_0-rmse:1.03386\tvalidation_1-rmse:8.08711\n",
      "| \u001b[95m 7       \u001b[0m | \u001b[95m 8.087   \u001b[0m | \u001b[95m 0.5     \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 25.98   \u001b[0m | \u001b[95m 50.0    \u001b[0m | \u001b[95m 16.0    \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 1.804e+0\u001b[0m | \u001b[95m 0.001   \u001b[0m | \u001b[95m 0.7516  \u001b[0m |\n",
      "[0]\tvalidation_0-rmse:35.94024\tvalidation_1-rmse:35.71738\n",
      "[100]\tvalidation_0-rmse:9.17653\tvalidation_1-rmse:9.64061\n",
      "[200]\tvalidation_0-rmse:6.46803\tvalidation_1-rmse:8.13953\n",
      "[300]\tvalidation_0-rmse:5.63595\tvalidation_1-rmse:8.05893\n",
      "[400]\tvalidation_0-rmse:5.02731\tvalidation_1-rmse:8.03397\n",
      "[500]\tvalidation_0-rmse:4.49092\tvalidation_1-rmse:8.02726\n",
      "[600]\tvalidation_0-rmse:4.01718\tvalidation_1-rmse:8.02513\n",
      "[700]\tvalidation_0-rmse:3.58868\tvalidation_1-rmse:8.02115\n",
      "[800]\tvalidation_0-rmse:3.20484\tvalidation_1-rmse:8.02316\n",
      "[807]\tvalidation_0-rmse:3.18350\tvalidation_1-rmse:8.02291\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 8.02    \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 36.62   \u001b[0m | \u001b[0m 8.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 1.804e+0\u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 0.5     \u001b[0m |\n",
      "[0]\tvalidation_0-rmse:35.92216\tvalidation_1-rmse:35.69683\n",
      "[100]\tvalidation_0-rmse:9.11907\tvalidation_1-rmse:9.58707\n",
      "[200]\tvalidation_0-rmse:6.41283\tvalidation_1-rmse:8.12570\n",
      "[300]\tvalidation_0-rmse:5.55319\tvalidation_1-rmse:8.04599\n",
      "[400]\tvalidation_0-rmse:4.92850\tvalidation_1-rmse:8.02745\n",
      "[500]\tvalidation_0-rmse:4.36476\tvalidation_1-rmse:8.02096\n",
      "[589]\tvalidation_0-rmse:3.95640\tvalidation_1-rmse:8.02050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 8.019   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 39.65   \u001b[0m | \u001b[0m 50.0    \u001b[0m | \u001b[0m 8.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 1.806e+0\u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 0.5     \u001b[0m |\n",
      "[0]\tvalidation_0-rmse:35.89865\tvalidation_1-rmse:35.68850\n",
      "[100]\tvalidation_0-rmse:5.99838\tvalidation_1-rmse:9.22768\n",
      "[200]\tvalidation_0-rmse:2.28253\tvalidation_1-rmse:8.05553\n",
      "[300]\tvalidation_0-rmse:1.65159\tvalidation_1-rmse:8.03955\n",
      "[400]\tvalidation_0-rmse:1.34207\tvalidation_1-rmse:8.03757\n",
      "[476]\tvalidation_0-rmse:1.19134\tvalidation_1-rmse:8.03879\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 8.037   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 19.54   \u001b[0m | \u001b[0m 50.0    \u001b[0m | \u001b[0m 16.0    \u001b[0m | \u001b[0m 17.01   \u001b[0m | \u001b[0m 1.804e+0\u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "[0]\tvalidation_0-rmse:36.06340\tvalidation_1-rmse:35.83922\n",
      "[100]\tvalidation_0-rmse:6.31818\tvalidation_1-rmse:9.62989\n",
      "[200]\tvalidation_0-rmse:1.87861\tvalidation_1-rmse:8.12810\n",
      "[300]\tvalidation_0-rmse:1.46742\tvalidation_1-rmse:8.09138\n",
      "[400]\tvalidation_0-rmse:1.39863\tvalidation_1-rmse:8.08601\n",
      "[500]\tvalidation_0-rmse:1.36401\tvalidation_1-rmse:8.08294\n",
      "[600]\tvalidation_0-rmse:1.33859\tvalidation_1-rmse:8.08090\n",
      "[700]\tvalidation_0-rmse:1.32261\tvalidation_1-rmse:8.07926\n",
      "[800]\tvalidation_0-rmse:1.31013\tvalidation_1-rmse:8.07866\n",
      "[900]\tvalidation_0-rmse:1.30159\tvalidation_1-rmse:8.07788\n",
      "[1000]\tvalidation_0-rmse:1.29482\tvalidation_1-rmse:8.07758\n",
      "[1100]\tvalidation_0-rmse:1.28592\tvalidation_1-rmse:8.07676\n",
      "[1200]\tvalidation_0-rmse:1.28044\tvalidation_1-rmse:8.07665\n",
      "[1300]\tvalidation_0-rmse:1.27416\tvalidation_1-rmse:8.07592\n",
      "[1400]\tvalidation_0-rmse:1.27005\tvalidation_1-rmse:8.07551\n",
      "[1500]\tvalidation_0-rmse:1.26716\tvalidation_1-rmse:8.07531\n",
      "[1600]\tvalidation_0-rmse:1.26381\tvalidation_1-rmse:8.07521\n",
      "[1700]\tvalidation_0-rmse:1.26043\tvalidation_1-rmse:8.07517\n",
      "[1707]\tvalidation_0-rmse:1.26043\tvalidation_1-rmse:8.07519\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 8.075   \u001b[0m | \u001b[0m 0.6928  \u001b[0m | \u001b[0m 0.5522  \u001b[0m | \u001b[0m 38.54   \u001b[0m | \u001b[0m 27.62   \u001b[0m | \u001b[0m 15.9    \u001b[0m | \u001b[0m 2.195   \u001b[0m | \u001b[0m 1.805e+0\u001b[0m | \u001b[0m 0.2687  \u001b[0m | \u001b[0m 0.8386  \u001b[0m |\n",
      "[0]\tvalidation_0-rmse:35.99404\tvalidation_1-rmse:35.77336\n",
      "[100]\tvalidation_0-rmse:5.39518\tvalidation_1-rmse:9.35057\n",
      "[200]\tvalidation_0-rmse:1.32720\tvalidation_1-rmse:8.14041\n",
      "[300]\tvalidation_0-rmse:1.04467\tvalidation_1-rmse:8.12453\n",
      "[400]\tvalidation_0-rmse:1.00803\tvalidation_1-rmse:8.12223\n",
      "[500]\tvalidation_0-rmse:0.99363\tvalidation_1-rmse:8.12039\n",
      "[600]\tvalidation_0-rmse:0.98224\tvalidation_1-rmse:8.11918\n",
      "[700]\tvalidation_0-rmse:0.97363\tvalidation_1-rmse:8.11829\n",
      "[800]\tvalidation_0-rmse:0.96957\tvalidation_1-rmse:8.11789\n",
      "[900]\tvalidation_0-rmse:0.96579\tvalidation_1-rmse:8.11740\n",
      "[1000]\tvalidation_0-rmse:0.96185\tvalidation_1-rmse:8.11685\n",
      "[1100]\tvalidation_0-rmse:0.95902\tvalidation_1-rmse:8.11675\n",
      "[1200]\tvalidation_0-rmse:0.95608\tvalidation_1-rmse:8.11651\n",
      "[1300]\tvalidation_0-rmse:0.95363\tvalidation_1-rmse:8.11635\n",
      "[1400]\tvalidation_0-rmse:0.95056\tvalidation_1-rmse:8.11590\n",
      "[1500]\tvalidation_0-rmse:0.94810\tvalidation_1-rmse:8.11549\n",
      "[1600]\tvalidation_0-rmse:0.94517\tvalidation_1-rmse:8.11530\n",
      "[1700]\tvalidation_0-rmse:0.94330\tvalidation_1-rmse:8.11492\n",
      "[1800]\tvalidation_0-rmse:0.94043\tvalidation_1-rmse:8.11482\n",
      "[1805]\tvalidation_0-rmse:0.94043\tvalidation_1-rmse:8.11482\n",
      "| \u001b[95m 12      \u001b[0m | \u001b[95m 8.115   \u001b[0m | \u001b[95m 0.5     \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 22.37   \u001b[0m | \u001b[95m 33.13   \u001b[0m | \u001b[95m 16.0    \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 1.806e+0\u001b[0m | \u001b[95m 0.001   \u001b[0m | \u001b[95m 1.0     \u001b[0m |\n",
      "[0]\tvalidation_0-rmse:36.35070\tvalidation_1-rmse:36.12634\n",
      "[100]\tvalidation_0-rmse:13.95667\tvalidation_1-rmse:14.19511\n",
      "[200]\tvalidation_0-rmse:3.51179\tvalidation_1-rmse:8.34218\n",
      "[300]\tvalidation_0-rmse:1.17527\tvalidation_1-rmse:8.08945\n",
      "[400]\tvalidation_0-rmse:0.95149\tvalidation_1-rmse:8.07832\n",
      "[500]\tvalidation_0-rmse:0.90095\tvalidation_1-rmse:8.07400\n",
      "[600]\tvalidation_0-rmse:0.88343\tvalidation_1-rmse:8.07243\n",
      "[700]\tvalidation_0-rmse:0.87140\tvalidation_1-rmse:8.07143\n",
      "[800]\tvalidation_0-rmse:0.86501\tvalidation_1-rmse:8.07100\n",
      "[900]\tvalidation_0-rmse:0.85833\tvalidation_1-rmse:8.07061\n",
      "[1000]\tvalidation_0-rmse:0.85198\tvalidation_1-rmse:8.07022\n",
      "[1100]\tvalidation_0-rmse:0.84617\tvalidation_1-rmse:8.06974\n",
      "[1200]\tvalidation_0-rmse:0.84093\tvalidation_1-rmse:8.06913\n",
      "[1300]\tvalidation_0-rmse:0.83813\tvalidation_1-rmse:8.06893\n",
      "[1400]\tvalidation_0-rmse:0.83588\tvalidation_1-rmse:8.06883\n",
      "[1500]\tvalidation_0-rmse:0.83322\tvalidation_1-rmse:8.06858\n",
      "[1600]\tvalidation_0-rmse:0.83139\tvalidation_1-rmse:8.06838\n",
      "[1700]\tvalidation_0-rmse:0.82848\tvalidation_1-rmse:8.06832\n",
      "[1800]\tvalidation_0-rmse:0.82588\tvalidation_1-rmse:8.06825\n",
      "[1805]\tvalidation_0-rmse:0.82588\tvalidation_1-rmse:8.06825\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 8.068   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 17.2    \u001b[0m | \u001b[0m 13.49   \u001b[0m | \u001b[0m 16.0    \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 1.806e+0\u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "[0]\tvalidation_0-rmse:36.02514\tvalidation_1-rmse:35.80097\n",
      "[100]\tvalidation_0-rmse:8.01454\tvalidation_1-rmse:9.88253\n",
      "[200]\tvalidation_0-rmse:2.94873\tvalidation_1-rmse:8.26680\n",
      "[300]\tvalidation_0-rmse:1.69873\tvalidation_1-rmse:8.16925\n",
      "[400]\tvalidation_0-rmse:1.32595\tvalidation_1-rmse:8.15137\n",
      "[500]\tvalidation_0-rmse:1.17322\tvalidation_1-rmse:8.14329\n",
      "[600]\tvalidation_0-rmse:1.11008\tvalidation_1-rmse:8.14106\n",
      "[680]\tvalidation_0-rmse:1.11007\tvalidation_1-rmse:8.14108\n",
      "| \u001b[95m 14      \u001b[0m | \u001b[95m 8.141   \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 24.12   \u001b[0m | \u001b[95m 29.86   \u001b[0m | \u001b[95m 16.0    \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 1.806e+0\u001b[0m | \u001b[95m 10.0    \u001b[0m | \u001b[95m 1.0     \u001b[0m |\n",
      "[0]\tvalidation_0-rmse:36.15915\tvalidation_1-rmse:35.93490\n",
      "[100]\tvalidation_0-rmse:9.44550\tvalidation_1-rmse:9.99733\n",
      "[200]\tvalidation_0-rmse:5.99050\tvalidation_1-rmse:8.10135\n",
      "[300]\tvalidation_0-rmse:5.03378\tvalidation_1-rmse:8.01370\n",
      "[400]\tvalidation_0-rmse:4.40471\tvalidation_1-rmse:8.00021\n",
      "[500]\tvalidation_0-rmse:3.86323\tvalidation_1-rmse:7.99205\n",
      "[600]\tvalidation_0-rmse:3.41211\tvalidation_1-rmse:7.98715\n",
      "[700]\tvalidation_0-rmse:3.00448\tvalidation_1-rmse:7.98700\n",
      "[736]\tvalidation_0-rmse:2.85950\tvalidation_1-rmse:7.98727\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 7.986   \u001b[0m | \u001b[0m 0.7737  \u001b[0m | \u001b[0m 0.801   \u001b[0m | \u001b[0m 24.38   \u001b[0m | \u001b[0m 23.46   \u001b[0m | \u001b[0m 8.384   \u001b[0m | \u001b[0m 3.763   \u001b[0m | \u001b[0m 1.806e+0\u001b[0m | \u001b[0m 8.664   \u001b[0m | \u001b[0m 0.764   \u001b[0m |\n",
      "[0]\tvalidation_0-rmse:35.95456\tvalidation_1-rmse:35.72982\n",
      "[100]\tvalidation_0-rmse:7.63503\tvalidation_1-rmse:9.79653\n",
      "[200]\tvalidation_0-rmse:2.66036\tvalidation_1-rmse:8.26601\n",
      "[300]\tvalidation_0-rmse:1.54855\tvalidation_1-rmse:8.17375\n",
      "[400]\tvalidation_0-rmse:1.25342\tvalidation_1-rmse:8.15936\n",
      "[500]\tvalidation_0-rmse:1.14417\tvalidation_1-rmse:8.15388\n",
      "[600]\tvalidation_0-rmse:1.12619\tvalidation_1-rmse:8.15288\n",
      "[700]\tvalidation_0-rmse:1.09656\tvalidation_1-rmse:8.15159\n",
      "[759]\tvalidation_0-rmse:1.09657\tvalidation_1-rmse:8.15160\n",
      "| \u001b[95m 16      \u001b[0m | \u001b[95m 8.152   \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 25.11   \u001b[0m | \u001b[95m 35.96   \u001b[0m | \u001b[95m 16.0    \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 1.806e+0\u001b[0m | \u001b[95m 7.738   \u001b[0m | \u001b[95m 1.0     \u001b[0m |\n",
      "[0]\tvalidation_0-rmse:35.94959\tvalidation_1-rmse:35.72575\n",
      "[100]\tvalidation_0-rmse:8.02407\tvalidation_1-rmse:9.77885\n",
      "[200]\tvalidation_0-rmse:3.00777\tvalidation_1-rmse:8.23767\n",
      "[300]\tvalidation_0-rmse:1.66261\tvalidation_1-rmse:8.14352\n",
      "[400]\tvalidation_0-rmse:1.22934\tvalidation_1-rmse:8.12886\n",
      "[500]\tvalidation_0-rmse:1.07292\tvalidation_1-rmse:8.12245\n",
      "[600]\tvalidation_0-rmse:0.99992\tvalidation_1-rmse:8.11917\n",
      "[700]\tvalidation_0-rmse:0.95916\tvalidation_1-rmse:8.11781\n",
      "[800]\tvalidation_0-rmse:0.93424\tvalidation_1-rmse:8.11689\n",
      "[900]\tvalidation_0-rmse:0.91811\tvalidation_1-rmse:8.11642\n",
      "[1000]\tvalidation_0-rmse:0.90491\tvalidation_1-rmse:8.11615\n",
      "[1100]\tvalidation_0-rmse:0.89558\tvalidation_1-rmse:8.11590\n",
      "[1200]\tvalidation_0-rmse:0.88898\tvalidation_1-rmse:8.11564\n",
      "[1300]\tvalidation_0-rmse:0.88357\tvalidation_1-rmse:8.11520\n",
      "[1400]\tvalidation_0-rmse:0.87887\tvalidation_1-rmse:8.11501\n",
      "[1500]\tvalidation_0-rmse:0.87374\tvalidation_1-rmse:8.11482\n",
      "[1600]\tvalidation_0-rmse:0.87074\tvalidation_1-rmse:8.11460\n",
      "[1700]\tvalidation_0-rmse:0.86760\tvalidation_1-rmse:8.11445\n",
      "[1800]\tvalidation_0-rmse:0.86395\tvalidation_1-rmse:8.11439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1803]\tvalidation_0-rmse:0.86395\tvalidation_1-rmse:8.11439\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 8.114   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 17.93   \u001b[0m | \u001b[0m 36.16   \u001b[0m | \u001b[0m 16.0    \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 1.804e+0\u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "[0]\tvalidation_0-rmse:35.93428\tvalidation_1-rmse:35.70999\n",
      "[100]\tvalidation_0-rmse:8.51595\tvalidation_1-rmse:9.73218\n",
      "[200]\tvalidation_0-rmse:4.15781\tvalidation_1-rmse:8.15273\n",
      "[300]\tvalidation_0-rmse:2.75313\tvalidation_1-rmse:8.04786\n",
      "[400]\tvalidation_0-rmse:2.03103\tvalidation_1-rmse:8.03228\n",
      "[500]\tvalidation_0-rmse:1.62759\tvalidation_1-rmse:8.02822\n",
      "[600]\tvalidation_0-rmse:1.42220\tvalidation_1-rmse:8.02559\n",
      "[700]\tvalidation_0-rmse:1.31621\tvalidation_1-rmse:8.02342\n",
      "[800]\tvalidation_0-rmse:1.25402\tvalidation_1-rmse:8.02216\n",
      "[900]\tvalidation_0-rmse:1.21402\tvalidation_1-rmse:8.02185\n",
      "[1000]\tvalidation_0-rmse:1.18952\tvalidation_1-rmse:8.02172\n",
      "[1041]\tvalidation_0-rmse:1.18100\tvalidation_1-rmse:8.02163\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 8.021   \u001b[0m | \u001b[0m 0.8726  \u001b[0m | \u001b[0m 0.802   \u001b[0m | \u001b[0m 26.98   \u001b[0m | \u001b[0m 38.83   \u001b[0m | \u001b[0m 15.19   \u001b[0m | \u001b[0m 6.825   \u001b[0m | \u001b[0m 1.806e+0\u001b[0m | \u001b[0m 9.56    \u001b[0m | \u001b[0m 0.6642  \u001b[0m |\n",
      "[0]\tvalidation_0-rmse:36.00600\tvalidation_1-rmse:35.78182\n",
      "[100]\tvalidation_0-rmse:8.34437\tvalidation_1-rmse:9.95444\n",
      "[200]\tvalidation_0-rmse:3.88964\tvalidation_1-rmse:8.24795\n",
      "[300]\tvalidation_0-rmse:2.59698\tvalidation_1-rmse:8.13235\n",
      "[400]\tvalidation_0-rmse:1.96358\tvalidation_1-rmse:8.11081\n",
      "[500]\tvalidation_0-rmse:1.60224\tvalidation_1-rmse:8.10199\n",
      "[600]\tvalidation_0-rmse:1.42150\tvalidation_1-rmse:8.09769\n",
      "[700]\tvalidation_0-rmse:1.32485\tvalidation_1-rmse:8.09495\n",
      "[800]\tvalidation_0-rmse:1.26680\tvalidation_1-rmse:8.09350\n",
      "[900]\tvalidation_0-rmse:1.22722\tvalidation_1-rmse:8.09244\n",
      "[1000]\tvalidation_0-rmse:1.19984\tvalidation_1-rmse:8.09186\n",
      "[1100]\tvalidation_0-rmse:1.18041\tvalidation_1-rmse:8.09077\n",
      "[1200]\tvalidation_0-rmse:1.16395\tvalidation_1-rmse:8.09005\n",
      "[1300]\tvalidation_0-rmse:1.15079\tvalidation_1-rmse:8.08936\n",
      "[1400]\tvalidation_0-rmse:1.14112\tvalidation_1-rmse:8.08880\n",
      "[1500]\tvalidation_0-rmse:1.13394\tvalidation_1-rmse:8.08861\n",
      "[1592]\tvalidation_0-rmse:1.12931\tvalidation_1-rmse:8.08900\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 8.089   \u001b[0m | \u001b[0m 0.7803  \u001b[0m | \u001b[0m 0.5204  \u001b[0m | \u001b[0m 26.87   \u001b[0m | \u001b[0m 31.12   \u001b[0m | \u001b[0m 15.95   \u001b[0m | \u001b[0m 1.577   \u001b[0m | \u001b[0m 1.805e+0\u001b[0m | \u001b[0m 5.334   \u001b[0m | \u001b[0m 0.5289  \u001b[0m |\n",
      "[0]\tvalidation_0-rmse:35.98286\tvalidation_1-rmse:35.75854\n",
      "[100]\tvalidation_0-rmse:7.53082\tvalidation_1-rmse:9.82470\n",
      "[200]\tvalidation_0-rmse:2.51068\tvalidation_1-rmse:8.23775\n",
      "[300]\tvalidation_0-rmse:1.41655\tvalidation_1-rmse:8.15043\n",
      "[400]\tvalidation_0-rmse:1.14634\tvalidation_1-rmse:8.13524\n",
      "[500]\tvalidation_0-rmse:1.04045\tvalidation_1-rmse:8.12963\n",
      "[600]\tvalidation_0-rmse:1.02269\tvalidation_1-rmse:8.12860\n",
      "[700]\tvalidation_0-rmse:1.00419\tvalidation_1-rmse:8.12826\n",
      "[742]\tvalidation_0-rmse:1.00419\tvalidation_1-rmse:8.12826\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 8.128   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 20.96   \u001b[0m | \u001b[0m 33.16   \u001b[0m | \u001b[0m 16.0    \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 1.806e+0\u001b[0m | \u001b[0m 6.748   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "[0]\tvalidation_0-rmse:35.93118\tvalidation_1-rmse:35.70593\n",
      "[100]\tvalidation_0-rmse:6.75301\tvalidation_1-rmse:9.78690\n",
      "[200]\tvalidation_0-rmse:1.99011\tvalidation_1-rmse:8.25891\n",
      "[300]\tvalidation_0-rmse:1.20416\tvalidation_1-rmse:8.18600\n",
      "[400]\tvalidation_0-rmse:1.05124\tvalidation_1-rmse:8.17481\n",
      "[500]\tvalidation_0-rmse:1.01719\tvalidation_1-rmse:8.17208\n",
      "[560]\tvalidation_0-rmse:1.01719\tvalidation_1-rmse:8.17209\n",
      "| \u001b[95m 21      \u001b[0m | \u001b[95m 8.172   \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 22.11   \u001b[0m | \u001b[95m 39.66   \u001b[0m | \u001b[95m 16.0    \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 1.804e+0\u001b[0m | \u001b[95m 3.371   \u001b[0m | \u001b[95m 1.0     \u001b[0m |\n",
      "[0]\tvalidation_0-rmse:35.92824\tvalidation_1-rmse:35.72065\n",
      "[100]\tvalidation_0-rmse:5.04793\tvalidation_1-rmse:9.47108\n",
      "[200]\tvalidation_0-rmse:1.18304\tvalidation_1-rmse:8.36679\n",
      "[300]\tvalidation_0-rmse:0.92275\tvalidation_1-rmse:8.35310\n",
      "[397]\tvalidation_0-rmse:0.91926\tvalidation_1-rmse:8.35595\n",
      "| \u001b[95m 22      \u001b[0m | \u001b[95m 8.353   \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 17.53   \u001b[0m | \u001b[95m 41.68   \u001b[0m | \u001b[95m 16.0    \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 1.806e+0\u001b[0m | \u001b[95m 0.001   \u001b[0m | \u001b[95m 1.0     \u001b[0m |\n",
      "[0]\tvalidation_0-rmse:35.91959\tvalidation_1-rmse:35.71703\n",
      "[100]\tvalidation_0-rmse:5.28289\tvalidation_1-rmse:9.34281\n",
      "[200]\tvalidation_0-rmse:1.20064\tvalidation_1-rmse:8.13012\n",
      "[300]\tvalidation_0-rmse:0.86326\tvalidation_1-rmse:8.10908\n",
      "[400]\tvalidation_0-rmse:0.82471\tvalidation_1-rmse:8.10668\n",
      "[500]\tvalidation_0-rmse:0.80678\tvalidation_1-rmse:8.10497\n",
      "[600]\tvalidation_0-rmse:0.79566\tvalidation_1-rmse:8.10408\n",
      "[700]\tvalidation_0-rmse:0.78754\tvalidation_1-rmse:8.10325\n",
      "[800]\tvalidation_0-rmse:0.78272\tvalidation_1-rmse:8.10297\n",
      "[900]\tvalidation_0-rmse:0.77666\tvalidation_1-rmse:8.10256\n",
      "[1000]\tvalidation_0-rmse:0.77298\tvalidation_1-rmse:8.10231\n",
      "[1100]\tvalidation_0-rmse:0.76909\tvalidation_1-rmse:8.10216\n",
      "[1200]\tvalidation_0-rmse:0.76615\tvalidation_1-rmse:8.10203\n",
      "[1300]\tvalidation_0-rmse:0.76346\tvalidation_1-rmse:8.10180\n",
      "[1400]\tvalidation_0-rmse:0.76107\tvalidation_1-rmse:8.10161\n",
      "[1500]\tvalidation_0-rmse:0.75770\tvalidation_1-rmse:8.10125\n",
      "[1600]\tvalidation_0-rmse:0.75572\tvalidation_1-rmse:8.10116\n",
      "[1700]\tvalidation_0-rmse:0.75388\tvalidation_1-rmse:8.10107\n",
      "[1800]\tvalidation_0-rmse:0.75249\tvalidation_1-rmse:8.10107\n",
      "[1804]\tvalidation_0-rmse:0.75229\tvalidation_1-rmse:8.10105\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 8.101   \u001b[0m | \u001b[0m 0.5938  \u001b[0m | \u001b[0m 0.9632  \u001b[0m | \u001b[0m 14.03   \u001b[0m | \u001b[0m 44.26   \u001b[0m | \u001b[0m 15.49   \u001b[0m | \u001b[0m 1.425   \u001b[0m | \u001b[0m 1.805e+0\u001b[0m | \u001b[0m 0.1551  \u001b[0m | \u001b[0m 0.9501  \u001b[0m |\n",
      "[0]\tvalidation_0-rmse:35.94014\tvalidation_1-rmse:35.72986\n",
      "[100]\tvalidation_0-rmse:5.08960\tvalidation_1-rmse:9.47918\n",
      "[200]\tvalidation_0-rmse:1.18955\tvalidation_1-rmse:8.39195\n",
      "[300]\tvalidation_0-rmse:0.92587\tvalidation_1-rmse:8.38103\n",
      "[400]\tvalidation_0-rmse:0.91163\tvalidation_1-rmse:8.38231\n",
      "[423]\tvalidation_0-rmse:0.91159\tvalidation_1-rmse:8.38251\n",
      "| \u001b[95m 24      \u001b[0m | \u001b[95m 8.38    \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 18.41   \u001b[0m | \u001b[95m 39.74   \u001b[0m | \u001b[95m 16.0    \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 1.806e+0\u001b[0m | \u001b[95m 0.001   \u001b[0m | \u001b[95m 1.0     \u001b[0m |\n",
      "[0]\tvalidation_0-rmse:35.93489\tvalidation_1-rmse:35.71254\n",
      "[100]\tvalidation_0-rmse:6.76950\tvalidation_1-rmse:9.58531\n",
      "[200]\tvalidation_0-rmse:2.49441\tvalidation_1-rmse:8.09793\n",
      "[300]\tvalidation_0-rmse:1.58740\tvalidation_1-rmse:8.04437\n",
      "[400]\tvalidation_0-rmse:1.24267\tvalidation_1-rmse:8.04021\n",
      "[500]\tvalidation_0-rmse:1.06885\tvalidation_1-rmse:8.03748\n",
      "[600]\tvalidation_0-rmse:0.99221\tvalidation_1-rmse:8.03604\n",
      "[700]\tvalidation_0-rmse:0.95113\tvalidation_1-rmse:8.03472\n",
      "[800]\tvalidation_0-rmse:0.92770\tvalidation_1-rmse:8.03318\n",
      "[900]\tvalidation_0-rmse:0.90999\tvalidation_1-rmse:8.03323\n",
      "[1000]\tvalidation_0-rmse:0.89674\tvalidation_1-rmse:8.03271\n",
      "[1100]\tvalidation_0-rmse:0.88580\tvalidation_1-rmse:8.03243\n",
      "[1148]\tvalidation_0-rmse:0.88191\tvalidation_1-rmse:8.03265\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 8.032   \u001b[0m | \u001b[0m 0.564   \u001b[0m | \u001b[0m 0.8612  \u001b[0m | \u001b[0m 17.09   \u001b[0m | \u001b[0m 40.42   \u001b[0m | \u001b[0m 12.91   \u001b[0m | \u001b[0m 2.596   \u001b[0m | \u001b[0m 1.805e+0\u001b[0m | \u001b[0m 1.5     \u001b[0m | \u001b[0m 0.7361  \u001b[0m |\n",
      "[0]\tvalidation_0-rmse:35.95366\tvalidation_1-rmse:35.73504\n",
      "[100]\tvalidation_0-rmse:6.14579\tvalidation_1-rmse:9.38283\n",
      "[200]\tvalidation_0-rmse:2.13138\tvalidation_1-rmse:8.06831\n",
      "[300]\tvalidation_0-rmse:1.45627\tvalidation_1-rmse:8.04277\n",
      "[400]\tvalidation_0-rmse:1.21251\tvalidation_1-rmse:8.04004\n",
      "[500]\tvalidation_0-rmse:1.08781\tvalidation_1-rmse:8.03858\n",
      "[600]\tvalidation_0-rmse:1.02822\tvalidation_1-rmse:8.03695\n",
      "[700]\tvalidation_0-rmse:0.99142\tvalidation_1-rmse:8.03621\n",
      "[800]\tvalidation_0-rmse:0.97007\tvalidation_1-rmse:8.03598\n",
      "[855]\tvalidation_0-rmse:0.96102\tvalidation_1-rmse:8.03593\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m 8.036   \u001b[0m | \u001b[0m 0.7923  \u001b[0m | \u001b[0m 0.7736  \u001b[0m | \u001b[0m 21.47   \u001b[0m | \u001b[0m 37.81   \u001b[0m | \u001b[0m 15.25   \u001b[0m | \u001b[0m 7.283   \u001b[0m | \u001b[0m 1.806e+0\u001b[0m | \u001b[0m 0.3239  \u001b[0m | \u001b[0m 0.7152  \u001b[0m |\n",
      "[0]\tvalidation_0-rmse:35.91864\tvalidation_1-rmse:35.69710\n",
      "[100]\tvalidation_0-rmse:6.72314\tvalidation_1-rmse:9.69045\n",
      "[200]\tvalidation_0-rmse:2.24106\tvalidation_1-rmse:8.15784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300]\tvalidation_0-rmse:1.38830\tvalidation_1-rmse:8.09331\n",
      "[400]\tvalidation_0-rmse:1.13557\tvalidation_1-rmse:8.08512\n",
      "[500]\tvalidation_0-rmse:1.04162\tvalidation_1-rmse:8.08188\n",
      "[600]\tvalidation_0-rmse:0.99433\tvalidation_1-rmse:8.08012\n",
      "[700]\tvalidation_0-rmse:0.96575\tvalidation_1-rmse:8.07912\n",
      "[800]\tvalidation_0-rmse:0.94854\tvalidation_1-rmse:8.07857\n",
      "[900]\tvalidation_0-rmse:0.93444\tvalidation_1-rmse:8.07771\n",
      "[1000]\tvalidation_0-rmse:0.92517\tvalidation_1-rmse:8.07750\n",
      "[1046]\tvalidation_0-rmse:0.92063\tvalidation_1-rmse:8.07750\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 8.077   \u001b[0m | \u001b[0m 0.8738  \u001b[0m | \u001b[0m 0.9544  \u001b[0m | \u001b[0m 19.59   \u001b[0m | \u001b[0m 47.38   \u001b[0m | \u001b[0m 14.95   \u001b[0m | \u001b[0m 1.35    \u001b[0m | \u001b[0m 1.805e+0\u001b[0m | \u001b[0m 1.818   \u001b[0m | \u001b[0m 0.7323  \u001b[0m |\n",
      "[0]\tvalidation_0-rmse:35.94818\tvalidation_1-rmse:35.72638\n",
      "[100]\tvalidation_0-rmse:6.46825\tvalidation_1-rmse:9.58844\n",
      "[200]\tvalidation_0-rmse:2.14401\tvalidation_1-rmse:8.12038\n",
      "[300]\tvalidation_0-rmse:1.37342\tvalidation_1-rmse:8.07083\n",
      "[400]\tvalidation_0-rmse:1.15869\tvalidation_1-rmse:8.06545\n",
      "[500]\tvalidation_0-rmse:1.07564\tvalidation_1-rmse:8.06240\n",
      "[600]\tvalidation_0-rmse:1.03627\tvalidation_1-rmse:8.06119\n",
      "[700]\tvalidation_0-rmse:1.01037\tvalidation_1-rmse:8.06067\n",
      "[800]\tvalidation_0-rmse:0.99354\tvalidation_1-rmse:8.06044\n",
      "[900]\tvalidation_0-rmse:0.97953\tvalidation_1-rmse:8.05948\n",
      "[1000]\tvalidation_0-rmse:0.97086\tvalidation_1-rmse:8.05942\n",
      "[1094]\tvalidation_0-rmse:0.96327\tvalidation_1-rmse:8.05941\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m 8.059   \u001b[0m | \u001b[0m 0.7527  \u001b[0m | \u001b[0m 0.7031  \u001b[0m | \u001b[0m 20.89   \u001b[0m | \u001b[0m 37.99   \u001b[0m | \u001b[0m 15.19   \u001b[0m | \u001b[0m 1.847   \u001b[0m | \u001b[0m 1.805e+0\u001b[0m | \u001b[0m 0.9159  \u001b[0m | \u001b[0m 0.6806  \u001b[0m |\n",
      "[0]\tvalidation_0-rmse:35.95710\tvalidation_1-rmse:35.73373\n",
      "[100]\tvalidation_0-rmse:8.24844\tvalidation_1-rmse:9.78643\n",
      "[200]\tvalidation_0-rmse:3.57157\tvalidation_1-rmse:8.18771\n",
      "[300]\tvalidation_0-rmse:2.19328\tvalidation_1-rmse:8.08456\n",
      "[400]\tvalidation_0-rmse:1.59439\tvalidation_1-rmse:8.07474\n",
      "[500]\tvalidation_0-rmse:1.32216\tvalidation_1-rmse:8.06886\n",
      "[600]\tvalidation_0-rmse:1.19585\tvalidation_1-rmse:8.06515\n",
      "[700]\tvalidation_0-rmse:1.13086\tvalidation_1-rmse:8.06384\n",
      "[800]\tvalidation_0-rmse:1.09179\tvalidation_1-rmse:8.06283\n",
      "[900]\tvalidation_0-rmse:1.06345\tvalidation_1-rmse:8.06247\n",
      "[1000]\tvalidation_0-rmse:1.04330\tvalidation_1-rmse:8.06163\n",
      "[1100]\tvalidation_0-rmse:1.02975\tvalidation_1-rmse:8.06139\n",
      "[1200]\tvalidation_0-rmse:1.01870\tvalidation_1-rmse:8.06114\n",
      "[1300]\tvalidation_0-rmse:1.01118\tvalidation_1-rmse:8.06084\n",
      "[1400]\tvalidation_0-rmse:1.00470\tvalidation_1-rmse:8.06072\n",
      "[1500]\tvalidation_0-rmse:0.99778\tvalidation_1-rmse:8.06070\n",
      "[1570]\tvalidation_0-rmse:0.99392\tvalidation_1-rmse:8.06084\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m 8.061   \u001b[0m | \u001b[0m 0.9226  \u001b[0m | \u001b[0m 0.515   \u001b[0m | \u001b[0m 21.67   \u001b[0m | \u001b[0m 35.12   \u001b[0m | \u001b[0m 15.52   \u001b[0m | \u001b[0m 5.006   \u001b[0m | \u001b[0m 1.804e+0\u001b[0m | \u001b[0m 8.113   \u001b[0m | \u001b[0m 0.7458  \u001b[0m |\n",
      "[0]\tvalidation_0-rmse:35.93163\tvalidation_1-rmse:35.70717\n",
      "[100]\tvalidation_0-rmse:7.07007\tvalidation_1-rmse:9.76989\n",
      "[200]\tvalidation_0-rmse:2.26250\tvalidation_1-rmse:8.20481\n",
      "[300]\tvalidation_0-rmse:1.31343\tvalidation_1-rmse:8.12527\n",
      "[400]\tvalidation_0-rmse:1.07408\tvalidation_1-rmse:8.11265\n",
      "[500]\tvalidation_0-rmse:0.98394\tvalidation_1-rmse:8.10727\n",
      "[600]\tvalidation_0-rmse:0.93923\tvalidation_1-rmse:8.10552\n",
      "[700]\tvalidation_0-rmse:0.91536\tvalidation_1-rmse:8.10429\n",
      "[800]\tvalidation_0-rmse:0.89878\tvalidation_1-rmse:8.10368\n",
      "[900]\tvalidation_0-rmse:0.88884\tvalidation_1-rmse:8.10308\n",
      "[1000]\tvalidation_0-rmse:0.87873\tvalidation_1-rmse:8.10278\n",
      "[1072]\tvalidation_0-rmse:0.87350\tvalidation_1-rmse:8.10273\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m 8.103   \u001b[0m | \u001b[0m 0.5444  \u001b[0m | \u001b[0m 0.7408  \u001b[0m | \u001b[0m 18.2    \u001b[0m | \u001b[0m 40.31   \u001b[0m | \u001b[0m 15.34   \u001b[0m | \u001b[0m 1.926   \u001b[0m | \u001b[0m 1.804e+0\u001b[0m | \u001b[0m 3.797   \u001b[0m | \u001b[0m 0.9502  \u001b[0m |\n",
      "=====================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "xgbBO = BayesianOptimization(f=xgb_rmse_eval, pbounds=bayesian_params, random_state=0)\n",
    "xgbBO.maximize(init_points=5, n_iter=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'target': 7.985279821547924,\n",
       "  'params': {'colsample_bylevel': 0.7744067519636624,\n",
       "   'colsample_bytree': 0.8575946831862098,\n",
       "   'gamma': 60.276337607164386,\n",
       "   'max_delta_step': 27.699275966847946,\n",
       "   'max_depth': 11.389238394711239,\n",
       "   'min_child_weight': 32.64881154026615,\n",
       "   'n_estimators': 1804.8751744225253,\n",
       "   'reg_lambda': 8.917838234820016,\n",
       "   'subsample': 0.9818313802505146}},\n",
       " {'target': 8.039493857355893,\n",
       "  'params': {'colsample_bylevel': 0.6917207594128889,\n",
       "   'colsample_bytree': 0.8958625190413323,\n",
       "   'gamma': 52.88949197529045,\n",
       "   'max_delta_step': 28.834183493602684,\n",
       "   'max_depth': 15.404773106341288,\n",
       "   'min_child_weight': 4.48076685169646,\n",
       "   'n_estimators': 1804.1742585994032,\n",
       "   'reg_lambda': 0.20316375600581688,\n",
       "   'subsample': 0.916309922773969}},\n",
       " {'target': 7.987958221777498,\n",
       "  'params': {'colsample_bylevel': 0.8890783754749252,\n",
       "   'colsample_bytree': 0.9350060741234096,\n",
       "   'gamma': 97.8618342232764,\n",
       "   'max_delta_step': 40.158769646619454,\n",
       "   'max_depth': 11.691834898023455,\n",
       "   'min_child_weight': 39.245929638036316,\n",
       "   'n_estimators': 1804.2365488517378,\n",
       "   'reg_lambda': 6.3995702922539115,\n",
       "   'subsample': 0.5716766437045232}},\n",
       " {'target': 8.004578012351246,\n",
       "  'params': {'colsample_bylevel': 0.972334458524792,\n",
       "   'colsample_bytree': 0.7609241608750359,\n",
       "   'gamma': 41.46619399905236,\n",
       "   'max_delta_step': 13.96322499312672,\n",
       "   'max_depth': 14.193869515473732,\n",
       "   'min_child_weight': 23.35136627861088,\n",
       "   'n_estimators': 1805.1368678977374,\n",
       "   'reg_lambda': 0.18887921456311507,\n",
       "   'subsample': 0.8088177485379385}},\n",
       " {'target': 7.9667055983526565,\n",
       "  'params': {'colsample_bylevel': 0.8060478613612108,\n",
       "   'colsample_bytree': 0.8084669984373785,\n",
       "   'gamma': 94.37480785146242,\n",
       "   'max_delta_step': 34.40919465607069,\n",
       "   'max_depth': 10.876063204590288,\n",
       "   'min_child_weight': 22.41456573616773,\n",
       "   'n_estimators': 1805.3952623918544,\n",
       "   'reg_lambda': 0.6031944908210691,\n",
       "   'subsample': 0.8333833577228338}},\n",
       " {'target': 8.066990347195242,\n",
       "  'params': {'colsample_bylevel': 0.6987120240246601,\n",
       "   'colsample_bytree': 0.8956395265445477,\n",
       "   'gamma': 40.986099970129935,\n",
       "   'max_delta_step': 30.373846562134073,\n",
       "   'max_depth': 16.0,\n",
       "   'min_child_weight': 1.0,\n",
       "   'n_estimators': 1804.0,\n",
       "   'reg_lambda': 0.001,\n",
       "   'subsample': 0.8122019248188107}},\n",
       " {'target': 8.087074141454277,\n",
       "  'params': {'colsample_bylevel': 0.5,\n",
       "   'colsample_bytree': 1.0,\n",
       "   'gamma': 25.981619124876893,\n",
       "   'max_delta_step': 50.0,\n",
       "   'max_depth': 16.0,\n",
       "   'min_child_weight': 1.0,\n",
       "   'n_estimators': 1804.0,\n",
       "   'reg_lambda': 0.001,\n",
       "   'subsample': 0.7515702279287907}},\n",
       " {'target': 8.020138691469592,\n",
       "  'params': {'colsample_bylevel': 0.5,\n",
       "   'colsample_bytree': 0.5,\n",
       "   'gamma': 0.0,\n",
       "   'max_delta_step': 36.61956604094249,\n",
       "   'max_depth': 8.0,\n",
       "   'min_child_weight': 1.0,\n",
       "   'n_estimators': 1804.0,\n",
       "   'reg_lambda': 10.0,\n",
       "   'subsample': 0.5}},\n",
       " {'target': 8.019474864959104,\n",
       "  'params': {'colsample_bylevel': 1.0,\n",
       "   'colsample_bytree': 0.5,\n",
       "   'gamma': 39.65401001577019,\n",
       "   'max_delta_step': 50.0,\n",
       "   'max_depth': 8.0,\n",
       "   'min_child_weight': 1.0,\n",
       "   'n_estimators': 1806.0,\n",
       "   'reg_lambda': 10.0,\n",
       "   'subsample': 0.5}},\n",
       " {'target': 8.03740093982891,\n",
       "  'params': {'colsample_bylevel': 0.5,\n",
       "   'colsample_bytree': 1.0,\n",
       "   'gamma': 19.540438640122805,\n",
       "   'max_delta_step': 50.0,\n",
       "   'max_depth': 16.0,\n",
       "   'min_child_weight': 17.010857489049823,\n",
       "   'n_estimators': 1804.0,\n",
       "   'reg_lambda': 0.001,\n",
       "   'subsample': 1.0}},\n",
       " {'target': 8.075162485065709,\n",
       "  'params': {'colsample_bylevel': 0.6928161933038453,\n",
       "   'colsample_bytree': 0.5521890399171105,\n",
       "   'gamma': 38.543641507214296,\n",
       "   'max_delta_step': 27.62436921330537,\n",
       "   'max_depth': 15.895158556171124,\n",
       "   'min_child_weight': 2.1952661924151275,\n",
       "   'n_estimators': 1804.9914034127712,\n",
       "   'reg_lambda': 0.26874487443933326,\n",
       "   'subsample': 0.8386164457491582}},\n",
       " {'target': 8.114785340497294,\n",
       "  'params': {'colsample_bylevel': 0.5,\n",
       "   'colsample_bytree': 1.0,\n",
       "   'gamma': 22.366124302370718,\n",
       "   'max_delta_step': 33.12556708044323,\n",
       "   'max_depth': 16.0,\n",
       "   'min_child_weight': 1.0,\n",
       "   'n_estimators': 1806.0,\n",
       "   'reg_lambda': 0.001,\n",
       "   'subsample': 1.0}},\n",
       " {'target': 8.06818526895114,\n",
       "  'params': {'colsample_bylevel': 0.5,\n",
       "   'colsample_bytree': 1.0,\n",
       "   'gamma': 17.19743307638879,\n",
       "   'max_delta_step': 13.491251465747998,\n",
       "   'max_depth': 16.0,\n",
       "   'min_child_weight': 1.0,\n",
       "   'n_estimators': 1806.0,\n",
       "   'reg_lambda': 0.001,\n",
       "   'subsample': 1.0}},\n",
       " {'target': 8.141043051195755,\n",
       "  'params': {'colsample_bylevel': 1.0,\n",
       "   'colsample_bytree': 1.0,\n",
       "   'gamma': 24.120369152725775,\n",
       "   'max_delta_step': 29.858638800485608,\n",
       "   'max_depth': 16.0,\n",
       "   'min_child_weight': 1.0,\n",
       "   'n_estimators': 1806.0,\n",
       "   'reg_lambda': 10.0,\n",
       "   'subsample': 1.0}},\n",
       " {'target': 7.986033131114647,\n",
       "  'params': {'colsample_bylevel': 0.7737249017610173,\n",
       "   'colsample_bytree': 0.8009972627229118,\n",
       "   'gamma': 24.37601163698877,\n",
       "   'max_delta_step': 23.4583231847957,\n",
       "   'max_depth': 8.383573157626945,\n",
       "   'min_child_weight': 3.762871874738101,\n",
       "   'n_estimators': 1805.5991617076768,\n",
       "   'reg_lambda': 8.663771219335167,\n",
       "   'subsample': 0.764042130630451}},\n",
       " {'target': 8.151511845430413,\n",
       "  'params': {'colsample_bylevel': 1.0,\n",
       "   'colsample_bytree': 1.0,\n",
       "   'gamma': 25.110160923373414,\n",
       "   'max_delta_step': 35.96022728353258,\n",
       "   'max_depth': 16.0,\n",
       "   'min_child_weight': 1.0,\n",
       "   'n_estimators': 1805.8631840251348,\n",
       "   'reg_lambda': 7.738346943585153,\n",
       "   'subsample': 1.0}},\n",
       " {'target': 8.114367418846463,\n",
       "  'params': {'colsample_bylevel': 1.0,\n",
       "   'colsample_bytree': 0.5,\n",
       "   'gamma': 17.93379828844877,\n",
       "   'max_delta_step': 36.15599632204152,\n",
       "   'max_depth': 16.0,\n",
       "   'min_child_weight': 1.0,\n",
       "   'n_estimators': 1804.0,\n",
       "   'reg_lambda': 10.0,\n",
       "   'subsample': 1.0}},\n",
       " {'target': 8.021364567686216,\n",
       "  'params': {'colsample_bylevel': 0.8725602598798605,\n",
       "   'colsample_bytree': 0.8019823936227612,\n",
       "   'gamma': 26.975033307498776,\n",
       "   'max_delta_step': 38.82914809931374,\n",
       "   'max_depth': 15.190474471681078,\n",
       "   'min_child_weight': 6.825177315209741,\n",
       "   'n_estimators': 1805.9920268030066,\n",
       "   'reg_lambda': 9.55966043486307,\n",
       "   'subsample': 0.6642388855509891}},\n",
       " {'target': 8.08858086681745,\n",
       "  'params': {'colsample_bylevel': 0.7802838132862546,\n",
       "   'colsample_bytree': 0.5203981769780845,\n",
       "   'gamma': 26.865898090585638,\n",
       "   'max_delta_step': 31.122910684718157,\n",
       "   'max_depth': 15.954665035054422,\n",
       "   'min_child_weight': 1.5765698853755912,\n",
       "   'n_estimators': 1804.991327590132,\n",
       "   'reg_lambda': 5.33436748108618,\n",
       "   'subsample': 0.5289212867006365}},\n",
       " {'target': 8.12825112250086,\n",
       "  'params': {'colsample_bylevel': 1.0,\n",
       "   'colsample_bytree': 1.0,\n",
       "   'gamma': 20.963487879623578,\n",
       "   'max_delta_step': 33.157673896444,\n",
       "   'max_depth': 16.0,\n",
       "   'min_child_weight': 1.0,\n",
       "   'n_estimators': 1806.0,\n",
       "   'reg_lambda': 6.747636961538817,\n",
       "   'subsample': 1.0}},\n",
       " {'target': 8.172066407069503,\n",
       "  'params': {'colsample_bylevel': 1.0,\n",
       "   'colsample_bytree': 1.0,\n",
       "   'gamma': 22.113083785017782,\n",
       "   'max_delta_step': 39.65564423171253,\n",
       "   'max_depth': 16.0,\n",
       "   'min_child_weight': 1.0,\n",
       "   'n_estimators': 1804.0,\n",
       "   'reg_lambda': 3.3712074221158357,\n",
       "   'subsample': 1.0}},\n",
       " {'target': 8.352975233349872,\n",
       "  'params': {'colsample_bylevel': 1.0,\n",
       "   'colsample_bytree': 1.0,\n",
       "   'gamma': 17.528318210313852,\n",
       "   'max_delta_step': 41.68166710523737,\n",
       "   'max_depth': 16.0,\n",
       "   'min_child_weight': 1.0,\n",
       "   'n_estimators': 1806.0,\n",
       "   'reg_lambda': 0.001,\n",
       "   'subsample': 1.0}},\n",
       " {'target': 8.10101170712333,\n",
       "  'params': {'colsample_bylevel': 0.5938485160608066,\n",
       "   'colsample_bytree': 0.9632059455601734,\n",
       "   'gamma': 14.028848720704968,\n",
       "   'max_delta_step': 44.25630116644675,\n",
       "   'max_depth': 15.491301001188997,\n",
       "   'min_child_weight': 1.4250417722543296,\n",
       "   'n_estimators': 1804.8122878286406,\n",
       "   'reg_lambda': 0.1550773194682379,\n",
       "   'subsample': 0.9501176214218015}},\n",
       " {'target': 8.38046702776314,\n",
       "  'params': {'colsample_bylevel': 1.0,\n",
       "   'colsample_bytree': 1.0,\n",
       "   'gamma': 18.408396583162457,\n",
       "   'max_delta_step': 39.744007817975486,\n",
       "   'max_depth': 16.0,\n",
       "   'min_child_weight': 1.0,\n",
       "   'n_estimators': 1806.0,\n",
       "   'reg_lambda': 0.001,\n",
       "   'subsample': 1.0}},\n",
       " {'target': 8.032386848398877,\n",
       "  'params': {'colsample_bylevel': 0.5639903994595226,\n",
       "   'colsample_bytree': 0.861202512108542,\n",
       "   'gamma': 17.087149709084414,\n",
       "   'max_delta_step': 40.41855305903776,\n",
       "   'max_depth': 12.907766369373395,\n",
       "   'min_child_weight': 2.595902130626451,\n",
       "   'n_estimators': 1805.0728491625916,\n",
       "   'reg_lambda': 1.5001731366744537,\n",
       "   'subsample': 0.7361443210026358}},\n",
       " {'target': 8.03573596881236,\n",
       "  'params': {'colsample_bylevel': 0.7923157630046651,\n",
       "   'colsample_bytree': 0.7736439874471629,\n",
       "   'gamma': 21.467443080504168,\n",
       "   'max_delta_step': 37.81482078741947,\n",
       "   'max_depth': 15.25205655480534,\n",
       "   'min_child_weight': 7.282918639702323,\n",
       "   'n_estimators': 1805.9238755257995,\n",
       "   'reg_lambda': 0.3238945469132343,\n",
       "   'subsample': 0.7151548165558379}},\n",
       " {'target': 8.077220009036964,\n",
       "  'params': {'colsample_bylevel': 0.8738255218070556,\n",
       "   'colsample_bytree': 0.9543791891222004,\n",
       "   'gamma': 19.58759582513052,\n",
       "   'max_delta_step': 47.38406292839766,\n",
       "   'max_depth': 14.949588281379917,\n",
       "   'min_child_weight': 1.3503320376890233,\n",
       "   'n_estimators': 1804.5890146076401,\n",
       "   'reg_lambda': 1.8182000554976854,\n",
       "   'subsample': 0.7322607682810923}},\n",
       " {'target': 8.059329786618648,\n",
       "  'params': {'colsample_bylevel': 0.7526718960441574,\n",
       "   'colsample_bytree': 0.7030901440513745,\n",
       "   'gamma': 20.893815227541257,\n",
       "   'max_delta_step': 37.9917240332418,\n",
       "   'max_depth': 15.191034459742898,\n",
       "   'min_child_weight': 1.846842456825899,\n",
       "   'n_estimators': 1804.5541677595686,\n",
       "   'reg_lambda': 0.9158672740234236,\n",
       "   'subsample': 0.6805625046120496}},\n",
       " {'target': 8.060558628145293,\n",
       "  'params': {'colsample_bylevel': 0.9225830030733398,\n",
       "   'colsample_bytree': 0.5150443894073471,\n",
       "   'gamma': 21.66967019443814,\n",
       "   'max_delta_step': 35.11595127612692,\n",
       "   'max_depth': 15.521860026308367,\n",
       "   'min_child_weight': 5.0058173853982675,\n",
       "   'n_estimators': 1804.4486337638557,\n",
       "   'reg_lambda': 8.11277754640483,\n",
       "   'subsample': 0.7457535474348715}},\n",
       " {'target': 8.102708572220232,\n",
       "  'params': {'colsample_bylevel': 0.5443658002867995,\n",
       "   'colsample_bytree': 0.7408431942584,\n",
       "   'gamma': 18.204201463553993,\n",
       "   'max_delta_step': 40.31032822315992,\n",
       "   'max_depth': 15.343941075125642,\n",
       "   'min_child_weight': 1.9261521028431523,\n",
       "   'n_estimators': 1804.1673637698832,\n",
       "   'reg_lambda': 3.797304098761622,\n",
       "   'subsample': 0.9501515265759662}}]"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbBO.res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.985279821547924, 8.039493857355893, 7.987958221777498, 8.004578012351246, 7.9667055983526565, 8.066990347195242, 8.087074141454277, 8.020138691469592, 8.019474864959104, 8.03740093982891, 8.075162485065709, 8.114785340497294, 8.06818526895114, 8.141043051195755, 7.986033131114647, 8.151511845430413, 8.114367418846463, 8.021364567686216, 8.08858086681745, 8.12825112250086, 8.172066407069503, 8.352975233349872, 8.10101170712333, 8.38046702776314, 8.032386848398877, 8.03573596881236, 8.077220009036964, 8.059329786618648, 8.060558628145293, 8.102708572220232]\n",
      "maximum target index: 4\n"
     ]
    }
   ],
   "source": [
    "# dictionary에 있는 target값을 모두 추출\n",
    "target_list = []\n",
    "for result in xgbBO.res:\n",
    "    target = result['target']\n",
    "    target_list.append(target)\n",
    "print(target_list)\n",
    "# 가장 큰 target 값을 가지는 순번(index)를 추출\n",
    "print('maximum target index:', np.argmin(np.array(target_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'target': 7.9667055983526565, 'params': {'colsample_bylevel': 0.8060478613612108, 'colsample_bytree': 0.8084669984373785, 'gamma': 94.37480785146242, 'max_delta_step': 34.40919465607069, 'max_depth': 10.876063204590288, 'min_child_weight': 22.41456573616773, 'n_estimators': 1805.3952623918544, 'reg_lambda': 0.6031944908210691, 'subsample': 0.8333833577228338}}\n"
     ]
    }
   ],
   "source": [
    "# 가장 큰 target값을 가지는 index값을 기준으로 res에서 해당 parameter 추출. \n",
    "max_dict = xgbBO.res[np.argmin(np.array(target_list))]\n",
    "print(max_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr = X_train_select\n",
    "target =  y['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def train_apps_all_with_oof(ftr, target, nfolds=5):\n",
    "    ftr = ftr\n",
    "    target = target\n",
    "\n",
    "    # nfolds 개의 cross validatin fold set을 가지는 KFold 생성 \n",
    "    folds = KFold(n_splits=nfolds, shuffle=True, random_state=0)\n",
    "    \n",
    "    # Out of Folds로 학습된 모델의 validation set을 예측하여 결과 확률을 담을 array 생성.\n",
    "    # validation set가 n_split갯수만큼 있으므로 크기는 ftr_app의 크기가 되어야 함. \n",
    "    oof_preds = np.zeros((ftr.shape[0],))  \n",
    "    \n",
    "    # Ouf of Folds로 학습된 모델의 test dataset을 예측하여 결과 확률을 담을 array 생성. \n",
    "    test_preds = np.zeros(((X_test_select.shape[0],)))\n",
    "    \n",
    "    # n_estimators를 4000까지 확대. \n",
    "    clf = XGBRegressor(subsample =  0.5716766437045232, \n",
    "                       silent = -1, reg_lambda = 6.3995702922539115, \n",
    "                       n_estimators = 1804, min_child_weight = 39.245929638036316, \n",
    "                       max_depth = 12, learning_rate = 0.01, gamma = 97.8618342232764, \n",
    "                       colsample_bytree = 0.9350060741234096, colsample_bylevel = 0.8890783754749252,\n",
    "                       max_delta_step= 40.158769646619454)\n",
    "\n",
    "\n",
    "    # nfolds 번 cross validation Iteration 반복하면서 OOF 방식으로 학습 및 테스트 데이터 예측\n",
    "    for fold_idx, (train_idx, valid_idx) in enumerate(folds.split(ftr)):\n",
    "        print('##### iteration ', fold_idx, ' 시작')\n",
    "        # 학습용 데이터 세트의 인덱스와 검증용 데이터 세트의 인덱스 추출하여 이를 기반으로 학습/검증 데이터 추출\n",
    "        train_x = ftr.iloc[train_idx, :]\n",
    "        train_y = target.iloc[train_idx]\n",
    "        valid_x = ftr.iloc[valid_idx, :]\n",
    "        valid_y = target.iloc[valid_idx]\n",
    "        \n",
    "        # 추출된 학습/검증 데이터 세트로 모델 학습. early_stopping은 200으로 증가. \n",
    "        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], eval_metric= 'rmse', verbose= 200, \n",
    "                early_stopping_rounds= 200)\n",
    "        # 검증 데이터 세트로 예측된 확률 저장. 사용되지는 않음. \n",
    "        #oof_preds[valid_idx] = clf.predict(valid_x, num_iteration=clf.best_iteration_)       \n",
    "        # 학습된 모델로 테스트 데이터 세트에 예측 확률 계산. \n",
    "        # nfolds 번 반복 실행하므로 평균 확률을 구하기 위해 개별 수행시 마다 수행 횟수로 나눈 확률을 추후에 더해서 최종 평균 확률 계산. \n",
    "        test_preds += clf.predict(X_test_select)/folds.n_splits\n",
    "        \n",
    "        \n",
    "    return clf, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### iteration  0  시작\n",
      "[0]\tvalidation_0-rmse:36.22778\tvalidation_1-rmse:36.17917\n",
      "[200]\tvalidation_0-rmse:7.43421\tvalidation_1-rmse:9.43116\n",
      "[400]\tvalidation_0-rmse:4.20120\tvalidation_1-rmse:8.08235\n",
      "[600]\tvalidation_0-rmse:3.56227\tvalidation_1-rmse:8.04700\n",
      "[800]\tvalidation_0-rmse:3.15434\tvalidation_1-rmse:8.03798\n",
      "[1000]\tvalidation_0-rmse:2.84083\tvalidation_1-rmse:8.03036\n",
      "[1200]\tvalidation_0-rmse:2.59923\tvalidation_1-rmse:8.02655\n",
      "[1400]\tvalidation_0-rmse:2.43166\tvalidation_1-rmse:8.02384\n",
      "[1600]\tvalidation_0-rmse:2.31390\tvalidation_1-rmse:8.02187\n",
      "[1799]\tvalidation_0-rmse:2.23409\tvalidation_1-rmse:8.02146\n",
      "##### iteration  1  시작\n",
      "[0]\tvalidation_0-rmse:36.27621\tvalidation_1-rmse:35.98660\n",
      "[200]\tvalidation_0-rmse:7.45058\tvalidation_1-rmse:9.36272\n",
      "[400]\tvalidation_0-rmse:4.18650\tvalidation_1-rmse:7.90817\n",
      "[600]\tvalidation_0-rmse:3.53523\tvalidation_1-rmse:7.85063\n",
      "[800]\tvalidation_0-rmse:3.13855\tvalidation_1-rmse:7.83588\n",
      "[1000]\tvalidation_0-rmse:2.84510\tvalidation_1-rmse:7.82856\n",
      "[1200]\tvalidation_0-rmse:2.60907\tvalidation_1-rmse:7.82457\n",
      "[1400]\tvalidation_0-rmse:2.43513\tvalidation_1-rmse:7.82176\n",
      "[1600]\tvalidation_0-rmse:2.31756\tvalidation_1-rmse:7.81921\n",
      "[1799]\tvalidation_0-rmse:2.23650\tvalidation_1-rmse:7.81806\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-430-f9bdef8d039c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_preds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_apps_all_with_oof\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mftr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnfolds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-429-3b2e5a8613da>\u001b[0m in \u001b[0;36mtrain_apps_all_with_oof\u001b[1;34m(ftr, target, nfolds)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;31m# 추출된 학습/검증 데이터 세트로 모델 학습. early_stopping은 200으로 증가.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], eval_metric= 'rmse', verbose= 200, \n\u001b[0m\u001b[0;32m     41\u001b[0m                 early_stopping_rounds= 200)\n\u001b[0;32m     42\u001b[0m         \u001b[1;31m# 검증 데이터 세트로 예측된 확률 저장. 사용되지는 않음.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    431\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m    734\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_configure_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m         self._Booster = train(\n\u001b[0m\u001b[0;32m    737\u001b[0m             \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m             \u001b[0mtrain_dmatrix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[0mBooster\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtrained\u001b[0m \u001b[0mbooster\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m     \"\"\"\n\u001b[1;32m--> 189\u001b[1;33m     bst = _train_internal(params, dtrain,\n\u001b[0m\u001b[0;32m    190\u001b[0m                           \u001b[0mnum_boost_round\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_boost_round\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m                           \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[1;31m# Copy to serialise and unserialise booster to reset state and free\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;31m# training memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1346\u001b[0m             \u001b[0ma\u001b[0m \u001b[0mcopied\u001b[0m \u001b[0mbooster\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1347\u001b[0m         \"\"\"\n\u001b[1;32m-> 1348\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__copy__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1350\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m__copy__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__copy__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__deepcopy__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__deepcopy__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m__deepcopy__\u001b[1;34m(self, _)\u001b[0m\n\u001b[0;32m   1336\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__deepcopy__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1337\u001b[0m         \u001b[1;34m'''Return a copy of booster.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1338\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mBooster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1340\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, params, cache, model_file)\u001b[0m\n\u001b[0;32m   1208\u001b[0m             \u001b[1;31m# We use the pickle interface for getting memory snapshot from\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1209\u001b[0m             \u001b[1;31m# another model, and load the snapshot with this booster.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1210\u001b[1;33m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getstate__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1211\u001b[0m             \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'handle'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1212\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'handle'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m__getstate__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1246\u001b[0m             \u001b[0mlength\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_bst_ulong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1247\u001b[0m             \u001b[0mcptr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPOINTER\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_char\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1248\u001b[1;33m             _check_call(_LIB.XGBoosterSerializeToBuffer(self.handle,\n\u001b[0m\u001b[0;32m   1249\u001b[0m                                                         \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1250\u001b[0m                                                         ctypes.byref(cptr)))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clf, test_preds = train_apps_all_with_oof(ftr, target, nfolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([39.86416674, 42.71971416, 28.57365751, ..., 36.07904482,\n",
       "       34.42722082, 26.97699118])"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'submission_0615_1118.csv' is ready to submit.\n"
     ]
    }
   ],
   "source": [
    "t = pd.Timestamp.now()\n",
    "fname = f\"submission_{t.month:02}{t.day:02}_{t.hour:02}{t.minute:02}.csv\"\n",
    "submissions = pd.concat([pd.Series(IDtest, name=\"custid\"), pd.Series(test_preds, name=\"age\")] ,axis=1)\n",
    "submissions.to_csv(fname, index=False)\n",
    "print(\"'{}' is ready to submit.\" .format(fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr = X_train_select\n",
    "target =  y['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def train_apps_all_with_oof(ftr, target, nfolds=5):\n",
    "    ftr = ftr\n",
    "    target = target\n",
    "\n",
    "    # nfolds 개의 cross validatin fold set을 가지는 KFold 생성 \n",
    "    folds = KFold(n_splits=nfolds, shuffle=True, random_state=0)\n",
    "    \n",
    "    # Out of Folds로 학습된 모델의 validation set을 예측하여 결과 확률을 담을 array 생성.\n",
    "    # validation set가 n_split갯수만큼 있으므로 크기는 ftr_app의 크기가 되어야 함. \n",
    "    oof_preds = np.zeros((ftr.shape[0],))  \n",
    "    \n",
    "    # Ouf of Folds로 학습된 모델의 test dataset을 예측하여 결과 확률을 담을 array 생성. \n",
    "    test_preds = np.zeros(((X_test_select.shape[0],)))\n",
    "    \n",
    "    # n_estimators를 4000까지 확대. \n",
    "    clf = CatBoostRegressor(random_state=0,eval_metric='RMSE',loss_function='RMSE')\n",
    "\n",
    "    # nfolds 번 cross validation Iteration 반복하면서 OOF 방식으로 학습 및 테스트 데이터 예측\n",
    "    for fold_idx, (train_idx, valid_idx) in enumerate(folds.split(ftr)):\n",
    "        print('##### iteration ', fold_idx, ' 시작')\n",
    "        # 학습용 데이터 세트의 인덱스와 검증용 데이터 세트의 인덱스 추출하여 이를 기반으로 학습/검증 데이터 추출\n",
    "        train_x = ftr.iloc[train_idx, :]\n",
    "        train_y = target.iloc[train_idx]\n",
    "        valid_x = ftr.iloc[valid_idx, :]\n",
    "        valid_y = target.iloc[valid_idx]\n",
    "        \n",
    "        # 추출된 학습/검증 데이터 세트로 모델 학습. early_stopping은 200으로 증가. \n",
    "        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], use_best_model=True, verbose= 200, \n",
    "                early_stopping_rounds= 200)\n",
    "        # 검증 데이터 세트로 예측된 확률 저장. 사용되지는 않음. \n",
    "        #oof_preds[valid_idx] = clf.predict(valid_x, num_iteration=clf.best_iteration_)       \n",
    "        # 학습된 모델로 테스트 데이터 세트에 예측 확률 계산. \n",
    "        # nfolds 번 반복 실행하므로 평균 확률을 구하기 위해 개별 수행시 마다 수행 횟수로 나눈 확률을 추후에 더해서 최종 평균 확률 계산. \n",
    "        test_preds += clf.predict(X_test_select)/folds.n_splits\n",
    "        \n",
    "        \n",
    "    return clf, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf, test_preds = train_apps_all_with_oof(ftr, target, nfolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([38.82502123, 42.54239728, 27.3671957 , ..., 38.33219587,\n",
       "       33.4536716 , 28.00149744])"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'submission_0615_1156.csv' is ready to submit.\n"
     ]
    }
   ],
   "source": [
    "t = pd.Timestamp.now()\n",
    "fname = f\"submission_{t.month:02}{t.day:02}_{t.hour:02}{t.minute:02}.csv\"\n",
    "submissions = pd.concat([pd.Series(IDtest, name=\"custid\"), pd.Series(test_preds, name=\"age\")] ,axis=1)\n",
    "submissions.to_csv(fname, index=False)\n",
    "print(\"'{}' is ready to submit.\" .format(fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  릿지 HP tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr = X_train_select\n",
    "target =  y['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15110, 1239), (6477, 1239))"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(ftr, target, test_size=0.3, random_state=0)\n",
    "train_x.shape, valid_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def train_apps_all_with_oof(ftr, target, nfolds=5):\n",
    "    ftr = ftr\n",
    "    target = target\n",
    "\n",
    "    # nfolds 개의 cross validatin fold set을 가지는 KFold 생성 \n",
    "    folds = KFold(n_splits=nfolds, shuffle=True, random_state=0)\n",
    "    \n",
    "    # Out of Folds로 학습된 모델의 validation set을 예측하여 결과 확률을 담을 array 생성.\n",
    "    # validation set가 n_split갯수만큼 있으므로 크기는 ftr_app의 크기가 되어야 함. \n",
    "    oof_preds = np.zeros((ftr.shape[0],))  \n",
    "    \n",
    "    # Ouf of Folds로 학습된 모델의 test dataset을 예측하여 결과 확률을 담을 array 생성. \n",
    "    test_preds = np.zeros(((X_test_select.shape[0],)))\n",
    "    \n",
    "    # n_estimators를 4000까지 확대. \n",
    "    clf = Ridge(alpha=37)\n",
    "\n",
    "    # nfolds 번 cross validation Iteration 반복하면서 OOF 방식으로 학습 및 테스트 데이터 예측\n",
    "    for fold_idx, (train_idx, valid_idx) in enumerate(folds.split(ftr)):\n",
    "        print('##### iteration ', fold_idx, ' 시작')\n",
    "        # 학습용 데이터 세트의 인덱스와 검증용 데이터 세트의 인덱스 추출하여 이를 기반으로 학습/검증 데이터 추출\n",
    "        train_x = ftr[train_idx, :]\n",
    "        train_y = target[train_idx]\n",
    "        valid_x = ftr[valid_idx, :]\n",
    "        valid_y = target[valid_idx]\n",
    "        \n",
    "        # 추출된 학습/검증 데이터 세트로 모델 학습. early_stopping은 200으로 증가. \n",
    "        clf.fit(train_x, train_y)\n",
    "        # 검증 데이터 세트로 예측된 확률 저장. 사용되지는 않음. \n",
    "        #oof_preds[valid_idx] = clf.predict(valid_x, num_iteration=clf.best_iteration_)       \n",
    "        # 학습된 모델로 테스트 데이터 세트에 예측 확률 계산. \n",
    "        # nfolds 번 반복 실행하므로 평균 확률을 구하기 위해 개별 수행시 마다 수행 횟수로 나눈 확률을 추후에 더해서 최종 평균 확률 계산. \n",
    "        test_preds += clf.predict(X_test_select)/folds.n_splits\n",
    "        print(np.sqrt(mean_squared_error(valid_y, clf.predict(valid_x))))\n",
    "        \n",
    "        \n",
    "    return clf, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf, test_preds = train_apps_all_with_oof(ftr, target, nfolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35.78818772, 42.74331594, 28.4787379 , ..., 34.54083888,\n",
       "       35.01098712, 28.26364696])"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'submission_0615_1232.csv' is ready to submit.\n"
     ]
    }
   ],
   "source": [
    "t = pd.Timestamp.now()\n",
    "fname = f\"submission_{t.month:02}{t.day:02}_{t.hour:02}{t.minute:02}.csv\"\n",
    "submissions = pd.concat([pd.Series(IDtest, name=\"custid\"), pd.Series(test_preds, name=\"age\")] ,axis=1)\n",
    "submissions.to_csv(fname, index=False)\n",
    "print(\"'{}' is ready to submit.\" .format(fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM 서치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "models=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr = X_train_select\n",
    "target =  y['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15110, 1239), (6477, 1239))"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(ftr, target, test_size=0.3, random_state=0)\n",
    "train_x.shape, valid_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.9829445617967165\n"
     ]
    }
   ],
   "source": [
    "# LGBM\n",
    "model =LGBMRegressor(\n",
    "                nthread=4,\n",
    "                n_estimators=4000,\n",
    "                learning_rate=0.01,\n",
    "                max_depth=14,\n",
    "                num_leaves=43,\n",
    "                colsample_bytree=0.6243940572960339,\n",
    "                subsample=0.9456256393571061,\n",
    "                max_bin=308,\n",
    "                reg_alpha=40.75532541032781,\n",
    "                reg_lambda=1.3974457528318784,\n",
    "                min_child_weight=10.700102581559948,\n",
    "                min_child_samples=140,\n",
    "                silent=-1,\n",
    "                verbose=-1,\n",
    "                )\n",
    "\n",
    "model.fit(train_x, train_y, verbose=False)\n",
    "print(np.sqrt(mean_squared_error(valid_y, model.predict(valid_x))))\n",
    "\n",
    "models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catboost 서치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr = X_train_select\n",
    "target =  y['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15110, 1239), (6477, 1239))"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(ftr, target, test_size=0.3, random_state=0)\n",
    "train_x.shape, valid_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.020228654289912\n"
     ]
    }
   ],
   "source": [
    "# CatBoost\n",
    "model = CatBoostRegressor(random_state=0,eval_metric='RMSE',loss_function='RMSE')\n",
    "model.fit(train_x, train_y, verbose=False)\n",
    "print(np.sqrt(mean_squared_error(valid_y, model.predict(valid_x))))\n",
    "\n",
    "models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Ridge 서치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr = X_train_select\n",
    "target =  y['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15110, 1239), (6477, 1239))"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(ftr, target, test_size=0.3, random_state=0)\n",
    "train_x.shape, valid_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.396088295738776\n"
     ]
    }
   ],
   "source": [
    "# Ridge\n",
    "model = Ridge(alpha=37)\n",
    "model.fit(train_x, train_y)\n",
    "print(np.sqrt(mean_squared_error(valid_y, model.predict(valid_x))))\n",
    "models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBM 서치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr = X_train_select\n",
    "target =  y['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15110, 1239), (6477, 1239))"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(ftr, target, test_size=0.3, random_state=0)\n",
    "train_x.shape, valid_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.981946335709951\n"
     ]
    }
   ],
   "source": [
    "# Ridge\n",
    "model =  XGBRegressor(subsample =  0.5716766437045232, \n",
    "                       silent = -1, reg_lambda = 6.3995702922539115, \n",
    "                       n_estimators = 1804, min_child_weight = 39.245929638036316, \n",
    "                       max_depth = 12, learning_rate = 0.01, gamma = 97.8618342232764, \n",
    "                       colsample_bytree = 0.9350060741234096, colsample_bylevel = 0.8890783754749252,\n",
    "                       max_delta_step= 40.158769646619454)\n",
    "model.fit(train_x, train_y)\n",
    "print(np.sqrt(mean_squared_error(valid_y, model.predict(valid_x))))\n",
    "models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LGBMRegressor(colsample_bytree=0.61357, learning_rate=0.01, max_bin=431,\n",
       "               max_depth=9, min_child_samples=165, min_child_weight=19.2731,\n",
       "               n_estimators=4000, nthread=4, num_leaves=53, reg_alpha=7.5296,\n",
       "               reg_lambda=7.167, silent=-1, subsample=0.8966, verbose=-1),\n",
       " <catboost.core.CatBoostRegressor at 0x1c513f0ce80>,\n",
       " BayesianRidge(alpha_1=0.5657, alpha_2=0.3657, lambda_1=0.6798, lambda_2=0.4051),\n",
       " XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=0.806,\n",
       "              colsample_bynode=1, colsample_bytree=0.8085, gamma=94.3748,\n",
       "              gpu_id=-1, importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.01, max_delta_step=34.4092, max_depth=11,\n",
       "              min_child_weight=22.41457, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=1800, n_jobs=16, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=0.6032, scale_pos_weight=1,\n",
       "              subsample=0.8334, tree_method='exact', validate_parameters=1,\n",
       "              verbosity=None)]"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Image, clear_output\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib import font_manager, rc\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import kerastuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr = X_train_select\n",
    "target =  y['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15110, 1239), (6477, 1239))"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(ftr, target, test_size=0.3, random_state=0)\n",
    "X_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(i, h_size, num_hidden, activation, lr):\n",
    "    model = tf.keras.Sequential()\n",
    "    for _ in range(num_hidden):\n",
    "        model.add(tf.keras.layers.Dense(h_size, activation=activation))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='linear'))\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss='mse', metrics=[keras.metrics.RootMeanSquaredError()])\n",
    "    callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=5),]\n",
    "    hist = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), \n",
    "                 batch_size=128, epochs=300, callbacks=callbacks, shuffle=False, verbose=2)\n",
    "    \n",
    "    return min(hist.history['val_root_mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119/119 - 1s - loss: 55847.0820 - root_mean_squared_error: 236.3199 - val_loss: 51387.3203 - val_root_mean_squared_error: 226.6877\n",
      "Epoch 2/300\n",
      "119/119 - 1s - loss: 46789.5312 - root_mean_squared_error: 216.3089 - val_loss: 42890.5078 - val_root_mean_squared_error: 207.1002\n",
      "Epoch 3/300\n",
      "119/119 - 0s - loss: 38906.3242 - root_mean_squared_error: 197.2469 - val_loss: 35511.8789 - val_root_mean_squared_error: 188.4460\n",
      "Epoch 4/300\n",
      "119/119 - 1s - loss: 32082.7031 - root_mean_squared_error: 179.1165 - val_loss: 29147.9668 - val_root_mean_squared_error: 170.7278\n",
      "Epoch 5/300\n",
      "119/119 - 0s - loss: 26219.5996 - root_mean_squared_error: 161.9247 - val_loss: 23702.5098 - val_root_mean_squared_error: 153.9562\n",
      "Epoch 6/300\n",
      "119/119 - 0s - loss: 21223.8965 - root_mean_squared_error: 145.6842 - val_loss: 19084.2559 - val_root_mean_squared_error: 138.1458\n",
      "Epoch 7/300\n",
      "119/119 - 0s - loss: 17006.9844 - root_mean_squared_error: 130.4108 - val_loss: 15206.1475 - val_root_mean_squared_error: 123.3132\n",
      "Epoch 8/300\n",
      "119/119 - 0s - loss: 13484.3740 - root_mean_squared_error: 116.1222 - val_loss: 11985.2324 - val_root_mean_squared_error: 109.4771\n",
      "Epoch 9/300\n",
      "119/119 - 0s - loss: 10575.6191 - root_mean_squared_error: 102.8378 - val_loss: 9342.5957 - val_root_mean_squared_error: 96.6571\n",
      "Epoch 10/300\n",
      "119/119 - 0s - loss: 8204.3555 - root_mean_squared_error: 90.5779 - val_loss: 7203.5361 - val_root_mean_squared_error: 84.8736\n",
      "Epoch 11/300\n",
      "119/119 - 1s - loss: 6298.4971 - root_mean_squared_error: 79.3631 - val_loss: 5497.7422 - val_root_mean_squared_error: 74.1468\n",
      "Epoch 12/300\n",
      "119/119 - 0s - loss: 4790.4937 - root_mean_squared_error: 69.2134 - val_loss: 4159.6763 - val_root_mean_squared_error: 64.4956\n",
      "Epoch 13/300\n",
      "119/119 - 1s - loss: 3617.6797 - root_mean_squared_error: 60.1472 - val_loss: 3128.8723 - val_root_mean_squared_error: 55.9363\n",
      "Epoch 14/300\n",
      "119/119 - 1s - loss: 2722.6313 - root_mean_squared_error: 52.1788 - val_loss: 2350.3420 - val_root_mean_squared_error: 48.4803\n",
      "Epoch 15/300\n",
      "119/119 - 1s - loss: 2053.5281 - root_mean_squared_error: 45.3159 - val_loss: 1774.8960 - val_root_mean_squared_error: 42.1295\n",
      "Epoch 16/300\n",
      "119/119 - 1s - loss: 1564.4456 - root_mean_squared_error: 39.5531 - val_loss: 1359.3878 - val_root_mean_squared_error: 36.8699\n",
      "Epoch 17/300\n",
      "119/119 - 1s - loss: 1215.5259 - root_mean_squared_error: 34.8644 - val_loss: 1066.8229 - val_root_mean_squared_error: 32.6623\n",
      "Epoch 18/300\n",
      "119/119 - 0s - loss: 973.0016 - root_mean_squared_error: 31.1930 - val_loss: 866.2756 - val_root_mean_squared_error: 29.4326\n",
      "Epoch 19/300\n",
      "119/119 - 0s - loss: 809.0247 - root_mean_squared_error: 28.4434 - val_loss: 732.6251 - val_root_mean_squared_error: 27.0670\n",
      "Epoch 20/300\n",
      "119/119 - 1s - loss: 701.3053 - root_mean_squared_error: 26.4822 - val_loss: 646.1018 - val_root_mean_squared_error: 25.4185\n",
      "Epoch 21/300\n",
      "119/119 - 0s - loss: 632.5794 - root_mean_squared_error: 25.1511 - val_loss: 591.6632 - val_root_mean_squared_error: 24.3241\n",
      "Epoch 22/300\n",
      "119/119 - 1s - loss: 589.9376 - root_mean_squared_error: 24.2886 - val_loss: 558.2827 - val_root_mean_squared_error: 23.6280\n",
      "Epoch 23/300\n",
      "119/119 - 0s - loss: 564.0902 - root_mean_squared_error: 23.7506 - val_loss: 538.1886 - val_root_mean_squared_error: 23.1989\n",
      "Epoch 24/300\n",
      "119/119 - 0s - loss: 548.6163 - root_mean_squared_error: 23.4226 - val_loss: 526.1266 - val_root_mean_squared_error: 22.9374\n",
      "Epoch 25/300\n",
      "119/119 - 1s - loss: 539.2639 - root_mean_squared_error: 23.2221 - val_loss: 518.6973 - val_root_mean_squared_error: 22.7749\n",
      "Epoch 26/300\n",
      "119/119 - 0s - loss: 533.3430 - root_mean_squared_error: 23.0942 - val_loss: 513.8013 - val_root_mean_squared_error: 22.6672\n",
      "Epoch 27/300\n",
      "119/119 - 0s - loss: 529.2309 - root_mean_squared_error: 23.0050 - val_loss: 510.2029 - val_root_mean_squared_error: 22.5877\n",
      "Epoch 28/300\n",
      "119/119 - 0s - loss: 525.9968 - root_mean_squared_error: 22.9346 - val_loss: 507.2105 - val_root_mean_squared_error: 22.5213\n",
      "Epoch 29/300\n",
      "119/119 - 0s - loss: 523.1344 - root_mean_squared_error: 22.8721 - val_loss: 504.4569 - val_root_mean_squared_error: 22.4601\n",
      "Epoch 30/300\n",
      "119/119 - 0s - loss: 520.3836 - root_mean_squared_error: 22.8119 - val_loss: 501.7567 - val_root_mean_squared_error: 22.3999\n",
      "Epoch 31/300\n",
      "119/119 - 0s - loss: 517.6182 - root_mean_squared_error: 22.7512 - val_loss: 499.0216 - val_root_mean_squared_error: 22.3388\n",
      "Epoch 32/300\n",
      "119/119 - 0s - loss: 514.7804 - root_mean_squared_error: 22.6888 - val_loss: 496.2109 - val_root_mean_squared_error: 22.2758\n",
      "Epoch 33/300\n",
      "119/119 - 0s - loss: 511.8463 - root_mean_squared_error: 22.6240 - val_loss: 493.3069 - val_root_mean_squared_error: 22.2105\n",
      "Epoch 34/300\n",
      "119/119 - 0s - loss: 508.8065 - root_mean_squared_error: 22.5567 - val_loss: 490.3025 - val_root_mean_squared_error: 22.1428\n",
      "Epoch 35/300\n",
      "119/119 - 0s - loss: 505.6577 - root_mean_squared_error: 22.4868 - val_loss: 487.1945 - val_root_mean_squared_error: 22.0725\n",
      "Epoch 36/300\n",
      "119/119 - 0s - loss: 502.3989 - root_mean_squared_error: 22.4143 - val_loss: 483.9818 - val_root_mean_squared_error: 21.9996\n",
      "Epoch 37/300\n",
      "119/119 - 0s - loss: 499.0307 - root_mean_squared_error: 22.3390 - val_loss: 480.6647 - val_root_mean_squared_error: 21.9241\n",
      "Epoch 38/300\n",
      "119/119 - 0s - loss: 495.5540 - root_mean_squared_error: 22.2610 - val_loss: 477.2438 - val_root_mean_squared_error: 21.8459\n",
      "Epoch 39/300\n",
      "119/119 - 0s - loss: 491.9703 - root_mean_squared_error: 22.1804 - val_loss: 473.7208 - val_root_mean_squared_error: 21.7651\n",
      "Epoch 40/300\n",
      "119/119 - 0s - loss: 488.2814 - root_mean_squared_error: 22.0971 - val_loss: 470.0971 - val_root_mean_squared_error: 21.6817\n",
      "Epoch 41/300\n",
      "119/119 - 0s - loss: 484.4892 - root_mean_squared_error: 22.0111 - val_loss: 466.3754 - val_root_mean_squared_error: 21.5957\n",
      "Epoch 42/300\n",
      "119/119 - 0s - loss: 480.5967 - root_mean_squared_error: 21.9225 - val_loss: 462.5580 - val_root_mean_squared_error: 21.5072\n",
      "Epoch 43/300\n",
      "119/119 - 0s - loss: 476.6068 - root_mean_squared_error: 21.8313 - val_loss: 458.6484 - val_root_mean_squared_error: 21.4161\n",
      "Epoch 44/300\n",
      "119/119 - 0s - loss: 472.5231 - root_mean_squared_error: 21.7376 - val_loss: 454.6501 - val_root_mean_squared_error: 21.3225\n",
      "Epoch 45/300\n",
      "119/119 - 1s - loss: 468.3493 - root_mean_squared_error: 21.6414 - val_loss: 450.5671 - val_root_mean_squared_error: 21.2266\n",
      "Epoch 46/300\n",
      "119/119 - 0s - loss: 464.0903 - root_mean_squared_error: 21.5428 - val_loss: 446.4037 - val_root_mean_squared_error: 21.1283\n",
      "Epoch 47/300\n",
      "119/119 - 0s - loss: 459.7501 - root_mean_squared_error: 21.4418 - val_loss: 442.1647 - val_root_mean_squared_error: 21.0277\n",
      "Epoch 48/300\n",
      "119/119 - 0s - loss: 455.3340 - root_mean_squared_error: 21.3386 - val_loss: 437.8553 - val_root_mean_squared_error: 20.9250\n",
      "Epoch 49/300\n",
      "119/119 - 0s - loss: 450.8476 - root_mean_squared_error: 21.2332 - val_loss: 433.4805 - val_root_mean_squared_error: 20.8202\n",
      "Epoch 50/300\n",
      "119/119 - 0s - loss: 446.2963 - root_mean_squared_error: 21.1257 - val_loss: 429.0458 - val_root_mean_squared_error: 20.7134\n",
      "Epoch 51/300\n",
      "119/119 - 0s - loss: 441.6856 - root_mean_squared_error: 21.0163 - val_loss: 424.5569 - val_root_mean_squared_error: 20.6048\n",
      "Epoch 52/300\n",
      "119/119 - 0s - loss: 437.0218 - root_mean_squared_error: 20.9051 - val_loss: 420.0193 - val_root_mean_squared_error: 20.4944\n",
      "Epoch 53/300\n",
      "119/119 - 0s - loss: 432.3104 - root_mean_squared_error: 20.7921 - val_loss: 415.4391 - val_root_mean_squared_error: 20.3823\n",
      "Epoch 54/300\n",
      "119/119 - 0s - loss: 427.5576 - root_mean_squared_error: 20.6775 - val_loss: 410.8218 - val_root_mean_squared_error: 20.2687\n",
      "Epoch 55/300\n",
      "119/119 - 0s - loss: 422.7691 - root_mean_squared_error: 20.5613 - val_loss: 406.1730 - val_root_mean_squared_error: 20.1537\n",
      "Epoch 56/300\n",
      "119/119 - 0s - loss: 417.9508 - root_mean_squared_error: 20.4438 - val_loss: 401.4984 - val_root_mean_squared_error: 20.0374\n",
      "Epoch 57/300\n",
      "119/119 - 0s - loss: 413.1085 - root_mean_squared_error: 20.3251 - val_loss: 396.8032 - val_root_mean_squared_error: 19.9199\n",
      "Epoch 58/300\n",
      "119/119 - 0s - loss: 408.2473 - root_mean_squared_error: 20.2051 - val_loss: 392.0925 - val_root_mean_squared_error: 19.8013\n",
      "Epoch 59/300\n",
      "119/119 - 0s - loss: 403.3730 - root_mean_squared_error: 20.0841 - val_loss: 387.3717 - val_root_mean_squared_error: 19.6818\n",
      "Epoch 60/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119/119 - 0s - loss: 398.4903 - root_mean_squared_error: 19.9622 - val_loss: 382.6455 - val_root_mean_squared_error: 19.5613\n",
      "Epoch 61/300\n",
      "119/119 - 0s - loss: 393.6037 - root_mean_squared_error: 19.8394 - val_loss: 377.9179 - val_root_mean_squared_error: 19.4401\n",
      "Epoch 62/300\n",
      "119/119 - 0s - loss: 388.7182 - root_mean_squared_error: 19.7159 - val_loss: 373.1937 - val_root_mean_squared_error: 19.3182\n",
      "Epoch 63/300\n",
      "119/119 - 0s - loss: 383.8378 - root_mean_squared_error: 19.5918 - val_loss: 368.4764 - val_root_mean_squared_error: 19.1957\n",
      "Epoch 64/300\n",
      "119/119 - 0s - loss: 378.9663 - root_mean_squared_error: 19.4671 - val_loss: 363.7700 - val_root_mean_squared_error: 19.0728\n",
      "Epoch 65/300\n",
      "119/119 - 0s - loss: 374.1079 - root_mean_squared_error: 19.3419 - val_loss: 359.0778 - val_root_mean_squared_error: 18.9493\n",
      "Epoch 66/300\n",
      "119/119 - 0s - loss: 369.2654 - root_mean_squared_error: 19.2163 - val_loss: 354.4030 - val_root_mean_squared_error: 18.8256\n",
      "Epoch 67/300\n",
      "119/119 - 0s - loss: 364.4420 - root_mean_squared_error: 19.0904 - val_loss: 349.7485 - val_root_mean_squared_error: 18.7016\n",
      "Epoch 68/300\n",
      "119/119 - 0s - loss: 359.6408 - root_mean_squared_error: 18.9642 - val_loss: 345.1169 - val_root_mean_squared_error: 18.5773\n",
      "Epoch 69/300\n",
      "119/119 - 0s - loss: 354.8642 - root_mean_squared_error: 18.8378 - val_loss: 340.5109 - val_root_mean_squared_error: 18.4529\n",
      "Epoch 70/300\n",
      "119/119 - 0s - loss: 350.1146 - root_mean_squared_error: 18.7113 - val_loss: 335.9326 - val_root_mean_squared_error: 18.3285\n",
      "Epoch 71/300\n",
      "119/119 - 0s - loss: 345.3944 - root_mean_squared_error: 18.5848 - val_loss: 331.3840 - val_root_mean_squared_error: 18.2040\n",
      "Epoch 72/300\n",
      "119/119 - 0s - loss: 340.7054 - root_mean_squared_error: 18.4582 - val_loss: 326.8670 - val_root_mean_squared_error: 18.0795\n",
      "Epoch 73/300\n",
      "119/119 - 0s - loss: 336.0494 - root_mean_squared_error: 18.3316 - val_loss: 322.3833 - val_root_mean_squared_error: 17.9550\n",
      "Epoch 74/300\n",
      "119/119 - 0s - loss: 331.4280 - root_mean_squared_error: 18.2052 - val_loss: 317.9344 - val_root_mean_squared_error: 17.8307\n",
      "Epoch 75/300\n",
      "119/119 - 0s - loss: 326.8428 - root_mean_squared_error: 18.0788 - val_loss: 313.5217 - val_root_mean_squared_error: 17.7065\n",
      "Epoch 76/300\n",
      "119/119 - 0s - loss: 322.2949 - root_mean_squared_error: 17.9526 - val_loss: 309.1463 - val_root_mean_squared_error: 17.5826\n",
      "Epoch 77/300\n",
      "119/119 - 0s - loss: 317.7856 - root_mean_squared_error: 17.8265 - val_loss: 304.8093 - val_root_mean_squared_error: 17.4588\n",
      "Epoch 78/300\n",
      "119/119 - 0s - loss: 313.3160 - root_mean_squared_error: 17.7007 - val_loss: 300.5117 - val_root_mean_squared_error: 17.3353\n",
      "Epoch 79/300\n",
      "119/119 - 0s - loss: 308.8871 - root_mean_squared_error: 17.5752 - val_loss: 296.2545 - val_root_mean_squared_error: 17.2120\n",
      "Epoch 80/300\n",
      "119/119 - 0s - loss: 304.4997 - root_mean_squared_error: 17.4499 - val_loss: 292.0382 - val_root_mean_squared_error: 17.0891\n",
      "Epoch 81/300\n",
      "119/119 - 0s - loss: 300.1545 - root_mean_squared_error: 17.3250 - val_loss: 287.8638 - val_root_mean_squared_error: 16.9665\n",
      "Epoch 82/300\n",
      "119/119 - 0s - loss: 295.8524 - root_mean_squared_error: 17.2004 - val_loss: 283.7318 - val_root_mean_squared_error: 16.8443\n",
      "Epoch 83/300\n",
      "119/119 - 0s - loss: 291.5937 - root_mean_squared_error: 17.0761 - val_loss: 279.6425 - val_root_mean_squared_error: 16.7225\n",
      "Epoch 84/300\n",
      "119/119 - 0s - loss: 287.3792 - root_mean_squared_error: 16.9523 - val_loss: 275.5968 - val_root_mean_squared_error: 16.6011\n",
      "Epoch 85/300\n",
      "119/119 - 0s - loss: 283.2092 - root_mean_squared_error: 16.8288 - val_loss: 271.5949 - val_root_mean_squared_error: 16.4801\n",
      "Epoch 86/300\n",
      "119/119 - 0s - loss: 279.0843 - root_mean_squared_error: 16.7058 - val_loss: 267.6372 - val_root_mean_squared_error: 16.3596\n",
      "Epoch 87/300\n",
      "119/119 - 0s - loss: 275.0048 - root_mean_squared_error: 16.5833 - val_loss: 263.7242 - val_root_mean_squared_error: 16.2396\n",
      "Epoch 88/300\n",
      "119/119 - 0s - loss: 270.9710 - root_mean_squared_error: 16.4612 - val_loss: 259.8561 - val_root_mean_squared_error: 16.1201\n",
      "Epoch 89/300\n",
      "119/119 - 0s - loss: 266.9833 - root_mean_squared_error: 16.3396 - val_loss: 256.0332 - val_root_mean_squared_error: 16.0010\n",
      "Epoch 90/300\n",
      "119/119 - 0s - loss: 263.0417 - root_mean_squared_error: 16.2186 - val_loss: 252.2558 - val_root_mean_squared_error: 15.8826\n",
      "Epoch 91/300\n",
      "119/119 - 1s - loss: 259.1469 - root_mean_squared_error: 16.0980 - val_loss: 248.5241 - val_root_mean_squared_error: 15.7646\n",
      "Epoch 92/300\n",
      "119/119 - 0s - loss: 255.2985 - root_mean_squared_error: 15.9781 - val_loss: 244.8383 - val_root_mean_squared_error: 15.6473\n",
      "Epoch 93/300\n",
      "119/119 - 0s - loss: 251.4972 - root_mean_squared_error: 15.8587 - val_loss: 241.1988 - val_root_mean_squared_error: 15.5306\n",
      "Epoch 94/300\n",
      "119/119 - 0s - loss: 247.7428 - root_mean_squared_error: 15.7398 - val_loss: 237.6052 - val_root_mean_squared_error: 15.4144\n",
      "Epoch 95/300\n",
      "119/119 - 0s - loss: 244.0356 - root_mean_squared_error: 15.6216 - val_loss: 234.0581 - val_root_mean_squared_error: 15.2990\n",
      "Epoch 96/300\n",
      "119/119 - 0s - loss: 240.3757 - root_mean_squared_error: 15.5041 - val_loss: 230.5575 - val_root_mean_squared_error: 15.1841\n",
      "Epoch 97/300\n",
      "119/119 - 0s - loss: 236.7631 - root_mean_squared_error: 15.3871 - val_loss: 227.1035 - val_root_mean_squared_error: 15.0700\n",
      "Epoch 98/300\n",
      "119/119 - 0s - loss: 233.1979 - root_mean_squared_error: 15.2708 - val_loss: 223.6962 - val_root_mean_squared_error: 14.9565\n",
      "Epoch 99/300\n",
      "119/119 - 0s - loss: 229.6802 - root_mean_squared_error: 15.1552 - val_loss: 220.3356 - val_root_mean_squared_error: 14.8437\n",
      "Epoch 100/300\n",
      "119/119 - 0s - loss: 226.2101 - root_mean_squared_error: 15.0403 - val_loss: 217.0218 - val_root_mean_squared_error: 14.7317\n",
      "Epoch 101/300\n",
      "119/119 - 0s - loss: 222.7875 - root_mean_squared_error: 14.9261 - val_loss: 213.7547 - val_root_mean_squared_error: 14.6204\n",
      "Epoch 102/300\n",
      "119/119 - 0s - loss: 219.4126 - root_mean_squared_error: 14.8126 - val_loss: 210.5345 - val_root_mean_squared_error: 14.5098\n",
      "Epoch 103/300\n",
      "119/119 - 0s - loss: 216.0851 - root_mean_squared_error: 14.6998 - val_loss: 207.3613 - val_root_mean_squared_error: 14.4000\n",
      "Epoch 104/300\n",
      "119/119 - 0s - loss: 212.8054 - root_mean_squared_error: 14.5878 - val_loss: 204.2348 - val_root_mean_squared_error: 14.2911\n",
      "Epoch 105/300\n",
      "119/119 - 0s - loss: 209.5732 - root_mean_squared_error: 14.4766 - val_loss: 201.1552 - val_root_mean_squared_error: 14.1829\n",
      "Epoch 106/300\n",
      "119/119 - 0s - loss: 206.3885 - root_mean_squared_error: 14.3662 - val_loss: 198.1224 - val_root_mean_squared_error: 14.0756\n",
      "Epoch 107/300\n",
      "119/119 - 0s - loss: 203.2515 - root_mean_squared_error: 14.2566 - val_loss: 195.1364 - val_root_mean_squared_error: 13.9691\n",
      "Epoch 108/300\n",
      "119/119 - 0s - loss: 200.1621 - root_mean_squared_error: 14.1479 - val_loss: 192.1971 - val_root_mean_squared_error: 13.8635\n",
      "Epoch 109/300\n",
      "119/119 - 0s - loss: 197.1200 - root_mean_squared_error: 14.0399 - val_loss: 189.3046 - val_root_mean_squared_error: 13.7588\n",
      "Epoch 110/300\n",
      "119/119 - 0s - loss: 194.1254 - root_mean_squared_error: 13.9329 - val_loss: 186.4587 - val_root_mean_squared_error: 13.6550\n",
      "Epoch 111/300\n",
      "119/119 - 0s - loss: 191.1782 - root_mean_squared_error: 13.8267 - val_loss: 183.6594 - val_root_mean_squared_error: 13.5521\n",
      "Epoch 112/300\n",
      "119/119 - 0s - loss: 188.2784 - root_mean_squared_error: 13.7215 - val_loss: 180.9067 - val_root_mean_squared_error: 13.4502\n",
      "Epoch 113/300\n",
      "119/119 - 0s - loss: 185.4258 - root_mean_squared_error: 13.6171 - val_loss: 178.2003 - val_root_mean_squared_error: 13.3492\n",
      "Epoch 114/300\n",
      "119/119 - 0s - loss: 182.6203 - root_mean_squared_error: 13.5137 - val_loss: 175.5404 - val_root_mean_squared_error: 13.2492\n",
      "Epoch 115/300\n",
      "119/119 - 0s - loss: 179.8619 - root_mean_squared_error: 13.4113 - val_loss: 172.9267 - val_root_mean_squared_error: 13.1502\n",
      "Epoch 116/300\n",
      "119/119 - 0s - loss: 177.1506 - root_mean_squared_error: 13.3098 - val_loss: 170.3591 - val_root_mean_squared_error: 13.0522\n",
      "Epoch 117/300\n",
      "119/119 - 0s - loss: 174.4861 - root_mean_squared_error: 13.2093 - val_loss: 167.8377 - val_root_mean_squared_error: 12.9552\n",
      "Epoch 118/300\n",
      "119/119 - 0s - loss: 171.8685 - root_mean_squared_error: 13.1099 - val_loss: 165.3620 - val_root_mean_squared_error: 12.8593\n",
      "Epoch 119/300\n",
      "119/119 - 0s - loss: 169.2975 - root_mean_squared_error: 13.0114 - val_loss: 162.9324 - val_root_mean_squared_error: 12.7645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/300\n",
      "119/119 - 0s - loss: 166.7730 - root_mean_squared_error: 12.9141 - val_loss: 160.5484 - val_root_mean_squared_error: 12.6708\n",
      "Epoch 121/300\n",
      "119/119 - 0s - loss: 164.2950 - root_mean_squared_error: 12.8178 - val_loss: 158.2099 - val_root_mean_squared_error: 12.5782\n",
      "Epoch 122/300\n",
      "119/119 - 0s - loss: 161.8632 - root_mean_squared_error: 12.7225 - val_loss: 155.9168 - val_root_mean_squared_error: 12.4867\n",
      "Epoch 123/300\n",
      "119/119 - 0s - loss: 159.4776 - root_mean_squared_error: 12.6284 - val_loss: 153.6690 - val_root_mean_squared_error: 12.3963\n",
      "Epoch 124/300\n",
      "119/119 - 0s - loss: 157.1380 - root_mean_squared_error: 12.5355 - val_loss: 151.4662 - val_root_mean_squared_error: 12.3072\n",
      "Epoch 125/300\n",
      "119/119 - 0s - loss: 154.8443 - root_mean_squared_error: 12.4436 - val_loss: 149.3084 - val_root_mean_squared_error: 12.2192\n",
      "Epoch 126/300\n",
      "119/119 - 0s - loss: 152.5961 - root_mean_squared_error: 12.3530 - val_loss: 147.1954 - val_root_mean_squared_error: 12.1324\n",
      "Epoch 127/300\n",
      "119/119 - 0s - loss: 150.3934 - root_mean_squared_error: 12.2635 - val_loss: 145.1268 - val_root_mean_squared_error: 12.0469\n",
      "Epoch 128/300\n",
      "119/119 - 0s - loss: 148.2361 - root_mean_squared_error: 12.1752 - val_loss: 143.1026 - val_root_mean_squared_error: 11.9626\n",
      "Epoch 129/300\n",
      "119/119 - 0s - loss: 146.1238 - root_mean_squared_error: 12.0882 - val_loss: 141.1225 - val_root_mean_squared_error: 11.8795\n",
      "Epoch 130/300\n",
      "119/119 - 0s - loss: 144.0565 - root_mean_squared_error: 12.0024 - val_loss: 139.1864 - val_root_mean_squared_error: 11.7977\n",
      "Epoch 131/300\n",
      "119/119 - 0s - loss: 142.0337 - root_mean_squared_error: 11.9178 - val_loss: 137.2940 - val_root_mean_squared_error: 11.7173\n",
      "Epoch 132/300\n",
      "119/119 - 0s - loss: 140.0555 - root_mean_squared_error: 11.8345 - val_loss: 135.4451 - val_root_mean_squared_error: 11.6381\n",
      "Epoch 133/300\n",
      "119/119 - 0s - loss: 138.1215 - root_mean_squared_error: 11.7525 - val_loss: 133.6393 - val_root_mean_squared_error: 11.5602\n",
      "Epoch 134/300\n",
      "119/119 - 0s - loss: 136.2314 - root_mean_squared_error: 11.6718 - val_loss: 131.8765 - val_root_mean_squared_error: 11.4837\n",
      "Epoch 135/300\n",
      "119/119 - 0s - loss: 134.3852 - root_mean_squared_error: 11.5925 - val_loss: 130.1563 - val_root_mean_squared_error: 11.4086\n",
      "Epoch 136/300\n",
      "119/119 - 0s - loss: 132.5823 - root_mean_squared_error: 11.5144 - val_loss: 128.4786 - val_root_mean_squared_error: 11.3348\n",
      "Epoch 137/300\n",
      "119/119 - 0s - loss: 130.8227 - root_mean_squared_error: 11.4378 - val_loss: 126.8430 - val_root_mean_squared_error: 11.2625\n",
      "Epoch 138/300\n",
      "119/119 - 0s - loss: 129.1059 - root_mean_squared_error: 11.3625 - val_loss: 125.2492 - val_root_mean_squared_error: 11.1915\n",
      "Epoch 139/300\n",
      "119/119 - 0s - loss: 127.4318 - root_mean_squared_error: 11.2886 - val_loss: 123.6969 - val_root_mean_squared_error: 11.1219\n",
      "Epoch 140/300\n",
      "119/119 - 0s - loss: 125.7999 - root_mean_squared_error: 11.2161 - val_loss: 122.1856 - val_root_mean_squared_error: 11.0538\n",
      "Epoch 141/300\n",
      "119/119 - 0s - loss: 124.2100 - root_mean_squared_error: 11.1450 - val_loss: 120.7152 - val_root_mean_squared_error: 10.9870\n",
      "Epoch 142/300\n",
      "119/119 - 0s - loss: 122.6616 - root_mean_squared_error: 11.0753 - val_loss: 119.2852 - val_root_mean_squared_error: 10.9218\n",
      "Epoch 143/300\n",
      "119/119 - 0s - loss: 121.1546 - root_mean_squared_error: 11.0070 - val_loss: 117.8953 - val_root_mean_squared_error: 10.8580\n",
      "Epoch 144/300\n",
      "119/119 - 0s - loss: 119.6884 - root_mean_squared_error: 10.9402 - val_loss: 116.5450 - val_root_mean_squared_error: 10.7956\n",
      "Epoch 145/300\n",
      "119/119 - 1s - loss: 118.2627 - root_mean_squared_error: 10.8749 - val_loss: 115.2340 - val_root_mean_squared_error: 10.7347\n",
      "Epoch 146/300\n",
      "119/119 - 1s - loss: 116.8772 - root_mean_squared_error: 10.8110 - val_loss: 113.9618 - val_root_mean_squared_error: 10.6753\n",
      "Epoch 147/300\n",
      "119/119 - 1s - loss: 115.5313 - root_mean_squared_error: 10.7485 - val_loss: 112.7281 - val_root_mean_squared_error: 10.6173\n",
      "Epoch 148/300\n",
      "119/119 - 0s - loss: 114.2247 - root_mean_squared_error: 10.6876 - val_loss: 111.5322 - val_root_mean_squared_error: 10.5609\n",
      "Epoch 149/300\n",
      "119/119 - 0s - loss: 112.9569 - root_mean_squared_error: 10.6281 - val_loss: 110.3738 - val_root_mean_squared_error: 10.5059\n",
      "Epoch 150/300\n",
      "119/119 - 0s - loss: 111.7275 - root_mean_squared_error: 10.5701 - val_loss: 109.2524 - val_root_mean_squared_error: 10.4524\n",
      "Epoch 151/300\n",
      "119/119 - 0s - loss: 110.5359 - root_mean_squared_error: 10.5136 - val_loss: 108.1674 - val_root_mean_squared_error: 10.4004\n",
      "Epoch 152/300\n",
      "119/119 - 0s - loss: 109.3817 - root_mean_squared_error: 10.4586 - val_loss: 107.1184 - val_root_mean_squared_error: 10.3498\n",
      "Epoch 153/300\n",
      "119/119 - 0s - loss: 108.2643 - root_mean_squared_error: 10.4050 - val_loss: 106.1047 - val_root_mean_squared_error: 10.3007\n",
      "Epoch 154/300\n",
      "119/119 - 0s - loss: 107.1833 - root_mean_squared_error: 10.3529 - val_loss: 105.1258 - val_root_mean_squared_error: 10.2531\n",
      "Epoch 155/300\n",
      "119/119 - 0s - loss: 106.1380 - root_mean_squared_error: 10.3023 - val_loss: 104.1812 - val_root_mean_squared_error: 10.2069\n",
      "Epoch 156/300\n",
      "119/119 - 0s - loss: 105.1279 - root_mean_squared_error: 10.2532 - val_loss: 103.2702 - val_root_mean_squared_error: 10.1622\n",
      "Epoch 157/300\n",
      "119/119 - 1s - loss: 104.1523 - root_mean_squared_error: 10.2055 - val_loss: 102.3922 - val_root_mean_squared_error: 10.1189\n",
      "Epoch 158/300\n",
      "119/119 - 0s - loss: 103.2108 - root_mean_squared_error: 10.1593 - val_loss: 101.5465 - val_root_mean_squared_error: 10.0770\n",
      "Epoch 159/300\n",
      "119/119 - 0s - loss: 102.3026 - root_mean_squared_error: 10.1145 - val_loss: 100.7325 - val_root_mean_squared_error: 10.0366\n",
      "Epoch 160/300\n",
      "119/119 - 0s - loss: 101.4271 - root_mean_squared_error: 10.0711 - val_loss: 99.9495 - val_root_mean_squared_error: 9.9975\n",
      "Epoch 161/300\n",
      "119/119 - 0s - loss: 100.5836 - root_mean_squared_error: 10.0291 - val_loss: 99.1969 - val_root_mean_squared_error: 9.9598\n",
      "Epoch 162/300\n",
      "119/119 - 0s - loss: 99.7715 - root_mean_squared_error: 9.9886 - val_loss: 98.4738 - val_root_mean_squared_error: 9.9234\n",
      "Epoch 163/300\n",
      "119/119 - 0s - loss: 98.9900 - root_mean_squared_error: 9.9494 - val_loss: 97.7796 - val_root_mean_squared_error: 9.8884\n",
      "Epoch 164/300\n",
      "119/119 - 1s - loss: 98.2384 - root_mean_squared_error: 9.9115 - val_loss: 97.1135 - val_root_mean_squared_error: 9.8546\n",
      "Epoch 165/300\n",
      "119/119 - 0s - loss: 97.5160 - root_mean_squared_error: 9.8750 - val_loss: 96.4748 - val_root_mean_squared_error: 9.8222\n",
      "Epoch 166/300\n",
      "119/119 - 1s - loss: 96.8220 - root_mean_squared_error: 9.8398 - val_loss: 95.8626 - val_root_mean_squared_error: 9.7909\n",
      "Epoch 167/300\n",
      "119/119 - 1s - loss: 96.1557 - root_mean_squared_error: 9.8059 - val_loss: 95.2763 - val_root_mean_squared_error: 9.7610\n",
      "Epoch 168/300\n",
      "119/119 - 0s - loss: 95.5163 - root_mean_squared_error: 9.7732 - val_loss: 94.7149 - val_root_mean_squared_error: 9.7322\n",
      "Epoch 169/300\n",
      "119/119 - 1s - loss: 94.9031 - root_mean_squared_error: 9.7418 - val_loss: 94.1778 - val_root_mean_squared_error: 9.7045\n",
      "Epoch 170/300\n",
      "119/119 - 0s - loss: 94.3151 - root_mean_squared_error: 9.7116 - val_loss: 93.6640 - val_root_mean_squared_error: 9.6780\n",
      "Epoch 171/300\n",
      "119/119 - 0s - loss: 93.7516 - root_mean_squared_error: 9.6825 - val_loss: 93.1728 - val_root_mean_squared_error: 9.6526\n",
      "Epoch 172/300\n",
      "119/119 - 0s - loss: 93.2119 - root_mean_squared_error: 9.6546 - val_loss: 92.7033 - val_root_mean_squared_error: 9.6283\n",
      "Epoch 173/300\n",
      "119/119 - 1s - loss: 92.6950 - root_mean_squared_error: 9.6278 - val_loss: 92.2548 - val_root_mean_squared_error: 9.6049\n",
      "Epoch 174/300\n",
      "119/119 - 0s - loss: 92.2002 - root_mean_squared_error: 9.6021 - val_loss: 91.8264 - val_root_mean_squared_error: 9.5826\n",
      "Epoch 175/300\n",
      "119/119 - 0s - loss: 91.7268 - root_mean_squared_error: 9.5774 - val_loss: 91.4173 - val_root_mean_squared_error: 9.5612\n",
      "Epoch 176/300\n",
      "119/119 - 0s - loss: 91.2737 - root_mean_squared_error: 9.5537 - val_loss: 91.0267 - val_root_mean_squared_error: 9.5408\n",
      "Epoch 177/300\n",
      "119/119 - 0s - loss: 90.8404 - root_mean_squared_error: 9.5310 - val_loss: 90.6539 - val_root_mean_squared_error: 9.5212\n",
      "Epoch 178/300\n",
      "119/119 - 0s - loss: 90.4259 - root_mean_squared_error: 9.5093 - val_loss: 90.2979 - val_root_mean_squared_error: 9.5025\n",
      "Epoch 179/300\n",
      "119/119 - 0s - loss: 90.0295 - root_mean_squared_error: 9.4884 - val_loss: 89.9581 - val_root_mean_squared_error: 9.4846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180/300\n",
      "119/119 - 0s - loss: 89.6505 - root_mean_squared_error: 9.4684 - val_loss: 89.6337 - val_root_mean_squared_error: 9.4675\n",
      "Epoch 181/300\n",
      "119/119 - 0s - loss: 89.2879 - root_mean_squared_error: 9.4492 - val_loss: 89.3239 - val_root_mean_squared_error: 9.4511\n",
      "Epoch 182/300\n",
      "119/119 - 0s - loss: 88.9412 - root_mean_squared_error: 9.4309 - val_loss: 89.0281 - val_root_mean_squared_error: 9.4355\n",
      "Epoch 183/300\n",
      "119/119 - 1s - loss: 88.6096 - root_mean_squared_error: 9.4133 - val_loss: 88.7455 - val_root_mean_squared_error: 9.4205\n",
      "Epoch 184/300\n",
      "119/119 - 0s - loss: 88.2923 - root_mean_squared_error: 9.3964 - val_loss: 88.4754 - val_root_mean_squared_error: 9.4061\n",
      "Epoch 185/300\n",
      "119/119 - 0s - loss: 87.9887 - root_mean_squared_error: 9.3802 - val_loss: 88.2171 - val_root_mean_squared_error: 9.3924\n",
      "Epoch 186/300\n",
      "119/119 - 0s - loss: 87.6981 - root_mean_squared_error: 9.3647 - val_loss: 87.9701 - val_root_mean_squared_error: 9.3792\n",
      "Epoch 187/300\n",
      "119/119 - 1s - loss: 87.4198 - root_mean_squared_error: 9.3499 - val_loss: 87.7337 - val_root_mean_squared_error: 9.3666\n",
      "Epoch 188/300\n",
      "119/119 - 1s - loss: 87.1532 - root_mean_squared_error: 9.3356 - val_loss: 87.5073 - val_root_mean_squared_error: 9.3545\n",
      "Epoch 189/300\n",
      "119/119 - 1s - loss: 86.8977 - root_mean_squared_error: 9.3219 - val_loss: 87.2903 - val_root_mean_squared_error: 9.3429\n",
      "Epoch 190/300\n",
      "119/119 - 1s - loss: 86.6527 - root_mean_squared_error: 9.3087 - val_loss: 87.0822 - val_root_mean_squared_error: 9.3318\n",
      "Epoch 191/300\n",
      "119/119 - 0s - loss: 86.4176 - root_mean_squared_error: 9.2961 - val_loss: 86.8825 - val_root_mean_squared_error: 9.3211\n",
      "Epoch 192/300\n",
      "119/119 - 0s - loss: 86.1920 - root_mean_squared_error: 9.2840 - val_loss: 86.6906 - val_root_mean_squared_error: 9.3108\n",
      "Epoch 193/300\n",
      "119/119 - 0s - loss: 85.9752 - root_mean_squared_error: 9.2723 - val_loss: 86.5061 - val_root_mean_squared_error: 9.3009\n",
      "Epoch 194/300\n",
      "119/119 - 0s - loss: 85.7668 - root_mean_squared_error: 9.2610 - val_loss: 86.3286 - val_root_mean_squared_error: 9.2913\n",
      "Epoch 195/300\n",
      "119/119 - 0s - loss: 85.5663 - root_mean_squared_error: 9.2502 - val_loss: 86.1576 - val_root_mean_squared_error: 9.2821\n",
      "Epoch 196/300\n",
      "119/119 - 0s - loss: 85.3733 - root_mean_squared_error: 9.2398 - val_loss: 85.9926 - val_root_mean_squared_error: 9.2732\n",
      "Epoch 197/300\n",
      "119/119 - 1s - loss: 85.1873 - root_mean_squared_error: 9.2297 - val_loss: 85.8335 - val_root_mean_squared_error: 9.2646\n",
      "Epoch 198/300\n",
      "119/119 - 0s - loss: 85.0079 - root_mean_squared_error: 9.2200 - val_loss: 85.6796 - val_root_mean_squared_error: 9.2563\n",
      "Epoch 199/300\n",
      "119/119 - 0s - loss: 84.8348 - root_mean_squared_error: 9.2106 - val_loss: 85.5309 - val_root_mean_squared_error: 9.2483\n",
      "Epoch 200/300\n",
      "119/119 - 0s - loss: 84.6675 - root_mean_squared_error: 9.2015 - val_loss: 85.3868 - val_root_mean_squared_error: 9.2405\n",
      "Epoch 201/300\n",
      "119/119 - 0s - loss: 84.5058 - root_mean_squared_error: 9.1927 - val_loss: 85.2471 - val_root_mean_squared_error: 9.2329\n",
      "Epoch 202/300\n",
      "119/119 - 0s - loss: 84.3494 - root_mean_squared_error: 9.1842 - val_loss: 85.1116 - val_root_mean_squared_error: 9.2256\n",
      "Epoch 203/300\n",
      "119/119 - 0s - loss: 84.1978 - root_mean_squared_error: 9.1759 - val_loss: 84.9800 - val_root_mean_squared_error: 9.2185\n",
      "Epoch 204/300\n",
      "119/119 - 0s - loss: 84.0509 - root_mean_squared_error: 9.1679 - val_loss: 84.8521 - val_root_mean_squared_error: 9.2115\n",
      "Epoch 205/300\n",
      "119/119 - 0s - loss: 83.9083 - root_mean_squared_error: 9.1601 - val_loss: 84.7275 - val_root_mean_squared_error: 9.2048\n",
      "Epoch 206/300\n",
      "119/119 - 1s - loss: 83.7698 - root_mean_squared_error: 9.1526 - val_loss: 84.6062 - val_root_mean_squared_error: 9.1982\n",
      "Epoch 207/300\n",
      "119/119 - 1s - loss: 83.6353 - root_mean_squared_error: 9.1452 - val_loss: 84.4878 - val_root_mean_squared_error: 9.1917\n",
      "Epoch 208/300\n",
      "119/119 - 0s - loss: 83.5044 - root_mean_squared_error: 9.1381 - val_loss: 84.3724 - val_root_mean_squared_error: 9.1854\n",
      "Epoch 209/300\n",
      "119/119 - 0s - loss: 83.3769 - root_mean_squared_error: 9.1311 - val_loss: 84.2596 - val_root_mean_squared_error: 9.1793\n",
      "Epoch 210/300\n",
      "119/119 - 0s - loss: 83.2528 - root_mean_squared_error: 9.1243 - val_loss: 84.1493 - val_root_mean_squared_error: 9.1733\n",
      "Epoch 211/300\n",
      "119/119 - 0s - loss: 83.1316 - root_mean_squared_error: 9.1177 - val_loss: 84.0414 - val_root_mean_squared_error: 9.1674\n",
      "Epoch 212/300\n",
      "119/119 - 1s - loss: 83.0135 - root_mean_squared_error: 9.1112 - val_loss: 83.9357 - val_root_mean_squared_error: 9.1616\n",
      "Epoch 213/300\n",
      "119/119 - 0s - loss: 82.8981 - root_mean_squared_error: 9.1048 - val_loss: 83.8322 - val_root_mean_squared_error: 9.1560\n",
      "Epoch 214/300\n",
      "119/119 - 0s - loss: 82.7852 - root_mean_squared_error: 9.0986 - val_loss: 83.7306 - val_root_mean_squared_error: 9.1504\n",
      "Epoch 215/300\n",
      "119/119 - 0s - loss: 82.6749 - root_mean_squared_error: 9.0926 - val_loss: 83.6310 - val_root_mean_squared_error: 9.1450\n",
      "Epoch 216/300\n",
      "119/119 - 1s - loss: 82.5670 - root_mean_squared_error: 9.0866 - val_loss: 83.5332 - val_root_mean_squared_error: 9.1397\n",
      "Epoch 217/300\n",
      "119/119 - 0s - loss: 82.4612 - root_mean_squared_error: 9.0808 - val_loss: 83.4371 - val_root_mean_squared_error: 9.1344\n",
      "Epoch 218/300\n",
      "119/119 - 1s - loss: 82.3577 - root_mean_squared_error: 9.0751 - val_loss: 83.3427 - val_root_mean_squared_error: 9.1292\n",
      "Epoch 219/300\n",
      "119/119 - 1s - loss: 82.2561 - root_mean_squared_error: 9.0695 - val_loss: 83.2498 - val_root_mean_squared_error: 9.1241\n",
      "Epoch 220/300\n",
      "119/119 - 1s - loss: 82.1565 - root_mean_squared_error: 9.0640 - val_loss: 83.1585 - val_root_mean_squared_error: 9.1191\n",
      "Epoch 221/300\n",
      "119/119 - 0s - loss: 82.0587 - root_mean_squared_error: 9.0586 - val_loss: 83.0685 - val_root_mean_squared_error: 9.1142\n",
      "Epoch 222/300\n",
      "119/119 - 1s - loss: 81.9627 - root_mean_squared_error: 9.0533 - val_loss: 82.9800 - val_root_mean_squared_error: 9.1093\n",
      "Epoch 223/300\n",
      "119/119 - 0s - loss: 81.8683 - root_mean_squared_error: 9.0481 - val_loss: 82.8927 - val_root_mean_squared_error: 9.1045\n",
      "Epoch 224/300\n",
      "119/119 - 0s - loss: 81.7756 - root_mean_squared_error: 9.0430 - val_loss: 82.8068 - val_root_mean_squared_error: 9.0998\n",
      "Epoch 225/300\n",
      "119/119 - 0s - loss: 81.6845 - root_mean_squared_error: 9.0379 - val_loss: 82.7221 - val_root_mean_squared_error: 9.0952\n",
      "Epoch 226/300\n",
      "119/119 - 0s - loss: 81.5948 - root_mean_squared_error: 9.0330 - val_loss: 82.6386 - val_root_mean_squared_error: 9.0906\n",
      "Epoch 227/300\n",
      "119/119 - 0s - loss: 81.5066 - root_mean_squared_error: 9.0281 - val_loss: 82.5562 - val_root_mean_squared_error: 9.0860\n",
      "Epoch 228/300\n",
      "119/119 - 0s - loss: 81.4197 - root_mean_squared_error: 9.0233 - val_loss: 82.4749 - val_root_mean_squared_error: 9.0816\n",
      "Epoch 229/300\n",
      "119/119 - 0s - loss: 81.3342 - root_mean_squared_error: 9.0185 - val_loss: 82.3947 - val_root_mean_squared_error: 9.0772\n",
      "Epoch 230/300\n",
      "119/119 - 0s - loss: 81.2499 - root_mean_squared_error: 9.0139 - val_loss: 82.3156 - val_root_mean_squared_error: 9.0728\n",
      "Epoch 231/300\n",
      "119/119 - 0s - loss: 81.1669 - root_mean_squared_error: 9.0093 - val_loss: 82.2374 - val_root_mean_squared_error: 9.0685\n",
      "Epoch 232/300\n",
      "119/119 - 0s - loss: 81.0850 - root_mean_squared_error: 9.0047 - val_loss: 82.1603 - val_root_mean_squared_error: 9.0642\n",
      "Epoch 233/300\n",
      "119/119 - 0s - loss: 81.0043 - root_mean_squared_error: 9.0002 - val_loss: 82.0841 - val_root_mean_squared_error: 9.0600\n",
      "Epoch 234/300\n",
      "119/119 - 1s - loss: 80.9248 - root_mean_squared_error: 8.9958 - val_loss: 82.0089 - val_root_mean_squared_error: 9.0559\n",
      "Epoch 235/300\n",
      "119/119 - 0s - loss: 80.8463 - root_mean_squared_error: 8.9915 - val_loss: 81.9346 - val_root_mean_squared_error: 9.0518\n",
      "Epoch 236/300\n",
      "119/119 - 0s - loss: 80.7689 - root_mean_squared_error: 8.9872 - val_loss: 81.8611 - val_root_mean_squared_error: 9.0477\n",
      "Epoch 237/300\n",
      "119/119 - 0s - loss: 80.6925 - root_mean_squared_error: 8.9829 - val_loss: 81.7886 - val_root_mean_squared_error: 9.0437\n",
      "Epoch 238/300\n",
      "119/119 - 0s - loss: 80.6170 - root_mean_squared_error: 8.9787 - val_loss: 81.7169 - val_root_mean_squared_error: 9.0397\n",
      "Epoch 239/300\n",
      "119/119 - 0s - loss: 80.5425 - root_mean_squared_error: 8.9746 - val_loss: 81.6460 - val_root_mean_squared_error: 9.0358\n",
      "Epoch 240/300\n",
      "119/119 - 0s - loss: 80.4689 - root_mean_squared_error: 8.9704 - val_loss: 81.5759 - val_root_mean_squared_error: 9.0319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 241/300\n",
      "119/119 - 1s - loss: 80.3963 - root_mean_squared_error: 8.9664 - val_loss: 81.5067 - val_root_mean_squared_error: 9.0281\n",
      "Epoch 242/300\n",
      "119/119 - 1s - loss: 80.3245 - root_mean_squared_error: 8.9624 - val_loss: 81.4382 - val_root_mean_squared_error: 9.0243\n",
      "Epoch 243/300\n",
      "119/119 - 1s - loss: 80.2536 - root_mean_squared_error: 8.9584 - val_loss: 81.3705 - val_root_mean_squared_error: 9.0206\n",
      "Epoch 244/300\n",
      "119/119 - 1s - loss: 80.1836 - root_mean_squared_error: 8.9545 - val_loss: 81.3035 - val_root_mean_squared_error: 9.0168\n",
      "Epoch 245/300\n",
      "119/119 - 0s - loss: 80.1143 - root_mean_squared_error: 8.9507 - val_loss: 81.2373 - val_root_mean_squared_error: 9.0132\n",
      "Epoch 246/300\n",
      "119/119 - 0s - loss: 80.0459 - root_mean_squared_error: 8.9468 - val_loss: 81.1718 - val_root_mean_squared_error: 9.0095\n",
      "Epoch 247/300\n",
      "119/119 - 0s - loss: 79.9782 - root_mean_squared_error: 8.9431 - val_loss: 81.1071 - val_root_mean_squared_error: 9.0059\n",
      "Epoch 248/300\n",
      "119/119 - 0s - loss: 79.9113 - root_mean_squared_error: 8.9393 - val_loss: 81.0430 - val_root_mean_squared_error: 9.0024\n",
      "Epoch 249/300\n",
      "119/119 - 0s - loss: 79.8451 - root_mean_squared_error: 8.9356 - val_loss: 80.9796 - val_root_mean_squared_error: 8.9989\n",
      "Epoch 250/300\n",
      "119/119 - 0s - loss: 79.7796 - root_mean_squared_error: 8.9319 - val_loss: 80.9168 - val_root_mean_squared_error: 8.9954\n",
      "Epoch 251/300\n",
      "119/119 - 0s - loss: 79.7149 - root_mean_squared_error: 8.9283 - val_loss: 80.8548 - val_root_mean_squared_error: 8.9919\n",
      "Epoch 252/300\n",
      "119/119 - 0s - loss: 79.6508 - root_mean_squared_error: 8.9247 - val_loss: 80.7934 - val_root_mean_squared_error: 8.9885\n",
      "Epoch 253/300\n",
      "119/119 - 1s - loss: 79.5874 - root_mean_squared_error: 8.9212 - val_loss: 80.7326 - val_root_mean_squared_error: 8.9851\n",
      "Epoch 254/300\n",
      "119/119 - 0s - loss: 79.5247 - root_mean_squared_error: 8.9177 - val_loss: 80.6725 - val_root_mean_squared_error: 8.9818\n",
      "Epoch 255/300\n",
      "119/119 - 0s - loss: 79.4627 - root_mean_squared_error: 8.9142 - val_loss: 80.6130 - val_root_mean_squared_error: 8.9785\n",
      "Epoch 256/300\n",
      "119/119 - 0s - loss: 79.4013 - root_mean_squared_error: 8.9107 - val_loss: 80.5541 - val_root_mean_squared_error: 8.9752\n",
      "Epoch 257/300\n",
      "119/119 - 1s - loss: 79.3404 - root_mean_squared_error: 8.9073 - val_loss: 80.4958 - val_root_mean_squared_error: 8.9719\n",
      "Epoch 258/300\n",
      "119/119 - 1s - loss: 79.2802 - root_mean_squared_error: 8.9039 - val_loss: 80.4380 - val_root_mean_squared_error: 8.9687\n",
      "Epoch 259/300\n",
      "119/119 - 0s - loss: 79.2206 - root_mean_squared_error: 8.9006 - val_loss: 80.3809 - val_root_mean_squared_error: 8.9655\n",
      "Epoch 260/300\n",
      "119/119 - 1s - loss: 79.1616 - root_mean_squared_error: 8.8973 - val_loss: 80.3243 - val_root_mean_squared_error: 8.9624\n",
      "Epoch 261/300\n",
      "119/119 - 0s - loss: 79.1031 - root_mean_squared_error: 8.8940 - val_loss: 80.2683 - val_root_mean_squared_error: 8.9593\n",
      "Epoch 262/300\n",
      "119/119 - 0s - loss: 79.0453 - root_mean_squared_error: 8.8907 - val_loss: 80.2128 - val_root_mean_squared_error: 8.9562\n",
      "Epoch 263/300\n",
      "119/119 - 0s - loss: 78.9879 - root_mean_squared_error: 8.8875 - val_loss: 80.1579 - val_root_mean_squared_error: 8.9531\n",
      "Epoch 264/300\n",
      "119/119 - 0s - loss: 78.9312 - root_mean_squared_error: 8.8843 - val_loss: 80.1035 - val_root_mean_squared_error: 8.9501\n",
      "Epoch 265/300\n",
      "119/119 - 0s - loss: 78.8749 - root_mean_squared_error: 8.8812 - val_loss: 80.0497 - val_root_mean_squared_error: 8.9470\n",
      "Epoch 266/300\n",
      "119/119 - 0s - loss: 78.8192 - root_mean_squared_error: 8.8780 - val_loss: 79.9963 - val_root_mean_squared_error: 8.9441\n",
      "Epoch 267/300\n",
      "119/119 - 0s - loss: 78.7640 - root_mean_squared_error: 8.8749 - val_loss: 79.9435 - val_root_mean_squared_error: 8.9411\n",
      "Epoch 268/300\n",
      "119/119 - 1s - loss: 78.7093 - root_mean_squared_error: 8.8718 - val_loss: 79.8911 - val_root_mean_squared_error: 8.9382\n",
      "Epoch 269/300\n",
      "119/119 - 1s - loss: 78.6550 - root_mean_squared_error: 8.8688 - val_loss: 79.8393 - val_root_mean_squared_error: 8.9353\n",
      "Epoch 270/300\n",
      "119/119 - 0s - loss: 78.6013 - root_mean_squared_error: 8.8657 - val_loss: 79.7879 - val_root_mean_squared_error: 8.9324\n",
      "Epoch 271/300\n",
      "119/119 - 0s - loss: 78.5480 - root_mean_squared_error: 8.8627 - val_loss: 79.7370 - val_root_mean_squared_error: 8.9296\n",
      "Epoch 272/300\n",
      "119/119 - 0s - loss: 78.4952 - root_mean_squared_error: 8.8598 - val_loss: 79.6866 - val_root_mean_squared_error: 8.9267\n",
      "Epoch 273/300\n",
      "119/119 - 1s - loss: 78.4429 - root_mean_squared_error: 8.8568 - val_loss: 79.6367 - val_root_mean_squared_error: 8.9239\n",
      "Epoch 274/300\n",
      "119/119 - 0s - loss: 78.3910 - root_mean_squared_error: 8.8539 - val_loss: 79.5872 - val_root_mean_squared_error: 8.9212\n",
      "Epoch 275/300\n",
      "119/119 - 0s - loss: 78.3396 - root_mean_squared_error: 8.8510 - val_loss: 79.5381 - val_root_mean_squared_error: 8.9184\n",
      "Epoch 276/300\n",
      "119/119 - 0s - loss: 78.2886 - root_mean_squared_error: 8.8481 - val_loss: 79.4895 - val_root_mean_squared_error: 8.9157\n",
      "Epoch 277/300\n",
      "119/119 - 0s - loss: 78.2380 - root_mean_squared_error: 8.8452 - val_loss: 79.4413 - val_root_mean_squared_error: 8.9130\n",
      "Epoch 278/300\n",
      "119/119 - 0s - loss: 78.1878 - root_mean_squared_error: 8.8424 - val_loss: 79.3936 - val_root_mean_squared_error: 8.9103\n",
      "Epoch 279/300\n",
      "119/119 - 0s - loss: 78.1382 - root_mean_squared_error: 8.8396 - val_loss: 79.3463 - val_root_mean_squared_error: 8.9077\n",
      "Epoch 280/300\n",
      "119/119 - 0s - loss: 78.0888 - root_mean_squared_error: 8.8368 - val_loss: 79.2994 - val_root_mean_squared_error: 8.9050\n",
      "Epoch 281/300\n",
      "119/119 - 0s - loss: 78.0399 - root_mean_squared_error: 8.8340 - val_loss: 79.2528 - val_root_mean_squared_error: 8.9024\n",
      "Epoch 282/300\n",
      "119/119 - 0s - loss: 77.9913 - root_mean_squared_error: 8.8313 - val_loss: 79.2067 - val_root_mean_squared_error: 8.8998\n",
      "Epoch 283/300\n",
      "119/119 - 0s - loss: 77.9432 - root_mean_squared_error: 8.8285 - val_loss: 79.1610 - val_root_mean_squared_error: 8.8972\n",
      "Epoch 284/300\n",
      "119/119 - 0s - loss: 77.8955 - root_mean_squared_error: 8.8258 - val_loss: 79.1157 - val_root_mean_squared_error: 8.8947\n",
      "Epoch 285/300\n",
      "119/119 - 0s - loss: 77.8481 - root_mean_squared_error: 8.8232 - val_loss: 79.0708 - val_root_mean_squared_error: 8.8922\n",
      "Epoch 286/300\n",
      "119/119 - 0s - loss: 77.8011 - root_mean_squared_error: 8.8205 - val_loss: 79.0263 - val_root_mean_squared_error: 8.8897\n",
      "Epoch 287/300\n",
      "119/119 - 0s - loss: 77.7545 - root_mean_squared_error: 8.8179 - val_loss: 78.9821 - val_root_mean_squared_error: 8.8872\n",
      "Epoch 288/300\n",
      "119/119 - 0s - loss: 77.7082 - root_mean_squared_error: 8.8152 - val_loss: 78.9383 - val_root_mean_squared_error: 8.8847\n",
      "Epoch 289/300\n",
      "119/119 - 0s - loss: 77.6623 - root_mean_squared_error: 8.8126 - val_loss: 78.8948 - val_root_mean_squared_error: 8.8823\n",
      "Epoch 290/300\n",
      "119/119 - 0s - loss: 77.6167 - root_mean_squared_error: 8.8100 - val_loss: 78.8518 - val_root_mean_squared_error: 8.8799\n",
      "Epoch 291/300\n",
      "119/119 - 0s - loss: 77.5715 - root_mean_squared_error: 8.8075 - val_loss: 78.8090 - val_root_mean_squared_error: 8.8774\n",
      "Epoch 292/300\n",
      "119/119 - 0s - loss: 77.5266 - root_mean_squared_error: 8.8049 - val_loss: 78.7666 - val_root_mean_squared_error: 8.8751\n",
      "Epoch 293/300\n",
      "119/119 - 0s - loss: 77.4821 - root_mean_squared_error: 8.8024 - val_loss: 78.7246 - val_root_mean_squared_error: 8.8727\n",
      "Epoch 294/300\n",
      "119/119 - 0s - loss: 77.4379 - root_mean_squared_error: 8.7999 - val_loss: 78.6829 - val_root_mean_squared_error: 8.8703\n",
      "Epoch 295/300\n",
      "119/119 - 0s - loss: 77.3940 - root_mean_squared_error: 8.7974 - val_loss: 78.6415 - val_root_mean_squared_error: 8.8680\n",
      "Epoch 296/300\n",
      "119/119 - 0s - loss: 77.3504 - root_mean_squared_error: 8.7949 - val_loss: 78.6005 - val_root_mean_squared_error: 8.8657\n",
      "Epoch 297/300\n",
      "119/119 - 0s - loss: 77.3072 - root_mean_squared_error: 8.7924 - val_loss: 78.5598 - val_root_mean_squared_error: 8.8634\n",
      "Epoch 298/300\n",
      "119/119 - 0s - loss: 77.2642 - root_mean_squared_error: 8.7900 - val_loss: 78.5194 - val_root_mean_squared_error: 8.8611\n",
      "Epoch 299/300\n",
      "119/119 - 0s - loss: 77.2216 - root_mean_squared_error: 8.7876 - val_loss: 78.4793 - val_root_mean_squared_error: 8.8589\n",
      "Epoch 300/300\n",
      "119/119 - 0s - loss: 77.1792 - root_mean_squared_error: 8.7852 - val_loss: 78.4395 - val_root_mean_squared_error: 8.8566\n",
      "Epoch 1/300\n",
      "WARNING:tensorflow:Layer dense_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dense_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119/119 - 1s - loss: 10479.3896 - root_mean_squared_error: 102.3689 - val_loss: 81.8115 - val_root_mean_squared_error: 9.0450\n",
      "Epoch 2/300\n",
      "119/119 - 1s - loss: 81.0073 - root_mean_squared_error: 9.0004 - val_loss: 79.0262 - val_root_mean_squared_error: 8.8897\n",
      "Epoch 3/300\n",
      "119/119 - 1s - loss: 80.7780 - root_mean_squared_error: 8.9877 - val_loss: 82.4070 - val_root_mean_squared_error: 9.0778\n",
      "Epoch 4/300\n",
      "119/119 - 1s - loss: 80.2063 - root_mean_squared_error: 8.9558 - val_loss: 75.7367 - val_root_mean_squared_error: 8.7027\n",
      "Epoch 5/300\n",
      "119/119 - 1s - loss: 86.6749 - root_mean_squared_error: 9.3099 - val_loss: 106.9097 - val_root_mean_squared_error: 10.3397\n",
      "Epoch 6/300\n",
      "119/119 - 1s - loss: 92.4985 - root_mean_squared_error: 9.6176 - val_loss: 74.6331 - val_root_mean_squared_error: 8.6390\n",
      "Epoch 7/300\n",
      "119/119 - 1s - loss: 108.2411 - root_mean_squared_error: 10.4039 - val_loss: 117.9028 - val_root_mean_squared_error: 10.8583\n",
      "Epoch 8/300\n",
      "119/119 - 1s - loss: 109.3718 - root_mean_squared_error: 10.4581 - val_loss: 118.9082 - val_root_mean_squared_error: 10.9045\n",
      "Epoch 9/300\n",
      "119/119 - 1s - loss: 172.8929 - root_mean_squared_error: 13.1489 - val_loss: 74.0793 - val_root_mean_squared_error: 8.6069\n",
      "Epoch 10/300\n",
      "119/119 - 1s - loss: 158.8783 - root_mean_squared_error: 12.6047 - val_loss: 74.2127 - val_root_mean_squared_error: 8.6147\n",
      "Epoch 11/300\n",
      "119/119 - 1s - loss: 160.5793 - root_mean_squared_error: 12.6720 - val_loss: 87.0168 - val_root_mean_squared_error: 9.3283\n",
      "Epoch 12/300\n",
      "119/119 - 1s - loss: 171.2799 - root_mean_squared_error: 13.0874 - val_loss: 83.5625 - val_root_mean_squared_error: 9.1413\n",
      "Epoch 13/300\n",
      "119/119 - 1s - loss: 187.0608 - root_mean_squared_error: 13.6770 - val_loss: 89.3580 - val_root_mean_squared_error: 9.4529\n",
      "Epoch 14/300\n",
      "119/119 - 1s - loss: 170.4713 - root_mean_squared_error: 13.0565 - val_loss: 173.7760 - val_root_mean_squared_error: 13.1824\n",
      "Epoch 1/300\n",
      "WARNING:tensorflow:Layer dense_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dense_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119/119 - 1s - loss: 512682.6562 - root_mean_squared_error: 716.0186 - val_loss: 500992.2812 - val_root_mean_squared_error: 707.8081\n",
      "Epoch 2/300\n",
      "119/119 - 0s - loss: 483593.2188 - root_mean_squared_error: 695.4087 - val_loss: 472392.5312 - val_root_mean_squared_error: 687.3082\n",
      "Epoch 3/300\n",
      "119/119 - 0s - loss: 455792.6250 - root_mean_squared_error: 675.1241 - val_loss: 445033.4375 - val_root_mean_squared_error: 667.1083\n",
      "Epoch 4/300\n",
      "119/119 - 0s - loss: 429197.0938 - root_mean_squared_error: 655.1313 - val_loss: 418860.3438 - val_root_mean_squared_error: 647.1942\n",
      "Epoch 5/300\n",
      "119/119 - 0s - loss: 403757.3438 - root_mean_squared_error: 635.4190 - val_loss: 393827.8125 - val_root_mean_squared_error: 627.5570\n",
      "Epoch 6/300\n",
      "119/119 - 0s - loss: 379430.6875 - root_mean_squared_error: 615.9794 - val_loss: 369895.0625 - val_root_mean_squared_error: 608.1900\n",
      "Epoch 7/300\n",
      "119/119 - 0s - loss: 356178.0000 - root_mean_squared_error: 596.8065 - val_loss: 347024.2500 - val_root_mean_squared_error: 589.0876\n",
      "Epoch 8/300\n",
      "119/119 - 0s - loss: 333962.2812 - root_mean_squared_error: 577.8947 - val_loss: 325179.3125 - val_root_mean_squared_error: 570.2449\n",
      "Epoch 9/300\n",
      "119/119 - 0s - loss: 312748.7812 - root_mean_squared_error: 559.2394 - val_loss: 304325.1875 - val_root_mean_squared_error: 551.6567\n",
      "Epoch 10/300\n",
      "119/119 - 0s - loss: 292503.6875 - root_mean_squared_error: 540.8361 - val_loss: 284430.2500 - val_root_mean_squared_error: 533.3200\n",
      "Epoch 11/300\n",
      "119/119 - 0s - loss: 273196.1875 - root_mean_squared_error: 522.6817 - val_loss: 265462.9688 - val_root_mean_squared_error: 515.2310\n",
      "Epoch 12/300\n",
      "119/119 - 0s - loss: 254796.0938 - root_mean_squared_error: 504.7733 - val_loss: 247394.2969 - val_root_mean_squared_error: 497.3875\n",
      "Epoch 13/300\n",
      "119/119 - 0s - loss: 237274.4062 - root_mean_squared_error: 487.1082 - val_loss: 230195.4531 - val_root_mean_squared_error: 479.7869\n",
      "Epoch 14/300\n",
      "119/119 - 0s - loss: 220603.9062 - root_mean_squared_error: 469.6849 - val_loss: 213839.9062 - val_root_mean_squared_error: 462.4283\n",
      "Epoch 15/300\n",
      "119/119 - 0s - loss: 204757.7188 - root_mean_squared_error: 452.5016 - val_loss: 198300.6562 - val_root_mean_squared_error: 445.3096\n",
      "Epoch 16/300\n",
      "119/119 - 0s - loss: 189710.5312 - root_mean_squared_error: 435.5577 - val_loss: 183553.1719 - val_root_mean_squared_error: 428.4311\n",
      "Epoch 17/300\n",
      "119/119 - 0s - loss: 175437.9688 - root_mean_squared_error: 418.8531 - val_loss: 169573.2344 - val_root_mean_squared_error: 411.7927\n",
      "Epoch 18/300\n",
      "119/119 - 0s - loss: 161916.6094 - root_mean_squared_error: 402.3886 - val_loss: 156337.6719 - val_root_mean_squared_error: 395.3956\n",
      "Epoch 19/300\n",
      "119/119 - 0s - loss: 149123.5000 - root_mean_squared_error: 386.1651 - val_loss: 143823.4688 - val_root_mean_squared_error: 379.2407\n",
      "Epoch 20/300\n",
      "119/119 - 0s - loss: 137036.1094 - root_mean_squared_error: 370.1839 - val_loss: 132008.5938 - val_root_mean_squared_error: 363.3299\n",
      "Epoch 21/300\n",
      "119/119 - 0s - loss: 125633.1016 - root_mean_squared_error: 354.4476 - val_loss: 120871.6719 - val_root_mean_squared_error: 347.6660\n",
      "Epoch 22/300\n",
      "119/119 - 0s - loss: 114893.1562 - root_mean_squared_error: 338.9589 - val_loss: 110391.7422 - val_root_mean_squared_error: 332.2525\n",
      "Epoch 23/300\n",
      "119/119 - 0s - loss: 104795.8281 - root_mean_squared_error: 323.7218 - val_loss: 100547.9609 - val_root_mean_squared_error: 317.0930\n",
      "Epoch 24/300\n",
      "119/119 - 0s - loss: 95320.5781 - root_mean_squared_error: 308.7403 - val_loss: 91320.2266 - val_root_mean_squared_error: 302.1924\n",
      "Epoch 25/300\n",
      "119/119 - 0s - loss: 86447.4531 - root_mean_squared_error: 294.0195 - val_loss: 82688.3984 - val_root_mean_squared_error: 287.5559\n",
      "Epoch 26/300\n",
      "119/119 - 0s - loss: 78156.6797 - root_mean_squared_error: 279.5652 - val_loss: 74632.6641 - val_root_mean_squared_error: 273.1898\n",
      "Epoch 27/300\n",
      "119/119 - 0s - loss: 70428.6094 - root_mean_squared_error: 265.3839 - val_loss: 67133.3516 - val_root_mean_squared_error: 259.1010\n",
      "Epoch 28/300\n",
      "119/119 - 0s - loss: 63243.5820 - root_mean_squared_error: 251.4828 - val_loss: 60170.6211 - val_root_mean_squared_error: 245.2970\n",
      "Epoch 29/300\n",
      "119/119 - 0s - loss: 56582.0898 - root_mean_squared_error: 237.8699 - val_loss: 53724.9609 - val_root_mean_squared_error: 231.7865\n",
      "Epoch 30/300\n",
      "119/119 - 0s - loss: 50424.6797 - root_mean_squared_error: 224.5544 - val_loss: 47776.6875 - val_root_mean_squared_error: 218.5788\n",
      "Epoch 31/300\n",
      "119/119 - 1s - loss: 44751.6328 - root_mean_squared_error: 211.5458 - val_loss: 42305.8516 - val_root_mean_squared_error: 205.6839\n",
      "Epoch 32/300\n",
      "119/119 - 1s - loss: 39543.1992 - root_mean_squared_error: 198.8547 - val_loss: 37292.5977 - val_root_mean_squared_error: 193.1129\n",
      "Epoch 33/300\n",
      "119/119 - 1s - loss: 34779.4336 - root_mean_squared_error: 186.4924 - val_loss: 32716.6387 - val_root_mean_squared_error: 180.8774\n",
      "Epoch 34/300\n",
      "119/119 - 1s - loss: 30440.2715 - root_mean_squared_error: 174.4714 - val_loss: 28557.7812 - val_root_mean_squared_error: 168.9905\n",
      "Epoch 35/300\n",
      "119/119 - 0s - loss: 26505.3574 - root_mean_squared_error: 162.8047 - val_loss: 24795.4082 - val_root_mean_squared_error: 157.4656\n",
      "Epoch 36/300\n",
      "119/119 - 0s - loss: 22954.1973 - root_mean_squared_error: 151.5064 - val_loss: 21408.7559 - val_root_mean_squared_error: 146.3173\n",
      "Epoch 37/300\n",
      "119/119 - 0s - loss: 19766.0234 - root_mean_squared_error: 140.5917 - val_loss: 18376.7695 - val_root_mean_squared_error: 135.5609\n",
      "Epoch 38/300\n",
      "119/119 - 1s - loss: 16919.7812 - root_mean_squared_error: 130.0761 - val_loss: 15678.1943 - val_root_mean_squared_error: 125.2126\n",
      "Epoch 39/300\n",
      "119/119 - 0s - loss: 14394.3223 - root_mean_squared_error: 119.9763 - val_loss: 13291.6494 - val_root_mean_squared_error: 115.2894\n",
      "Epoch 40/300\n",
      "119/119 - 0s - loss: 12168.2803 - root_mean_squared_error: 110.3099 - val_loss: 11195.5762 - val_root_mean_squared_error: 105.8092\n",
      "Epoch 41/300\n",
      "119/119 - 1s - loss: 10220.2295 - root_mean_squared_error: 101.0952 - val_loss: 9368.3643 - val_root_mean_squared_error: 96.7903\n",
      "Epoch 42/300\n",
      "119/119 - 0s - loss: 8528.6816 - root_mean_squared_error: 92.3509 - val_loss: 7788.4219 - val_root_mean_squared_error: 88.2520\n",
      "Epoch 43/300\n",
      "119/119 - 1s - loss: 7072.2383 - root_mean_squared_error: 84.0966 - val_loss: 6434.2769 - val_root_mean_squared_error: 80.2139\n",
      "Epoch 44/300\n",
      "119/119 - 1s - loss: 5829.6665 - root_mean_squared_error: 76.3522 - val_loss: 5284.6812 - val_root_mean_squared_error: 72.6958\n",
      "Epoch 45/300\n",
      "119/119 - 0s - loss: 4780.0396 - root_mean_squared_error: 69.1378 - val_loss: 4318.7969 - val_root_mean_squared_error: 65.7176\n",
      "Epoch 46/300\n",
      "119/119 - 1s - loss: 3902.8928 - root_mean_squared_error: 62.4731 - val_loss: 3516.3054 - val_root_mean_squared_error: 59.2984\n",
      "Epoch 47/300\n",
      "119/119 - 0s - loss: 3178.3745 - root_mean_squared_error: 56.3771 - val_loss: 2857.6021 - val_root_mean_squared_error: 53.4565\n",
      "Epoch 48/300\n",
      "119/119 - 0s - loss: 2587.4233 - root_mean_squared_error: 50.8667 - val_loss: 2323.9607 - val_root_mean_squared_error: 48.2075\n",
      "Epoch 49/300\n",
      "119/119 - 0s - loss: 2111.9229 - root_mean_squared_error: 45.9557 - val_loss: 1897.6946 - val_root_mean_squared_error: 43.5625\n",
      "Epoch 50/300\n",
      "119/119 - 0s - loss: 1734.8851 - root_mean_squared_error: 41.6520 - val_loss: 1562.3334 - val_root_mean_squared_error: 39.5264\n",
      "Epoch 51/300\n",
      "119/119 - 0s - loss: 1440.5867 - root_mean_squared_error: 37.9551 - val_loss: 1302.7452 - val_root_mean_squared_error: 36.0936\n",
      "Epoch 52/300\n",
      "119/119 - 0s - loss: 1214.7050 - root_mean_squared_error: 34.8526 - val_loss: 1105.2632 - val_root_mean_squared_error: 33.2455\n",
      "Epoch 53/300\n",
      "119/119 - 0s - loss: 1044.4100 - root_mean_squared_error: 32.3173 - val_loss: 957.7642 - val_root_mean_squared_error: 30.9478\n",
      "Epoch 54/300\n",
      "119/119 - 0s - loss: 918.4255 - root_mean_squared_error: 30.3055 - val_loss: 849.6974 - val_root_mean_squared_error: 29.1496\n",
      "Epoch 55/300\n",
      "119/119 - 0s - loss: 827.0385 - root_mean_squared_error: 28.7583 - val_loss: 772.0767 - val_root_mean_squared_error: 27.7863\n",
      "Epoch 56/300\n",
      "119/119 - 0s - loss: 762.0649 - root_mean_squared_error: 27.6055 - val_loss: 717.4222 - val_root_mean_squared_error: 26.7847\n",
      "Epoch 57/300\n",
      "119/119 - 0s - loss: 716.7734 - root_mean_squared_error: 26.7726 - val_loss: 679.6609 - val_root_mean_squared_error: 26.0703\n",
      "Epoch 58/300\n",
      "119/119 - 0s - loss: 685.7670 - root_mean_squared_error: 26.1872 - val_loss: 653.9924 - val_root_mean_squared_error: 25.5733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/300\n",
      "119/119 - 0s - loss: 664.8378 - root_mean_squared_error: 25.7844 - val_loss: 636.7288 - val_root_mean_squared_error: 25.2335\n",
      "Epoch 60/300\n",
      "119/119 - 1s - loss: 650.7987 - root_mean_squared_error: 25.5108 - val_loss: 625.1209 - val_root_mean_squared_error: 25.0024\n",
      "Epoch 61/300\n",
      "119/119 - 1s - loss: 641.3099 - root_mean_squared_error: 25.3241 - val_loss: 617.1852 - val_root_mean_squared_error: 24.8432\n",
      "Epoch 62/300\n",
      "119/119 - 1s - loss: 634.7116 - root_mean_squared_error: 25.1935 - val_loss: 611.5378 - val_root_mean_squared_error: 24.7293\n",
      "Epoch 63/300\n",
      "119/119 - 0s - loss: 629.8663 - root_mean_squared_error: 25.0971 - val_loss: 607.2491 - val_root_mean_squared_error: 24.6424\n",
      "Epoch 64/300\n",
      "119/119 - 0s - loss: 626.0233 - root_mean_squared_error: 25.0205 - val_loss: 603.7161 - val_root_mean_squared_error: 24.5706\n",
      "Epoch 65/300\n",
      "119/119 - 0s - loss: 622.7062 - root_mean_squared_error: 24.9541 - val_loss: 600.5637 - val_root_mean_squared_error: 24.5064\n",
      "Epoch 66/300\n",
      "119/119 - 0s - loss: 619.6255 - root_mean_squared_error: 24.8923 - val_loss: 597.5683 - val_root_mean_squared_error: 24.4452\n",
      "Epoch 67/300\n",
      "119/119 - 0s - loss: 616.6130 - root_mean_squared_error: 24.8317 - val_loss: 594.6016 - val_root_mean_squared_error: 24.3845\n",
      "Epoch 68/300\n",
      "119/119 - 0s - loss: 613.5747 - root_mean_squared_error: 24.7704 - val_loss: 591.5930 - val_root_mean_squared_error: 24.3227\n",
      "Epoch 69/300\n",
      "119/119 - 0s - loss: 610.4612 - root_mean_squared_error: 24.7075 - val_loss: 588.5054 - val_root_mean_squared_error: 24.2591\n",
      "Epoch 70/300\n",
      "119/119 - 0s - loss: 607.2478 - root_mean_squared_error: 24.6424 - val_loss: 585.3199 - val_root_mean_squared_error: 24.1934\n",
      "Epoch 71/300\n",
      "119/119 - 0s - loss: 603.9225 - root_mean_squared_error: 24.5748 - val_loss: 582.0270 - val_root_mean_squared_error: 24.1252\n",
      "Epoch 72/300\n",
      "119/119 - 0s - loss: 600.4796 - root_mean_squared_error: 24.5047 - val_loss: 578.6222 - val_root_mean_squared_error: 24.0546\n",
      "Epoch 73/300\n",
      "119/119 - 0s - loss: 596.9177 - root_mean_squared_error: 24.4319 - val_loss: 575.1031 - val_root_mean_squared_error: 23.9813\n",
      "Epoch 74/300\n",
      "119/119 - 0s - loss: 593.2355 - root_mean_squared_error: 24.3564 - val_loss: 571.4694 - val_root_mean_squared_error: 23.9054\n",
      "Epoch 75/300\n",
      "119/119 - 0s - loss: 589.4333 - root_mean_squared_error: 24.2782 - val_loss: 567.7208 - val_root_mean_squared_error: 23.8269\n",
      "Epoch 76/300\n",
      "119/119 - 1s - loss: 585.5122 - root_mean_squared_error: 24.1974 - val_loss: 563.8580 - val_root_mean_squared_error: 23.7457\n",
      "Epoch 77/300\n",
      "119/119 - 0s - loss: 581.4730 - root_mean_squared_error: 24.1138 - val_loss: 559.8821 - val_root_mean_squared_error: 23.6618\n",
      "Epoch 78/300\n",
      "119/119 - 0s - loss: 577.3177 - root_mean_squared_error: 24.0274 - val_loss: 555.7951 - val_root_mean_squared_error: 23.5753\n",
      "Epoch 79/300\n",
      "119/119 - 0s - loss: 573.0481 - root_mean_squared_error: 23.9384 - val_loss: 551.5990 - val_root_mean_squared_error: 23.4861\n",
      "Epoch 80/300\n",
      "119/119 - 1s - loss: 568.6669 - root_mean_squared_error: 23.8467 - val_loss: 547.2964 - val_root_mean_squared_error: 23.3944\n",
      "Epoch 81/300\n",
      "119/119 - 0s - loss: 564.1774 - root_mean_squared_error: 23.7524 - val_loss: 542.8908 - val_root_mean_squared_error: 23.3000\n",
      "Epoch 82/300\n",
      "119/119 - 0s - loss: 559.5825 - root_mean_squared_error: 23.6555 - val_loss: 538.3856 - val_root_mean_squared_error: 23.2031\n",
      "Epoch 83/300\n",
      "119/119 - 0s - loss: 554.8868 - root_mean_squared_error: 23.5560 - val_loss: 533.7847 - val_root_mean_squared_error: 23.1038\n",
      "Epoch 84/300\n",
      "119/119 - 0s - loss: 550.0945 - root_mean_squared_error: 23.4541 - val_loss: 529.0931 - val_root_mean_squared_error: 23.0020\n",
      "Epoch 85/300\n",
      "119/119 - 0s - loss: 545.2104 - root_mean_squared_error: 23.3497 - val_loss: 524.3152 - val_root_mean_squared_error: 22.8979\n",
      "Epoch 86/300\n",
      "119/119 - 0s - loss: 540.2399 - root_mean_squared_error: 23.2431 - val_loss: 519.4566 - val_root_mean_squared_error: 22.7916\n",
      "Epoch 87/300\n",
      "119/119 - 0s - loss: 535.1887 - root_mean_squared_error: 23.1341 - val_loss: 514.5229 - val_root_mean_squared_error: 22.6831\n",
      "Epoch 88/300\n",
      "119/119 - 0s - loss: 530.0625 - root_mean_squared_error: 23.0231 - val_loss: 509.5198 - val_root_mean_squared_error: 22.5725\n",
      "Epoch 89/300\n",
      "119/119 - 0s - loss: 524.8675 - root_mean_squared_error: 22.9100 - val_loss: 504.4532 - val_root_mean_squared_error: 22.4600\n",
      "Epoch 90/300\n",
      "119/119 - 0s - loss: 519.6099 - root_mean_squared_error: 22.7950 - val_loss: 499.3293 - val_root_mean_squared_error: 22.3457\n",
      "Epoch 91/300\n",
      "119/119 - 0s - loss: 514.2961 - root_mean_squared_error: 22.6781 - val_loss: 494.1545 - val_root_mean_squared_error: 22.2296\n",
      "Epoch 92/300\n",
      "119/119 - 0s - loss: 508.9329 - root_mean_squared_error: 22.5595 - val_loss: 488.9349 - val_root_mean_squared_error: 22.1119\n",
      "Epoch 93/300\n",
      "119/119 - 0s - loss: 503.5262 - root_mean_squared_error: 22.4394 - val_loss: 483.6766 - val_root_mean_squared_error: 21.9926\n",
      "Epoch 94/300\n",
      "119/119 - 1s - loss: 498.0829 - root_mean_squared_error: 22.3178 - val_loss: 478.3858 - val_root_mean_squared_error: 21.8720\n",
      "Epoch 95/300\n",
      "119/119 - 0s - loss: 492.6086 - root_mean_squared_error: 22.1948 - val_loss: 473.0685 - val_root_mean_squared_error: 21.7501\n",
      "Epoch 96/300\n",
      "119/119 - 0s - loss: 487.1099 - root_mean_squared_error: 22.0706 - val_loss: 467.7302 - val_root_mean_squared_error: 21.6271\n",
      "Epoch 97/300\n",
      "119/119 - 0s - loss: 481.5922 - root_mean_squared_error: 21.9452 - val_loss: 462.3764 - val_root_mean_squared_error: 21.5029\n",
      "Epoch 98/300\n",
      "119/119 - 0s - loss: 476.0612 - root_mean_squared_error: 21.8188 - val_loss: 457.0127 - val_root_mean_squared_error: 21.3779\n",
      "Epoch 99/300\n",
      "119/119 - 0s - loss: 470.5224 - root_mean_squared_error: 21.6915 - val_loss: 451.6436 - val_root_mean_squared_error: 21.2519\n",
      "Epoch 100/300\n",
      "119/119 - 0s - loss: 464.9806 - root_mean_squared_error: 21.5634 - val_loss: 446.2740 - val_root_mean_squared_error: 21.1252\n",
      "Epoch 101/300\n",
      "119/119 - 0s - loss: 459.4407 - root_mean_squared_error: 21.4346 - val_loss: 440.9083 - val_root_mean_squared_error: 20.9978\n",
      "Epoch 102/300\n",
      "119/119 - 0s - loss: 453.9065 - root_mean_squared_error: 21.3051 - val_loss: 435.5507 - val_root_mean_squared_error: 20.8699\n",
      "Epoch 103/300\n",
      "119/119 - 0s - loss: 448.3826 - root_mean_squared_error: 21.1750 - val_loss: 430.2045 - val_root_mean_squared_error: 20.7414\n",
      "Epoch 104/300\n",
      "119/119 - 0s - loss: 442.8724 - root_mean_squared_error: 21.0445 - val_loss: 424.8737 - val_root_mean_squared_error: 20.6125\n",
      "Epoch 105/300\n",
      "119/119 - 0s - loss: 437.3798 - root_mean_squared_error: 20.9136 - val_loss: 419.5612 - val_root_mean_squared_error: 20.4832\n",
      "Epoch 106/300\n",
      "119/119 - 0s - loss: 431.9073 - root_mean_squared_error: 20.7824 - val_loss: 414.2701 - val_root_mean_squared_error: 20.3536\n",
      "Epoch 107/300\n",
      "119/119 - 0s - loss: 426.4582 - root_mean_squared_error: 20.6509 - val_loss: 409.0031 - val_root_mean_squared_error: 20.2238\n",
      "Epoch 108/300\n",
      "119/119 - 0s - loss: 421.0352 - root_mean_squared_error: 20.5191 - val_loss: 403.7624 - val_root_mean_squared_error: 20.0938\n",
      "Epoch 109/300\n",
      "119/119 - 0s - loss: 415.6404 - root_mean_squared_error: 20.3873 - val_loss: 398.5504 - val_root_mean_squared_error: 19.9637\n",
      "Epoch 110/300\n",
      "119/119 - 1s - loss: 410.2760 - root_mean_squared_error: 20.2553 - val_loss: 393.3691 - val_root_mean_squared_error: 19.8335\n",
      "Epoch 111/300\n",
      "119/119 - 0s - loss: 404.9442 - root_mean_squared_error: 20.1232 - val_loss: 388.2202 - val_root_mean_squared_error: 19.7033\n",
      "Epoch 112/300\n",
      "119/119 - 1s - loss: 399.6464 - root_mean_squared_error: 19.9912 - val_loss: 383.1056 - val_root_mean_squared_error: 19.5731\n",
      "Epoch 113/300\n",
      "119/119 - 0s - loss: 394.3842 - root_mean_squared_error: 19.8591 - val_loss: 378.0264 - val_root_mean_squared_error: 19.4429\n",
      "Epoch 114/300\n",
      "119/119 - 0s - loss: 389.1591 - root_mean_squared_error: 19.7271 - val_loss: 372.9841 - val_root_mean_squared_error: 19.3128\n",
      "Epoch 115/300\n",
      "119/119 - 0s - loss: 383.9725 - root_mean_squared_error: 19.5952 - val_loss: 367.9802 - val_root_mean_squared_error: 19.1828\n",
      "Epoch 116/300\n",
      "119/119 - 0s - loss: 378.8253 - root_mean_squared_error: 19.4634 - val_loss: 363.0153 - val_root_mean_squared_error: 19.0530\n",
      "Epoch 117/300\n",
      "119/119 - 0s - loss: 373.7186 - root_mean_squared_error: 19.3318 - val_loss: 358.0904 - val_root_mean_squared_error: 18.9233\n",
      "Epoch 118/300\n",
      "119/119 - 0s - loss: 368.6532 - root_mean_squared_error: 19.2003 - val_loss: 353.2065 - val_root_mean_squared_error: 18.7938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/300\n",
      "119/119 - 1s - loss: 363.6304 - root_mean_squared_error: 19.0691 - val_loss: 348.3644 - val_root_mean_squared_error: 18.6645\n",
      "Epoch 120/300\n",
      "119/119 - 0s - loss: 358.6501 - root_mean_squared_error: 18.9381 - val_loss: 343.5645 - val_root_mean_squared_error: 18.5355\n",
      "Epoch 121/300\n",
      "119/119 - 0s - loss: 353.7137 - root_mean_squared_error: 18.8073 - val_loss: 338.8076 - val_root_mean_squared_error: 18.4067\n",
      "Epoch 122/300\n",
      "119/119 - 0s - loss: 348.8214 - root_mean_squared_error: 18.6768 - val_loss: 334.0942 - val_root_mean_squared_error: 18.2782\n",
      "Epoch 123/300\n",
      "119/119 - 1s - loss: 343.9738 - root_mean_squared_error: 18.5465 - val_loss: 329.4248 - val_root_mean_squared_error: 18.1501\n",
      "Epoch 124/300\n",
      "119/119 - 0s - loss: 339.1714 - root_mean_squared_error: 18.4166 - val_loss: 324.7998 - val_root_mean_squared_error: 18.0222\n",
      "Epoch 125/300\n",
      "119/119 - 0s - loss: 334.4146 - root_mean_squared_error: 18.2870 - val_loss: 320.2196 - val_root_mean_squared_error: 17.8947\n",
      "Epoch 126/300\n",
      "119/119 - 0s - loss: 329.7037 - root_mean_squared_error: 18.1577 - val_loss: 315.6846 - val_root_mean_squared_error: 17.7675\n",
      "Epoch 127/300\n",
      "119/119 - 1s - loss: 325.0392 - root_mean_squared_error: 18.0288 - val_loss: 311.1949 - val_root_mean_squared_error: 17.6407\n",
      "Epoch 128/300\n",
      "119/119 - 0s - loss: 320.4211 - root_mean_squared_error: 17.9003 - val_loss: 306.7510 - val_root_mean_squared_error: 17.5143\n",
      "Epoch 129/300\n",
      "119/119 - 0s - loss: 315.8499 - root_mean_squared_error: 17.7722 - val_loss: 302.3530 - val_root_mean_squared_error: 17.3883\n",
      "Epoch 130/300\n",
      "119/119 - 0s - loss: 311.3257 - root_mean_squared_error: 17.6444 - val_loss: 298.0013 - val_root_mean_squared_error: 17.2627\n",
      "Epoch 131/300\n",
      "119/119 - 0s - loss: 306.8487 - root_mean_squared_error: 17.5171 - val_loss: 293.6961 - val_root_mean_squared_error: 17.1376\n",
      "Epoch 132/300\n",
      "119/119 - 0s - loss: 302.4192 - root_mean_squared_error: 17.3902 - val_loss: 289.4374 - val_root_mean_squared_error: 17.0129\n",
      "Epoch 133/300\n",
      "119/119 - 0s - loss: 298.0374 - root_mean_squared_error: 17.2638 - val_loss: 285.2255 - val_root_mean_squared_error: 16.8886\n",
      "Epoch 134/300\n",
      "119/119 - 0s - loss: 293.7031 - root_mean_squared_error: 17.1378 - val_loss: 281.0605 - val_root_mean_squared_error: 16.7649\n",
      "Epoch 135/300\n",
      "119/119 - 0s - loss: 289.4167 - root_mean_squared_error: 17.0123 - val_loss: 276.9426 - val_root_mean_squared_error: 16.6416\n",
      "Epoch 136/300\n",
      "119/119 - 0s - loss: 285.1782 - root_mean_squared_error: 16.8872 - val_loss: 272.8718 - val_root_mean_squared_error: 16.5188\n",
      "Epoch 137/300\n",
      "119/119 - 0s - loss: 280.9879 - root_mean_squared_error: 16.7627 - val_loss: 268.8484 - val_root_mean_squared_error: 16.3966\n",
      "Epoch 138/300\n",
      "119/119 - 0s - loss: 276.8457 - root_mean_squared_error: 16.6387 - val_loss: 264.8723 - val_root_mean_squared_error: 16.2749\n",
      "Epoch 139/300\n",
      "119/119 - 0s - loss: 272.7516 - root_mean_squared_error: 16.5152 - val_loss: 260.9435 - val_root_mean_squared_error: 16.1537\n",
      "Epoch 140/300\n",
      "119/119 - 0s - loss: 268.7059 - root_mean_squared_error: 16.3923 - val_loss: 257.0624 - val_root_mean_squared_error: 16.0332\n",
      "Epoch 141/300\n",
      "119/119 - 0s - loss: 264.7085 - root_mean_squared_error: 16.2699 - val_loss: 253.2287 - val_root_mean_squared_error: 15.9132\n",
      "Epoch 142/300\n",
      "119/119 - 0s - loss: 260.7594 - root_mean_squared_error: 16.1480 - val_loss: 249.4427 - val_root_mean_squared_error: 15.7938\n",
      "Epoch 143/300\n",
      "119/119 - 0s - loss: 256.8587 - root_mean_squared_error: 16.0268 - val_loss: 245.7043 - val_root_mean_squared_error: 15.6750\n",
      "Epoch 144/300\n",
      "119/119 - 0s - loss: 253.0065 - root_mean_squared_error: 15.9062 - val_loss: 242.0136 - val_root_mean_squared_error: 15.5568\n",
      "Epoch 145/300\n",
      "119/119 - 0s - loss: 249.2026 - root_mean_squared_error: 15.7862 - val_loss: 238.3706 - val_root_mean_squared_error: 15.4393\n",
      "Epoch 146/300\n",
      "119/119 - 0s - loss: 245.4473 - root_mean_squared_error: 15.6668 - val_loss: 234.7752 - val_root_mean_squared_error: 15.3224\n",
      "Epoch 147/300\n",
      "119/119 - 0s - loss: 241.7405 - root_mean_squared_error: 15.5480 - val_loss: 231.2276 - val_root_mean_squared_error: 15.2062\n",
      "Epoch 148/300\n",
      "119/119 - 0s - loss: 238.0820 - root_mean_squared_error: 15.4299 - val_loss: 227.7278 - val_root_mean_squared_error: 15.0907\n",
      "Epoch 149/300\n",
      "119/119 - 0s - loss: 234.4721 - root_mean_squared_error: 15.3125 - val_loss: 224.2755 - val_root_mean_squared_error: 14.9758\n",
      "Epoch 150/300\n",
      "119/119 - 0s - loss: 230.9105 - root_mean_squared_error: 15.1957 - val_loss: 220.8710 - val_root_mean_squared_error: 14.8617\n",
      "Epoch 151/300\n",
      "119/119 - 0s - loss: 227.3975 - root_mean_squared_error: 15.0797 - val_loss: 217.5141 - val_root_mean_squared_error: 14.7484\n",
      "Epoch 152/300\n",
      "119/119 - 0s - loss: 223.9328 - root_mean_squared_error: 14.9644 - val_loss: 214.2049 - val_root_mean_squared_error: 14.6357\n",
      "Epoch 153/300\n",
      "119/119 - 0s - loss: 220.5165 - root_mean_squared_error: 14.8498 - val_loss: 210.9434 - val_root_mean_squared_error: 14.5239\n",
      "Epoch 154/300\n",
      "119/119 - 0s - loss: 217.1485 - root_mean_squared_error: 14.7360 - val_loss: 207.7294 - val_root_mean_squared_error: 14.4128\n",
      "Epoch 155/300\n",
      "119/119 - 0s - loss: 213.8289 - root_mean_squared_error: 14.6229 - val_loss: 204.5629 - val_root_mean_squared_error: 14.3025\n",
      "Epoch 156/300\n",
      "119/119 - 0s - loss: 210.5574 - root_mean_squared_error: 14.5106 - val_loss: 201.4439 - val_root_mean_squared_error: 14.1931\n",
      "Epoch 157/300\n",
      "119/119 - 0s - loss: 207.3343 - root_mean_squared_error: 14.3991 - val_loss: 198.3724 - val_root_mean_squared_error: 14.0845\n",
      "Epoch 158/300\n",
      "119/119 - 0s - loss: 204.1592 - root_mean_squared_error: 14.2884 - val_loss: 195.3483 - val_root_mean_squared_error: 13.9767\n",
      "Epoch 159/300\n",
      "119/119 - 1s - loss: 201.0324 - root_mean_squared_error: 14.1786 - val_loss: 192.3714 - val_root_mean_squared_error: 13.8698\n",
      "Epoch 160/300\n",
      "119/119 - 0s - loss: 197.9534 - root_mean_squared_error: 14.0696 - val_loss: 189.4418 - val_root_mean_squared_error: 13.7638\n",
      "Epoch 161/300\n",
      "119/119 - 0s - loss: 194.9224 - root_mean_squared_error: 13.9615 - val_loss: 186.5594 - val_root_mean_squared_error: 13.6587\n",
      "Epoch 162/300\n",
      "119/119 - 0s - loss: 191.9393 - root_mean_squared_error: 13.8542 - val_loss: 183.7242 - val_root_mean_squared_error: 13.5545\n",
      "Epoch 163/300\n",
      "119/119 - 1s - loss: 189.0040 - root_mean_squared_error: 13.7479 - val_loss: 180.9359 - val_root_mean_squared_error: 13.4512\n",
      "Epoch 164/300\n",
      "119/119 - 0s - loss: 186.1164 - root_mean_squared_error: 13.6424 - val_loss: 178.1945 - val_root_mean_squared_error: 13.3490\n",
      "Epoch 165/300\n",
      "119/119 - 0s - loss: 183.2764 - root_mean_squared_error: 13.5380 - val_loss: 175.4999 - val_root_mean_squared_error: 13.2476\n",
      "Epoch 166/300\n",
      "119/119 - 0s - loss: 180.4839 - root_mean_squared_error: 13.4344 - val_loss: 172.8522 - val_root_mean_squared_error: 13.1473\n",
      "Epoch 167/300\n",
      "119/119 - 0s - loss: 177.7388 - root_mean_squared_error: 13.3319 - val_loss: 170.2509 - val_root_mean_squared_error: 13.0480\n",
      "Epoch 168/300\n",
      "119/119 - 0s - loss: 175.0411 - root_mean_squared_error: 13.2303 - val_loss: 167.6962 - val_root_mean_squared_error: 12.9498\n",
      "Epoch 169/300\n",
      "119/119 - 0s - loss: 172.3904 - root_mean_squared_error: 13.1298 - val_loss: 165.1878 - val_root_mean_squared_error: 12.8525\n",
      "Epoch 170/300\n",
      "119/119 - 0s - loss: 169.7869 - root_mean_squared_error: 13.0302 - val_loss: 162.7258 - val_root_mean_squared_error: 12.7564\n",
      "Epoch 171/300\n",
      "119/119 - 0s - loss: 167.2303 - root_mean_squared_error: 12.9318 - val_loss: 160.3098 - val_root_mean_squared_error: 12.6614\n",
      "Epoch 172/300\n",
      "119/119 - 0s - loss: 164.7205 - root_mean_squared_error: 12.8343 - val_loss: 157.9398 - val_root_mean_squared_error: 12.5674\n",
      "Epoch 173/300\n",
      "119/119 - 0s - loss: 162.2573 - root_mean_squared_error: 12.7380 - val_loss: 155.6155 - val_root_mean_squared_error: 12.4746\n",
      "Epoch 174/300\n",
      "119/119 - 0s - loss: 159.8406 - root_mean_squared_error: 12.6428 - val_loss: 153.3370 - val_root_mean_squared_error: 12.3829\n",
      "Epoch 175/300\n",
      "119/119 - 0s - loss: 157.4702 - root_mean_squared_error: 12.5487 - val_loss: 151.1039 - val_root_mean_squared_error: 12.2924\n",
      "Epoch 176/300\n",
      "119/119 - 0s - loss: 155.1461 - root_mean_squared_error: 12.4558 - val_loss: 148.9162 - val_root_mean_squared_error: 12.2031\n",
      "Epoch 177/300\n",
      "119/119 - 0s - loss: 152.8680 - root_mean_squared_error: 12.3640 - val_loss: 146.7737 - val_root_mean_squared_error: 12.1150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 178/300\n",
      "119/119 - 0s - loss: 150.6357 - root_mean_squared_error: 12.2734 - val_loss: 144.6761 - val_root_mean_squared_error: 12.0281\n",
      "Epoch 179/300\n",
      "119/119 - 0s - loss: 148.4490 - root_mean_squared_error: 12.1840 - val_loss: 142.6232 - val_root_mean_squared_error: 11.9425\n",
      "Epoch 180/300\n",
      "119/119 - 0s - loss: 146.3078 - root_mean_squared_error: 12.0958 - val_loss: 140.6150 - val_root_mean_squared_error: 11.8581\n",
      "Epoch 181/300\n",
      "119/119 - 0s - loss: 144.2119 - root_mean_squared_error: 12.0088 - val_loss: 138.6510 - val_root_mean_squared_error: 11.7750\n",
      "Epoch 182/300\n",
      "119/119 - 0s - loss: 142.1609 - root_mean_squared_error: 11.9231 - val_loss: 136.7312 - val_root_mean_squared_error: 11.6932\n",
      "Epoch 183/300\n",
      "119/119 - 0s - loss: 140.1549 - root_mean_squared_error: 11.8387 - val_loss: 134.8552 - val_root_mean_squared_error: 11.6127\n",
      "Epoch 184/300\n",
      "119/119 - 0s - loss: 138.1934 - root_mean_squared_error: 11.7556 - val_loss: 133.0229 - val_root_mean_squared_error: 11.5336\n",
      "Epoch 185/300\n",
      "119/119 - 0s - loss: 136.2762 - root_mean_squared_error: 11.6737 - val_loss: 131.2340 - val_root_mean_squared_error: 11.4557\n",
      "Epoch 186/300\n",
      "119/119 - 0s - loss: 134.4032 - root_mean_squared_error: 11.5932 - val_loss: 129.4882 - val_root_mean_squared_error: 11.3793\n",
      "Epoch 187/300\n",
      "119/119 - 0s - loss: 132.5740 - root_mean_squared_error: 11.5141 - val_loss: 127.7852 - val_root_mean_squared_error: 11.3042\n",
      "Epoch 188/300\n",
      "119/119 - 0s - loss: 130.7884 - root_mean_squared_error: 11.4363 - val_loss: 126.1248 - val_root_mean_squared_error: 11.2305\n",
      "Epoch 189/300\n",
      "119/119 - 0s - loss: 129.0461 - root_mean_squared_error: 11.3598 - val_loss: 124.5067 - val_root_mean_squared_error: 11.1583\n",
      "Epoch 190/300\n",
      "119/119 - 0s - loss: 127.3467 - root_mean_squared_error: 11.2848 - val_loss: 122.9305 - val_root_mean_squared_error: 11.0874\n",
      "Epoch 191/300\n",
      "119/119 - 0s - loss: 125.6901 - root_mean_squared_error: 11.2112 - val_loss: 121.3960 - val_root_mean_squared_error: 11.0180\n",
      "Epoch 192/300\n",
      "119/119 - 0s - loss: 124.0759 - root_mean_squared_error: 11.1389 - val_loss: 119.9027 - val_root_mean_squared_error: 10.9500\n",
      "Epoch 193/300\n",
      "119/119 - 0s - loss: 122.5036 - root_mean_squared_error: 11.0681 - val_loss: 118.4504 - val_root_mean_squared_error: 10.8835\n",
      "Epoch 194/300\n",
      "119/119 - 0s - loss: 120.9731 - root_mean_squared_error: 10.9988 - val_loss: 117.0386 - val_root_mean_squared_error: 10.8184\n",
      "Epoch 195/300\n",
      "119/119 - 0s - loss: 119.4840 - root_mean_squared_error: 10.9309 - val_loss: 115.6671 - val_root_mean_squared_error: 10.7549\n",
      "Epoch 196/300\n",
      "119/119 - 0s - loss: 118.0358 - root_mean_squared_error: 10.8644 - val_loss: 114.3353 - val_root_mean_squared_error: 10.6928\n",
      "Epoch 197/300\n",
      "119/119 - 0s - loss: 116.6282 - root_mean_squared_error: 10.7995 - val_loss: 113.0429 - val_root_mean_squared_error: 10.6322\n",
      "Epoch 198/300\n",
      "119/119 - 0s - loss: 115.2608 - root_mean_squared_error: 10.7360 - val_loss: 111.7895 - val_root_mean_squared_error: 10.5731\n",
      "Epoch 199/300\n",
      "119/119 - 1s - loss: 113.9332 - root_mean_squared_error: 10.6740 - val_loss: 110.5746 - val_root_mean_squared_error: 10.5154\n",
      "Epoch 200/300\n",
      "119/119 - 0s - loss: 112.6449 - root_mean_squared_error: 10.6134 - val_loss: 109.3977 - val_root_mean_squared_error: 10.4593\n",
      "Epoch 201/300\n",
      "119/119 - 0s - loss: 111.3956 - root_mean_squared_error: 10.5544 - val_loss: 108.2585 - val_root_mean_squared_error: 10.4047\n",
      "Epoch 202/300\n",
      "119/119 - 0s - loss: 110.1846 - root_mean_squared_error: 10.4969 - val_loss: 107.1564 - val_root_mean_squared_error: 10.3516\n",
      "Epoch 203/300\n",
      "119/119 - 0s - loss: 109.0116 - root_mean_squared_error: 10.4409 - val_loss: 106.0908 - val_root_mean_squared_error: 10.3000\n",
      "Epoch 204/300\n",
      "119/119 - 0s - loss: 107.8760 - root_mean_squared_error: 10.3863 - val_loss: 105.0612 - val_root_mean_squared_error: 10.2499\n",
      "Epoch 205/300\n",
      "119/119 - 0s - loss: 106.7773 - root_mean_squared_error: 10.3333 - val_loss: 104.0672 - val_root_mean_squared_error: 10.2013\n",
      "Epoch 206/300\n",
      "119/119 - 0s - loss: 105.7151 - root_mean_squared_error: 10.2818 - val_loss: 103.1080 - val_root_mean_squared_error: 10.1542\n",
      "Epoch 207/300\n",
      "119/119 - 0s - loss: 104.6887 - root_mean_squared_error: 10.2317 - val_loss: 102.1833 - val_root_mean_squared_error: 10.1086\n",
      "Epoch 208/300\n",
      "119/119 - 0s - loss: 103.6974 - root_mean_squared_error: 10.1832 - val_loss: 101.2922 - val_root_mean_squared_error: 10.0644\n",
      "Epoch 209/300\n",
      "119/119 - 0s - loss: 102.7409 - root_mean_squared_error: 10.1361 - val_loss: 100.4342 - val_root_mean_squared_error: 10.0217\n",
      "Epoch 210/300\n",
      "119/119 - 0s - loss: 101.8184 - root_mean_squared_error: 10.0905 - val_loss: 99.6087 - val_root_mean_squared_error: 9.9804\n",
      "Epoch 211/300\n",
      "119/119 - 0s - loss: 100.9293 - root_mean_squared_error: 10.0464 - val_loss: 98.8150 - val_root_mean_squared_error: 9.9406\n",
      "Epoch 212/300\n",
      "119/119 - 0s - loss: 100.0730 - root_mean_squared_error: 10.0036 - val_loss: 98.0524 - val_root_mean_squared_error: 9.9021\n",
      "Epoch 213/300\n",
      "119/119 - 0s - loss: 99.2487 - root_mean_squared_error: 9.9624 - val_loss: 97.3202 - val_root_mean_squared_error: 9.8651\n",
      "Epoch 214/300\n",
      "119/119 - 0s - loss: 98.4558 - root_mean_squared_error: 9.9225 - val_loss: 96.6176 - val_root_mean_squared_error: 9.8294\n",
      "Epoch 215/300\n",
      "119/119 - 1s - loss: 97.6937 - root_mean_squared_error: 9.8840 - val_loss: 95.9440 - val_root_mean_squared_error: 9.7951\n",
      "Epoch 216/300\n",
      "119/119 - 0s - loss: 96.9614 - root_mean_squared_error: 9.8469 - val_loss: 95.2986 - val_root_mean_squared_error: 9.7621\n",
      "Epoch 217/300\n",
      "119/119 - 0s - loss: 96.2584 - root_mean_squared_error: 9.8111 - val_loss: 94.6806 - val_root_mean_squared_error: 9.7304\n",
      "Epoch 218/300\n",
      "119/119 - 0s - loss: 95.5839 - root_mean_squared_error: 9.7767 - val_loss: 94.0892 - val_root_mean_squared_error: 9.7000\n",
      "Epoch 219/300\n",
      "119/119 - 0s - loss: 94.9370 - root_mean_squared_error: 9.7436 - val_loss: 93.5236 - val_root_mean_squared_error: 9.6708\n",
      "Epoch 220/300\n",
      "119/119 - 0s - loss: 94.3170 - root_mean_squared_error: 9.7117 - val_loss: 92.9830 - val_root_mean_squared_error: 9.6428\n",
      "Epoch 221/300\n",
      "119/119 - 0s - loss: 93.7231 - root_mean_squared_error: 9.6811 - val_loss: 92.4667 - val_root_mean_squared_error: 9.6160\n",
      "Epoch 222/300\n",
      "119/119 - 0s - loss: 93.1545 - root_mean_squared_error: 9.6517 - val_loss: 91.9737 - val_root_mean_squared_error: 9.5903\n",
      "Epoch 223/300\n",
      "119/119 - 0s - loss: 92.6103 - root_mean_squared_error: 9.6234 - val_loss: 91.5033 - val_root_mean_squared_error: 9.5657\n",
      "Epoch 224/300\n",
      "119/119 - 0s - loss: 92.0899 - root_mean_squared_error: 9.5963 - val_loss: 91.0546 - val_root_mean_squared_error: 9.5423\n",
      "Epoch 225/300\n",
      "119/119 - 1s - loss: 91.5922 - root_mean_squared_error: 9.5704 - val_loss: 90.6267 - val_root_mean_squared_error: 9.5198\n",
      "Epoch 226/300\n",
      "119/119 - 0s - loss: 91.1165 - root_mean_squared_error: 9.5455 - val_loss: 90.2188 - val_root_mean_squared_error: 9.4984\n",
      "Epoch 227/300\n",
      "119/119 - 0s - loss: 90.6620 - root_mean_squared_error: 9.5217 - val_loss: 89.8302 - val_root_mean_squared_error: 9.4779\n",
      "Epoch 228/300\n",
      "119/119 - 0s - loss: 90.2278 - root_mean_squared_error: 9.4988 - val_loss: 89.4599 - val_root_mean_squared_error: 9.4583\n",
      "Epoch 229/300\n",
      "119/119 - 0s - loss: 89.8132 - root_mean_squared_error: 9.4770 - val_loss: 89.1072 - val_root_mean_squared_error: 9.4397\n",
      "Epoch 230/300\n",
      "119/119 - 0s - loss: 89.4172 - root_mean_squared_error: 9.4561 - val_loss: 88.7712 - val_root_mean_squared_error: 9.4218\n",
      "Epoch 231/300\n",
      "119/119 - 0s - loss: 89.0392 - root_mean_squared_error: 9.4361 - val_loss: 88.4512 - val_root_mean_squared_error: 9.4048\n",
      "Epoch 232/300\n",
      "119/119 - 0s - loss: 88.6782 - root_mean_squared_error: 9.4169 - val_loss: 88.1463 - val_root_mean_squared_error: 9.3886\n",
      "Epoch 233/300\n",
      "119/119 - 0s - loss: 88.3336 - root_mean_squared_error: 9.3986 - val_loss: 87.8559 - val_root_mean_squared_error: 9.3731\n",
      "Epoch 234/300\n",
      "119/119 - 0s - loss: 88.0045 - root_mean_squared_error: 9.3811 - val_loss: 87.5790 - val_root_mean_squared_error: 9.3584\n",
      "Epoch 235/300\n",
      "119/119 - 0s - loss: 87.6902 - root_mean_squared_error: 9.3643 - val_loss: 87.3152 - val_root_mean_squared_error: 9.3443\n",
      "Epoch 236/300\n",
      "119/119 - 1s - loss: 87.3900 - root_mean_squared_error: 9.3483 - val_loss: 87.0636 - val_root_mean_squared_error: 9.3308\n",
      "Epoch 237/300\n",
      "119/119 - 0s - loss: 87.1031 - root_mean_squared_error: 9.3329 - val_loss: 86.8235 - val_root_mean_squared_error: 9.3179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 238/300\n",
      "119/119 - 0s - loss: 86.8289 - root_mean_squared_error: 9.3182 - val_loss: 86.5943 - val_root_mean_squared_error: 9.3056\n",
      "Epoch 239/300\n",
      "119/119 - 0s - loss: 86.5667 - root_mean_squared_error: 9.3041 - val_loss: 86.3753 - val_root_mean_squared_error: 9.2938\n",
      "Epoch 240/300\n",
      "119/119 - 0s - loss: 86.3159 - root_mean_squared_error: 9.2906 - val_loss: 86.1660 - val_root_mean_squared_error: 9.2826\n",
      "Epoch 241/300\n",
      "119/119 - 0s - loss: 86.0757 - root_mean_squared_error: 9.2777 - val_loss: 85.9658 - val_root_mean_squared_error: 9.2718\n",
      "Epoch 242/300\n",
      "119/119 - 0s - loss: 85.8457 - root_mean_squared_error: 9.2653 - val_loss: 85.7741 - val_root_mean_squared_error: 9.2614\n",
      "Epoch 243/300\n",
      "119/119 - 0s - loss: 85.6253 - root_mean_squared_error: 9.2534 - val_loss: 85.5904 - val_root_mean_squared_error: 9.2515\n",
      "Epoch 244/300\n",
      "119/119 - 0s - loss: 85.4139 - root_mean_squared_error: 9.2420 - val_loss: 85.4141 - val_root_mean_squared_error: 9.2420\n",
      "Epoch 245/300\n",
      "119/119 - 0s - loss: 85.2110 - root_mean_squared_error: 9.2310 - val_loss: 85.2449 - val_root_mean_squared_error: 9.2328\n",
      "Epoch 246/300\n",
      "119/119 - 0s - loss: 85.0161 - root_mean_squared_error: 9.2204 - val_loss: 85.0822 - val_root_mean_squared_error: 9.2240\n",
      "Epoch 247/300\n",
      "119/119 - 0s - loss: 84.8287 - root_mean_squared_error: 9.2102 - val_loss: 84.9256 - val_root_mean_squared_error: 9.2155\n",
      "Epoch 248/300\n",
      "119/119 - 0s - loss: 84.6484 - root_mean_squared_error: 9.2005 - val_loss: 84.7747 - val_root_mean_squared_error: 9.2073\n",
      "Epoch 249/300\n",
      "119/119 - 0s - loss: 84.4747 - root_mean_squared_error: 9.1910 - val_loss: 84.6292 - val_root_mean_squared_error: 9.1994\n",
      "Epoch 250/300\n",
      "119/119 - 0s - loss: 84.3073 - root_mean_squared_error: 9.1819 - val_loss: 84.4887 - val_root_mean_squared_error: 9.1918\n",
      "Epoch 251/300\n",
      "119/119 - 0s - loss: 84.1457 - root_mean_squared_error: 9.1731 - val_loss: 84.3528 - val_root_mean_squared_error: 9.1844\n",
      "Epoch 252/300\n",
      "119/119 - 0s - loss: 83.9898 - root_mean_squared_error: 9.1646 - val_loss: 84.2213 - val_root_mean_squared_error: 9.1772\n",
      "Epoch 253/300\n",
      "119/119 - 0s - loss: 83.8389 - root_mean_squared_error: 9.1564 - val_loss: 84.0938 - val_root_mean_squared_error: 9.1703\n",
      "Epoch 254/300\n",
      "119/119 - 0s - loss: 83.6929 - root_mean_squared_error: 9.1484 - val_loss: 83.9702 - val_root_mean_squared_error: 9.1635\n",
      "Epoch 255/300\n",
      "119/119 - 0s - loss: 83.5516 - root_mean_squared_error: 9.1407 - val_loss: 83.8501 - val_root_mean_squared_error: 9.1570\n",
      "Epoch 256/300\n",
      "119/119 - 0s - loss: 83.4145 - root_mean_squared_error: 9.1332 - val_loss: 83.7333 - val_root_mean_squared_error: 9.1506\n",
      "Epoch 257/300\n",
      "119/119 - 0s - loss: 83.2815 - root_mean_squared_error: 9.1259 - val_loss: 83.6196 - val_root_mean_squared_error: 9.1444\n",
      "Epoch 258/300\n",
      "119/119 - 0s - loss: 83.1523 - root_mean_squared_error: 9.1188 - val_loss: 83.5089 - val_root_mean_squared_error: 9.1383\n",
      "Epoch 259/300\n",
      "119/119 - 0s - loss: 83.0267 - root_mean_squared_error: 9.1119 - val_loss: 83.4008 - val_root_mean_squared_error: 9.1324\n",
      "Epoch 260/300\n",
      "119/119 - 0s - loss: 82.9044 - root_mean_squared_error: 9.1052 - val_loss: 83.2953 - val_root_mean_squared_error: 9.1266\n",
      "Epoch 261/300\n",
      "119/119 - 0s - loss: 82.7853 - root_mean_squared_error: 9.0986 - val_loss: 83.1922 - val_root_mean_squared_error: 9.1210\n",
      "Epoch 262/300\n",
      "119/119 - 0s - loss: 82.6692 - root_mean_squared_error: 9.0923 - val_loss: 83.0914 - val_root_mean_squared_error: 9.1154\n",
      "Epoch 263/300\n",
      "119/119 - 0s - loss: 82.5560 - root_mean_squared_error: 9.0860 - val_loss: 82.9927 - val_root_mean_squared_error: 9.1100\n",
      "Epoch 264/300\n",
      "119/119 - 0s - loss: 82.4454 - root_mean_squared_error: 9.0799 - val_loss: 82.8959 - val_root_mean_squared_error: 9.1047\n",
      "Epoch 265/300\n",
      "119/119 - 0s - loss: 82.3373 - root_mean_squared_error: 9.0740 - val_loss: 82.8011 - val_root_mean_squared_error: 9.0995\n",
      "Epoch 266/300\n",
      "119/119 - 0s - loss: 82.2317 - root_mean_squared_error: 9.0682 - val_loss: 82.7080 - val_root_mean_squared_error: 9.0944\n",
      "Epoch 267/300\n",
      "119/119 - 0s - loss: 82.1283 - root_mean_squared_error: 9.0625 - val_loss: 82.6167 - val_root_mean_squared_error: 9.0894\n",
      "Epoch 268/300\n",
      "119/119 - 0s - loss: 82.0271 - root_mean_squared_error: 9.0569 - val_loss: 82.5270 - val_root_mean_squared_error: 9.0844\n",
      "Epoch 269/300\n",
      "119/119 - 0s - loss: 81.9279 - root_mean_squared_error: 9.0514 - val_loss: 82.4387 - val_root_mean_squared_error: 9.0796\n",
      "Epoch 270/300\n",
      "119/119 - 0s - loss: 81.8307 - root_mean_squared_error: 9.0460 - val_loss: 82.3520 - val_root_mean_squared_error: 9.0748\n",
      "Epoch 271/300\n",
      "119/119 - 0s - loss: 81.7353 - root_mean_squared_error: 9.0408 - val_loss: 82.2667 - val_root_mean_squared_error: 9.0701\n",
      "Epoch 272/300\n",
      "119/119 - 0s - loss: 81.6417 - root_mean_squared_error: 9.0356 - val_loss: 82.1826 - val_root_mean_squared_error: 9.0655\n",
      "Epoch 273/300\n",
      "119/119 - 0s - loss: 81.5497 - root_mean_squared_error: 9.0305 - val_loss: 82.0999 - val_root_mean_squared_error: 9.0609\n",
      "Epoch 274/300\n",
      "119/119 - 0s - loss: 81.4594 - root_mean_squared_error: 9.0255 - val_loss: 82.0184 - val_root_mean_squared_error: 9.0564\n",
      "Epoch 275/300\n",
      "119/119 - 0s - loss: 81.3707 - root_mean_squared_error: 9.0206 - val_loss: 81.9380 - val_root_mean_squared_error: 9.0520\n",
      "Epoch 276/300\n",
      "119/119 - 0s - loss: 81.2833 - root_mean_squared_error: 9.0157 - val_loss: 81.8588 - val_root_mean_squared_error: 9.0476\n",
      "Epoch 277/300\n",
      "119/119 - 0s - loss: 81.1975 - root_mean_squared_error: 9.0110 - val_loss: 81.7807 - val_root_mean_squared_error: 9.0433\n",
      "Epoch 278/300\n",
      "119/119 - 0s - loss: 81.1129 - root_mean_squared_error: 9.0063 - val_loss: 81.7036 - val_root_mean_squared_error: 9.0390\n",
      "Epoch 279/300\n",
      "119/119 - 0s - loss: 81.0297 - root_mean_squared_error: 9.0017 - val_loss: 81.6276 - val_root_mean_squared_error: 9.0348\n",
      "Epoch 280/300\n",
      "119/119 - 0s - loss: 80.9478 - root_mean_squared_error: 8.9971 - val_loss: 81.5525 - val_root_mean_squared_error: 9.0306\n",
      "Epoch 281/300\n",
      "119/119 - 0s - loss: 80.8671 - root_mean_squared_error: 8.9926 - val_loss: 81.4785 - val_root_mean_squared_error: 9.0265\n",
      "Epoch 282/300\n",
      "119/119 - 0s - loss: 80.7875 - root_mean_squared_error: 8.9882 - val_loss: 81.4053 - val_root_mean_squared_error: 9.0225\n",
      "Epoch 283/300\n",
      "119/119 - 0s - loss: 80.7091 - root_mean_squared_error: 8.9838 - val_loss: 81.3331 - val_root_mean_squared_error: 9.0185\n",
      "Epoch 284/300\n",
      "119/119 - 0s - loss: 80.6318 - root_mean_squared_error: 8.9795 - val_loss: 81.2617 - val_root_mean_squared_error: 9.0145\n",
      "Epoch 285/300\n",
      "119/119 - 0s - loss: 80.5555 - root_mean_squared_error: 8.9753 - val_loss: 81.1913 - val_root_mean_squared_error: 9.0106\n",
      "Epoch 286/300\n",
      "119/119 - 0s - loss: 80.4802 - root_mean_squared_error: 8.9711 - val_loss: 81.1216 - val_root_mean_squared_error: 9.0068\n",
      "Epoch 287/300\n",
      "119/119 - 0s - loss: 80.4060 - root_mean_squared_error: 8.9669 - val_loss: 81.0528 - val_root_mean_squared_error: 9.0029\n",
      "Epoch 288/300\n",
      "119/119 - 0s - loss: 80.3327 - root_mean_squared_error: 8.9628 - val_loss: 80.9848 - val_root_mean_squared_error: 8.9992\n",
      "Epoch 289/300\n",
      "119/119 - 0s - loss: 80.2603 - root_mean_squared_error: 8.9588 - val_loss: 80.9176 - val_root_mean_squared_error: 8.9954\n",
      "Epoch 290/300\n",
      "119/119 - 0s - loss: 80.1889 - root_mean_squared_error: 8.9548 - val_loss: 80.8511 - val_root_mean_squared_error: 8.9917\n",
      "Epoch 291/300\n",
      "119/119 - 1s - loss: 80.1184 - root_mean_squared_error: 8.9509 - val_loss: 80.7854 - val_root_mean_squared_error: 8.9881\n",
      "Epoch 292/300\n",
      "119/119 - 0s - loss: 80.0487 - root_mean_squared_error: 8.9470 - val_loss: 80.7204 - val_root_mean_squared_error: 8.9845\n",
      "Epoch 293/300\n",
      "119/119 - 1s - loss: 79.9798 - root_mean_squared_error: 8.9431 - val_loss: 80.6562 - val_root_mean_squared_error: 8.9809\n",
      "Epoch 294/300\n",
      "119/119 - 0s - loss: 79.9117 - root_mean_squared_error: 8.9393 - val_loss: 80.5927 - val_root_mean_squared_error: 8.9773\n",
      "Epoch 295/300\n",
      "119/119 - 0s - loss: 79.8445 - root_mean_squared_error: 8.9356 - val_loss: 80.5298 - val_root_mean_squared_error: 8.9738\n",
      "Epoch 296/300\n",
      "119/119 - 0s - loss: 79.7780 - root_mean_squared_error: 8.9319 - val_loss: 80.4677 - val_root_mean_squared_error: 8.9704\n",
      "Epoch 297/300\n",
      "119/119 - 0s - loss: 79.7123 - root_mean_squared_error: 8.9282 - val_loss: 80.4062 - val_root_mean_squared_error: 8.9669\n",
      "Epoch 298/300\n",
      "119/119 - 0s - loss: 79.6473 - root_mean_squared_error: 8.9245 - val_loss: 80.3454 - val_root_mean_squared_error: 8.9636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299/300\n",
      "119/119 - 0s - loss: 79.5831 - root_mean_squared_error: 8.9209 - val_loss: 80.2852 - val_root_mean_squared_error: 8.9602\n",
      "Epoch 300/300\n",
      "119/119 - 0s - loss: 79.5195 - root_mean_squared_error: 8.9174 - val_loss: 80.2257 - val_root_mean_squared_error: 8.9569\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i in range(3):\n",
    "    h_sizes = [ 8, 16,32,64]\n",
    "    h_size = np.random.choice(h_sizes)\n",
    "    num_hidden = np.random.randint(0, 3)\n",
    "    activation = np.random.choice(['relu'])\n",
    "    lr = 10 ** np.random.randint(-5, 0)\n",
    "\n",
    "    val_loss = experiment(i, h_size, num_hidden, activation, lr)\n",
    "    results.append({\n",
    "        'val_loss': val_loss,\n",
    "        'h_size': h_size,\n",
    "        'num_hidden': num_hidden,\n",
    "        'activation': activation,\n",
    "        'lr': lr\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 8.856608390808105,\n",
       "  'h_size': 64,\n",
       "  'num_hidden': 0,\n",
       "  'activation': 'relu',\n",
       "  'lr': 1e-05},\n",
       " {'val_loss': 8.606934547424316,\n",
       "  'h_size': 64,\n",
       "  'num_hidden': 1,\n",
       "  'activation': 'relu',\n",
       "  'lr': 0.01},\n",
       " {'val_loss': 8.956877708435059,\n",
       "  'h_size': 16,\n",
       "  'num_hidden': 0,\n",
       "  'activation': 'relu',\n",
       "  'lr': 1e-05}]"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1239)]            0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                39680     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 41,409\n",
      "Trainable params: 41,409\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input = keras.Input(shape=(X_train.shape[1],))\n",
    "\n",
    "x = keras.layers.Dense(32, activation='relu')(input) #함수모형의 API   swish\n",
    "x = keras.layers.Dense(32, activation='relu')(x)\n",
    "x = keras.layers.Dense(16, activation='relu')(x)\n",
    "x = keras.layers.Dense(8, activation='relu')(x)\n",
    "output = keras.layers.Dense(1, activation='linear')(x) #회귀문제  항등함수 회귀는 아웃풋 뉴런이 하나여야함\n",
    "model = keras.Model(input, output) #모델을 입력부터하지않고 X부터도 사용할 수 있음. 어디서부터 어디까지 모델로쓰겠다.\n",
    "\n",
    "model.summary() #파라미터가 학습해야할 웨이트 308*16+1=4944 파라미터가 클수록 시간이오래걸린다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "119/119 - 1s - loss: 273.0324 - root_mean_squared_error: 16.5237 - val_loss: 93.2007 - val_root_mean_squared_error: 9.6540\n",
      "Epoch 2/300\n",
      "119/119 - 0s - loss: 89.7516 - root_mean_squared_error: 9.4737 - val_loss: 78.6515 - val_root_mean_squared_error: 8.8686\n",
      "Epoch 3/300\n",
      "119/119 - 0s - loss: 83.1571 - root_mean_squared_error: 9.1190 - val_loss: 75.0908 - val_root_mean_squared_error: 8.6655\n",
      "Epoch 4/300\n",
      "119/119 - 1s - loss: 80.5735 - root_mean_squared_error: 8.9763 - val_loss: 73.9898 - val_root_mean_squared_error: 8.6017\n",
      "Epoch 5/300\n",
      "119/119 - 0s - loss: 78.4848 - root_mean_squared_error: 8.8592 - val_loss: 77.0355 - val_root_mean_squared_error: 8.7770\n",
      "Epoch 6/300\n",
      "119/119 - 0s - loss: 77.3461 - root_mean_squared_error: 8.7947 - val_loss: 74.8501 - val_root_mean_squared_error: 8.6516\n",
      "Epoch 7/300\n",
      "119/119 - 0s - loss: 77.0305 - root_mean_squared_error: 8.7767 - val_loss: 75.7902 - val_root_mean_squared_error: 8.7058\n",
      "Epoch 8/300\n",
      "119/119 - 0s - loss: 75.9993 - root_mean_squared_error: 8.7178 - val_loss: 76.4475 - val_root_mean_squared_error: 8.7434\n",
      "Epoch 9/300\n",
      "119/119 - 0s - loss: 76.6049 - root_mean_squared_error: 8.7524 - val_loss: 72.9382 - val_root_mean_squared_error: 8.5404\n",
      "Epoch 10/300\n",
      "119/119 - 0s - loss: 76.6363 - root_mean_squared_error: 8.7542 - val_loss: 87.0208 - val_root_mean_squared_error: 9.3285\n",
      "Epoch 11/300\n",
      "119/119 - 0s - loss: 73.7248 - root_mean_squared_error: 8.5863 - val_loss: 75.2091 - val_root_mean_squared_error: 8.6723\n",
      "Epoch 12/300\n",
      "119/119 - 0s - loss: 72.2470 - root_mean_squared_error: 8.4998 - val_loss: 75.7386 - val_root_mean_squared_error: 8.7028\n",
      "Epoch 13/300\n",
      "119/119 - 0s - loss: 71.2397 - root_mean_squared_error: 8.4404 - val_loss: 84.0877 - val_root_mean_squared_error: 9.1699\n",
      "Epoch 14/300\n",
      "119/119 - 1s - loss: 71.3953 - root_mean_squared_error: 8.4496 - val_loss: 74.8541 - val_root_mean_squared_error: 8.6518\n",
      "8.651823153754746\n"
     ]
    }
   ],
   "source": [
    "# loss, optimizer, metrics 설정 옵티마이저의 이름을쓰는건 옵티마이저의 디폴트값으로만 쓸것이고 함수로 쓸경우는 디폴트로 안하고 값을 바꾸겠다는것 .\n",
    "#model.compile(loss='mse', optimizer='adam', metrics=[keras.metrics.RootMeanSquaredError()])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse', metrics=[keras.metrics.RootMeanSquaredError()])\n",
    "# batch size, epoch, 조기종료조건 등 설정\n",
    "callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=5),] #에퐄 돌렸음에도 불구하고 더이상 감소가없으면 스톱\n",
    "# keras.callbacks.ModelCheckpoint(filepath='best_nn_model.h5', monitor='val_loss', save_best_only=True)]\n",
    "\n",
    "hist = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), \n",
    "                 batch_size=128, epochs=300, callbacks=callbacks, shuffle=False, verbose=2) #셔플 트루로하면 셔플하고함.\n",
    "##배치사이즈를 크게하면 한번에 확돌아가니까 .. 배치사이즈를 작게하면 웨이트갱신은 빨리일어나지만 에포크가 느림.\n",
    "##배치사이즈를 크게하면 크게할수록 좋긴한데 어느시점부터는 똑같다.\n",
    "\n",
    "model.save('nn_model_rid.h5')\n",
    "\n",
    "dnn_model = keras.models.load_model('nn_model_rid.h5')\n",
    "\n",
    "print(np.sqrt(mean_squared_error(y_valid, dnn_model.predict(X_valid).flatten())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  weight 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr = X_train_select\n",
    "target =  y['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15110, 1239), (6477, 1239))"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(ftr, target, test_size=0.3, random_state=0)\n",
    "X_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.238833482671076 1 1 1 1 6\n",
      "8.150245818532403 1 1 1 2 5\n",
      "8.077880892495697 1 1 1 3 4\n",
      "8.022177734676063 1 1 1 4 3\n",
      "7.983485105975248 1 1 1 5 2\n",
      "7.962050987265401 1 1 1 6 1\n",
      "8.206749403528812 1 1 2 1 5\n",
      "8.125054026279276 1 1 2 2 4\n",
      "8.059763835663919 1 1 2 3 3\n",
      "8.01127992828635 1 1 2 4 2\n",
      "7.9799086616352435 1 1 2 5 1\n",
      "8.182293800780377 1 1 3 1 4\n",
      "8.107609779948517 1 1 3 2 3\n",
      "8.049488406469056 1 1 3 3 2\n",
      "8.008290325697972 1 1 3 4 1\n",
      "8.165535212034094 1 1 4 1 3\n",
      "8.097963132511456 1 1 4 2 2\n",
      "8.047084658301692 1 1 4 3 1\n",
      "8.156521065077307 1 1 5 1 2\n",
      "8.096141971582636 1 1 5 2 1\n",
      "8.15527705551189 1 1 6 1 1\n",
      "8.154386442964217 1 2 1 1 5\n",
      "8.080170125238855 1 2 1 2 4\n",
      "8.022580834413844 1 2 1 3 3\n",
      "7.981978456860174 1 2 1 4 2\n",
      "7.95862300170727 1 2 1 5 1\n",
      "8.12915874566435 1 2 2 1 4\n",
      "8.06200907020526 1 2 2 2 3\n",
      "8.011634124841372 1 2 2 3 2\n",
      "7.9783516794477105 1 2 2 4 1\n",
      "8.111674484975374 1 2 3 1 3\n",
      "8.051687300565298 1 2 3 2 2\n",
      "8.008595184311645 1 2 3 3 1\n",
      "8.101983776995382 1 2 4 1 2\n",
      "8.049234988398206 1 2 4 2 1\n",
      "8.10011460867483 1 2 5 1 1\n",
      "8.085881404528083 1 3 1 1 4\n",
      "8.026431975553434 1 3 1 2 3\n",
      "7.983937950706121 1 3 1 3 2\n",
      "7.9586709447736395 1 3 1 4 1\n",
      "8.06768409814068 1 3 2 1 3\n",
      "8.015441097195705 1 3 2 2 2\n",
      "7.98026241801888 1 3 2 3 1\n",
      "8.057320427183132 1 3 3 1 2\n",
      "8.012354153358766 1 3 3 2 1\n",
      "8.054820643635445 1 3 4 1 1\n",
      "8.033726194646114 1 4 1 1 3\n",
      "7.989361031090218 1 4 1 2 2\n",
      "7.962194785401137 1 4 1 3 1\n",
      "8.022695925676631 1 4 2 1 2\n",
      "7.985638381819997 1 4 2 2 1\n",
      "8.019562371760928 1 4 3 1 1\n",
      "7.998240648504764 1 5 1 1 2\n",
      "7.969189905975862 1 5 1 2 1\n",
      "7.994472576346197 1 5 2 1 1\n",
      "7.979647173860429 1 6 1 1 1\n",
      "8.146494089052212 2 1 1 1 5\n",
      "8.073572651128353 2 1 1 2 4\n",
      "8.017313001540678 2 1 1 3 3\n",
      "7.9780676245846305 2 1 1 4 2\n",
      "7.956088324945854 2 1 1 5 1\n",
      "8.121299552017517 2 1 2 1 4\n",
      "8.055454870277092 2 1 2 2 3\n",
      "8.00641759404574 2 1 2 3 2\n",
      "7.9744978071954655 2 1 2 4 1\n",
      "8.103856135837004 2 1 3 1 3\n",
      "8.045182914330603 2 1 3 2 2\n",
      "8.003435199053952 2 1 3 3 1\n",
      "8.094213937300674 2 1 4 1 2\n",
      "8.042786858841911 2 1 4 2 1\n",
      "8.09240085716947 2 1 5 1 1\n",
      "8.07695271170405 2 2 1 1 4\n",
      "8.018813858346338 2 2 1 2 3\n",
      "7.977663132664197 2 2 1 3 2\n",
      "7.953764233419606 2 2 1 4 1\n",
      "8.058793367816067 2 2 2 1 3\n",
      "8.007871017891592 2 2 2 2 2\n",
      "7.974043449677351 2 2 2 3 1\n",
      "8.048476447379363 2 2 3 1 2\n",
      "8.00483967077251 2 2 3 2 1\n",
      "8.046032132561189 2 2 4 1 1\n",
      "8.023763638463498 2 3 1 1 3\n",
      "7.9807264368163136 2 3 1 2 2\n",
      "7.9549185410781185 2 3 1 3 1\n",
      "8.012778113189444 2 3 2 1 2\n",
      "7.977058477831678 2 3 2 2 1\n",
      "8.009699159722343 2 3 3 1 1\n",
      "7.987253543262223 2 4 1 1 2\n",
      "7.959549727759653 2 4 1 2 1\n",
      "7.983538957521666 2 4 2 1 1\n",
      "7.967651727405954 2 5 1 1 1\n",
      "8.071583456954977 3 1 1 1 4\n",
      "8.014783122948709 3 1 1 2 3\n",
      "7.97499594308919 3 1 1 3 2\n",
      "7.95247729521696 3 1 1 4 1\n",
      "8.053470168777924 3 1 2 1 3\n",
      "8.003893294655906 3 1 2 2 2\n",
      "7.971433810219807 3 1 2 3 1\n",
      "8.043204657237906 3 1 3 1 2\n",
      "8.000918985165185 3 1 3 2 1\n",
      "8.04081699404694 3 1 4 1 1\n",
      "8.017382157399656 3 2 1 1 3\n",
      "7.975694741523785 3 2 1 2 2\n",
      "7.951258992523339 3 2 1 3 1\n",
      "8.00644638045011 3 2 2 1 2\n",
      "7.97208322389896 3 2 2 2 1\n",
      "8.003423517497081 3 2 3 1 1\n",
      "7.979861667584936 3 3 1 1 2\n",
      "7.953520186453632 3 3 1 2 1\n",
      "7.976202365477968 3 3 2 1 1\n",
      "7.959257908602217 3 4 1 1 1\n",
      "8.01459030556941 4 1 1 1 3\n",
      "7.974272765407324 4 1 1 2 2\n",
      "7.951221133489724 4 1 1 3 1\n",
      "8.003709238304014 4 1 2 1 2\n",
      "7.970719369998654 4 1 2 2 1\n",
      "8.00074388716883 4 1 3 1 1\n",
      "7.976075017181285 4 2 1 1 2\n",
      "7.9511094961928155 4 2 1 2 1\n",
      "7.972472730610121 4 2 2 1 1\n",
      "7.95447711911564 4 3 1 1 1\n",
      "7.975898726920429 5 1 1 1 2\n",
      "7.952320948065739 5 1 1 2 1\n",
      "7.972355115150298 5 1 2 1 1\n",
      "7.953315874394593 5 2 1 1 1\n",
      "7.955775759392374 6 1 1 1 1\n",
      "7.9511094961928155 [4, 2, 1, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "# 최적의 가중치 찾기 \n",
    "weights = []\n",
    "rmse_best = 1000\n",
    "for i in range(1, 10, 1):\n",
    "    for j in range(1, 10, 1):\n",
    "        for k in range(1, 10, 1):\n",
    "            for l in range(1, 10, 1):\n",
    "                for m in range(1, 10, 1):\n",
    "                    if (i+j+k+l+m) != 10:\n",
    "                        continue\n",
    "                    pred = (models[0].predict(X_valid) * i + models[1].predict(X_valid) * j + models[2].predict(X_valid) * k+ models[3].predict(X_valid) * l+dnn_model.predict(X_valid).flatten() * m)/10\n",
    "                    rmse = np.sqrt(mean_squared_error(y_valid, pred))\n",
    "                    print(rmse, i,j,k,l,m)            \n",
    "                    if rmse < rmse_best:\n",
    "                        weights = [i,j,k,l,m]\n",
    "                        rmse_best = rmse \n",
    "print(rmse_best, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.235621955569359\n"
     ]
    }
   ],
   "source": [
    "# Ensemble: 가중평균 \n",
    "weights = [0.4,0.2,0.1,0.2,0.1]\n",
    "pred = models[0].predict(X_valid) * weights[0] + models[1].predict(X_valid) * weights[1] + models[2].predict(X_valid) * weights[2] +models[3].predict(X_valid) * weights[3] + dnn_model.predict(X_valid).flatten() * weights[4] \n",
    "print(np.sqrt(mean_squared_error(y_valid, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전체 데이터로 재학습(마지막)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.067435\n",
      "0:\tlearn: 10.2904299\ttotal: 48.5ms\tremaining: 48.5s\n",
      "1:\tlearn: 10.1524071\ttotal: 85.6ms\tremaining: 42.7s\n",
      "2:\tlearn: 10.0321994\ttotal: 121ms\tremaining: 40.4s\n",
      "3:\tlearn: 9.9191487\ttotal: 165ms\tremaining: 41.1s\n",
      "4:\tlearn: 9.8146364\ttotal: 201ms\tremaining: 40s\n",
      "5:\tlearn: 9.7287970\ttotal: 252ms\tremaining: 41.8s\n",
      "6:\tlearn: 9.6391617\ttotal: 290ms\tremaining: 41.2s\n",
      "7:\tlearn: 9.5571880\ttotal: 330ms\tremaining: 40.9s\n",
      "8:\tlearn: 9.4875662\ttotal: 366ms\tremaining: 40.3s\n",
      "9:\tlearn: 9.4196763\ttotal: 406ms\tremaining: 40.2s\n",
      "10:\tlearn: 9.3547861\ttotal: 443ms\tremaining: 39.9s\n",
      "11:\tlearn: 9.3008807\ttotal: 487ms\tremaining: 40.1s\n",
      "12:\tlearn: 9.2474364\ttotal: 524ms\tremaining: 39.8s\n",
      "13:\tlearn: 9.1975252\ttotal: 560ms\tremaining: 39.4s\n",
      "14:\tlearn: 9.1478490\ttotal: 596ms\tremaining: 39.2s\n",
      "15:\tlearn: 9.1034573\ttotal: 632ms\tremaining: 38.9s\n",
      "16:\tlearn: 9.0624510\ttotal: 667ms\tremaining: 38.6s\n",
      "17:\tlearn: 9.0205755\ttotal: 705ms\tremaining: 38.4s\n",
      "18:\tlearn: 8.9834731\ttotal: 741ms\tremaining: 38.3s\n",
      "19:\tlearn: 8.9502275\ttotal: 778ms\tremaining: 38.1s\n",
      "20:\tlearn: 8.9165493\ttotal: 817ms\tremaining: 38.1s\n",
      "21:\tlearn: 8.8824720\ttotal: 872ms\tremaining: 38.8s\n",
      "22:\tlearn: 8.8523399\ttotal: 910ms\tremaining: 38.7s\n",
      "23:\tlearn: 8.8261103\ttotal: 949ms\tremaining: 38.6s\n",
      "24:\tlearn: 8.7977681\ttotal: 986ms\tremaining: 38.4s\n",
      "25:\tlearn: 8.7730095\ttotal: 1.02s\tremaining: 38.3s\n",
      "26:\tlearn: 8.7463840\ttotal: 1.06s\tremaining: 38.2s\n",
      "27:\tlearn: 8.7242163\ttotal: 1.09s\tremaining: 38s\n",
      "28:\tlearn: 8.7013433\ttotal: 1.13s\tremaining: 38s\n",
      "29:\tlearn: 8.6803899\ttotal: 1.17s\tremaining: 37.9s\n",
      "30:\tlearn: 8.6611313\ttotal: 1.21s\tremaining: 37.7s\n",
      "31:\tlearn: 8.6422943\ttotal: 1.24s\tremaining: 37.6s\n",
      "32:\tlearn: 8.6263296\ttotal: 1.28s\tremaining: 37.5s\n",
      "33:\tlearn: 8.6076212\ttotal: 1.31s\tremaining: 37.3s\n",
      "34:\tlearn: 8.5920346\ttotal: 1.35s\tremaining: 37.2s\n",
      "35:\tlearn: 8.5781134\ttotal: 1.38s\tremaining: 37.1s\n",
      "36:\tlearn: 8.5638931\ttotal: 1.42s\tremaining: 37s\n",
      "37:\tlearn: 8.5498145\ttotal: 1.46s\tremaining: 36.9s\n",
      "38:\tlearn: 8.5373684\ttotal: 1.5s\tremaining: 36.9s\n",
      "39:\tlearn: 8.5238906\ttotal: 1.54s\tremaining: 36.9s\n",
      "40:\tlearn: 8.5111935\ttotal: 1.57s\tremaining: 36.8s\n",
      "41:\tlearn: 8.4992293\ttotal: 1.61s\tremaining: 36.7s\n",
      "42:\tlearn: 8.4869306\ttotal: 1.65s\tremaining: 36.6s\n",
      "43:\tlearn: 8.4776875\ttotal: 1.68s\tremaining: 36.5s\n",
      "44:\tlearn: 8.4673730\ttotal: 1.72s\tremaining: 36.5s\n",
      "45:\tlearn: 8.4564710\ttotal: 1.75s\tremaining: 36.4s\n",
      "46:\tlearn: 8.4450962\ttotal: 1.79s\tremaining: 36.3s\n",
      "47:\tlearn: 8.4355474\ttotal: 1.83s\tremaining: 36.2s\n",
      "48:\tlearn: 8.4260424\ttotal: 1.86s\tremaining: 36.2s\n",
      "49:\tlearn: 8.4152541\ttotal: 1.9s\tremaining: 36.1s\n",
      "50:\tlearn: 8.4071909\ttotal: 1.94s\tremaining: 36s\n",
      "51:\tlearn: 8.3973986\ttotal: 1.97s\tremaining: 36s\n",
      "52:\tlearn: 8.3879285\ttotal: 2.01s\tremaining: 35.9s\n",
      "53:\tlearn: 8.3793125\ttotal: 2.05s\tremaining: 35.9s\n",
      "54:\tlearn: 8.3734377\ttotal: 2.08s\tremaining: 35.8s\n",
      "55:\tlearn: 8.3645489\ttotal: 2.12s\tremaining: 35.8s\n",
      "56:\tlearn: 8.3564737\ttotal: 2.16s\tremaining: 35.7s\n",
      "57:\tlearn: 8.3484763\ttotal: 2.2s\tremaining: 35.7s\n",
      "58:\tlearn: 8.3415843\ttotal: 2.23s\tremaining: 35.6s\n",
      "59:\tlearn: 8.3354072\ttotal: 2.27s\tremaining: 35.5s\n",
      "60:\tlearn: 8.3297735\ttotal: 2.32s\tremaining: 35.7s\n",
      "61:\tlearn: 8.3233834\ttotal: 2.36s\tremaining: 35.7s\n",
      "62:\tlearn: 8.3167247\ttotal: 2.4s\tremaining: 35.7s\n",
      "63:\tlearn: 8.3106262\ttotal: 2.44s\tremaining: 35.6s\n",
      "64:\tlearn: 8.3047612\ttotal: 2.47s\tremaining: 35.6s\n",
      "65:\tlearn: 8.2989909\ttotal: 2.51s\tremaining: 35.5s\n",
      "66:\tlearn: 8.2922653\ttotal: 2.55s\tremaining: 35.5s\n",
      "67:\tlearn: 8.2861681\ttotal: 2.58s\tremaining: 35.4s\n",
      "68:\tlearn: 8.2811369\ttotal: 2.62s\tremaining: 35.3s\n",
      "69:\tlearn: 8.2765178\ttotal: 2.65s\tremaining: 35.2s\n",
      "70:\tlearn: 8.2716911\ttotal: 2.69s\tremaining: 35.2s\n",
      "71:\tlearn: 8.2676138\ttotal: 2.73s\tremaining: 35.1s\n",
      "72:\tlearn: 8.2630336\ttotal: 2.76s\tremaining: 35.1s\n",
      "73:\tlearn: 8.2577153\ttotal: 2.79s\tremaining: 35s\n",
      "74:\tlearn: 8.2537142\ttotal: 2.83s\tremaining: 34.9s\n",
      "75:\tlearn: 8.2495497\ttotal: 2.87s\tremaining: 34.8s\n",
      "76:\tlearn: 8.2436558\ttotal: 2.9s\tremaining: 34.8s\n",
      "77:\tlearn: 8.2378635\ttotal: 2.94s\tremaining: 34.7s\n",
      "78:\tlearn: 8.2327859\ttotal: 2.97s\tremaining: 34.7s\n",
      "79:\tlearn: 8.2282572\ttotal: 3.01s\tremaining: 34.6s\n",
      "80:\tlearn: 8.2231467\ttotal: 3.05s\tremaining: 34.6s\n",
      "81:\tlearn: 8.2199070\ttotal: 3.08s\tremaining: 34.5s\n",
      "82:\tlearn: 8.2156490\ttotal: 3.13s\tremaining: 34.6s\n",
      "83:\tlearn: 8.2119058\ttotal: 3.17s\tremaining: 34.5s\n",
      "84:\tlearn: 8.2065678\ttotal: 3.2s\tremaining: 34.5s\n",
      "85:\tlearn: 8.2029293\ttotal: 3.23s\tremaining: 34.4s\n",
      "86:\tlearn: 8.1998483\ttotal: 3.27s\tremaining: 34.3s\n",
      "87:\tlearn: 8.1949060\ttotal: 3.31s\tremaining: 34.3s\n",
      "88:\tlearn: 8.1901909\ttotal: 3.35s\tremaining: 34.3s\n",
      "89:\tlearn: 8.1863761\ttotal: 3.38s\tremaining: 34.2s\n",
      "90:\tlearn: 8.1824311\ttotal: 3.42s\tremaining: 34.2s\n",
      "91:\tlearn: 8.1793598\ttotal: 3.45s\tremaining: 34.1s\n",
      "92:\tlearn: 8.1763449\ttotal: 3.49s\tremaining: 34s\n",
      "93:\tlearn: 8.1725334\ttotal: 3.52s\tremaining: 34s\n",
      "94:\tlearn: 8.1675037\ttotal: 3.56s\tremaining: 33.9s\n",
      "95:\tlearn: 8.1632121\ttotal: 3.6s\tremaining: 33.9s\n",
      "96:\tlearn: 8.1580945\ttotal: 3.63s\tremaining: 33.8s\n",
      "97:\tlearn: 8.1541524\ttotal: 3.67s\tremaining: 33.8s\n",
      "98:\tlearn: 8.1506240\ttotal: 3.71s\tremaining: 33.8s\n",
      "99:\tlearn: 8.1464756\ttotal: 3.75s\tremaining: 33.7s\n",
      "100:\tlearn: 8.1421509\ttotal: 3.78s\tremaining: 33.7s\n",
      "101:\tlearn: 8.1374601\ttotal: 3.82s\tremaining: 33.6s\n",
      "102:\tlearn: 8.1345981\ttotal: 3.85s\tremaining: 33.5s\n",
      "103:\tlearn: 8.1301175\ttotal: 3.89s\tremaining: 33.5s\n",
      "104:\tlearn: 8.1273583\ttotal: 3.92s\tremaining: 33.4s\n",
      "105:\tlearn: 8.1244340\ttotal: 3.96s\tremaining: 33.4s\n",
      "106:\tlearn: 8.1208457\ttotal: 3.99s\tremaining: 33.3s\n",
      "107:\tlearn: 8.1175842\ttotal: 4.03s\tremaining: 33.3s\n",
      "108:\tlearn: 8.1130032\ttotal: 4.07s\tremaining: 33.2s\n",
      "109:\tlearn: 8.1098607\ttotal: 4.1s\tremaining: 33.2s\n",
      "110:\tlearn: 8.1066439\ttotal: 4.14s\tremaining: 33.1s\n",
      "111:\tlearn: 8.1026007\ttotal: 4.17s\tremaining: 33.1s\n",
      "112:\tlearn: 8.0990567\ttotal: 4.21s\tremaining: 33.1s\n",
      "113:\tlearn: 8.0955994\ttotal: 4.25s\tremaining: 33s\n",
      "114:\tlearn: 8.0919931\ttotal: 4.28s\tremaining: 33s\n",
      "115:\tlearn: 8.0880805\ttotal: 4.32s\tremaining: 32.9s\n",
      "116:\tlearn: 8.0833215\ttotal: 4.36s\tremaining: 32.9s\n",
      "117:\tlearn: 8.0795830\ttotal: 4.39s\tremaining: 32.8s\n",
      "118:\tlearn: 8.0760196\ttotal: 4.43s\tremaining: 32.8s\n",
      "119:\tlearn: 8.0726681\ttotal: 4.47s\tremaining: 32.8s\n",
      "120:\tlearn: 8.0692571\ttotal: 4.5s\tremaining: 32.7s\n",
      "121:\tlearn: 8.0661672\ttotal: 4.53s\tremaining: 32.6s\n",
      "122:\tlearn: 8.0629387\ttotal: 4.57s\tremaining: 32.6s\n",
      "123:\tlearn: 8.0601193\ttotal: 4.6s\tremaining: 32.5s\n",
      "124:\tlearn: 8.0563009\ttotal: 4.64s\tremaining: 32.5s\n",
      "125:\tlearn: 8.0542106\ttotal: 4.67s\tremaining: 32.4s\n",
      "126:\tlearn: 8.0506162\ttotal: 4.71s\tremaining: 32.4s\n",
      "127:\tlearn: 8.0470346\ttotal: 4.74s\tremaining: 32.3s\n",
      "128:\tlearn: 8.0448728\ttotal: 4.78s\tremaining: 32.3s\n",
      "129:\tlearn: 8.0408451\ttotal: 4.81s\tremaining: 32.2s\n",
      "130:\tlearn: 8.0366430\ttotal: 4.85s\tremaining: 32.2s\n",
      "131:\tlearn: 8.0336992\ttotal: 4.88s\tremaining: 32.1s\n",
      "132:\tlearn: 8.0306903\ttotal: 4.92s\tremaining: 32.1s\n",
      "133:\tlearn: 8.0271772\ttotal: 4.96s\tremaining: 32s\n",
      "134:\tlearn: 8.0228268\ttotal: 4.99s\tremaining: 32s\n",
      "135:\tlearn: 8.0193195\ttotal: 5.03s\tremaining: 31.9s\n",
      "136:\tlearn: 8.0159289\ttotal: 5.07s\tremaining: 31.9s\n",
      "137:\tlearn: 8.0125589\ttotal: 5.1s\tremaining: 31.9s\n",
      "138:\tlearn: 8.0082503\ttotal: 5.13s\tremaining: 31.8s\n",
      "139:\tlearn: 8.0049419\ttotal: 5.17s\tremaining: 31.8s\n",
      "140:\tlearn: 8.0006074\ttotal: 5.21s\tremaining: 31.7s\n",
      "141:\tlearn: 7.9979433\ttotal: 5.24s\tremaining: 31.7s\n",
      "142:\tlearn: 7.9934409\ttotal: 5.28s\tremaining: 31.6s\n",
      "143:\tlearn: 7.9901378\ttotal: 5.31s\tremaining: 31.6s\n",
      "144:\tlearn: 7.9867561\ttotal: 5.35s\tremaining: 31.5s\n",
      "145:\tlearn: 7.9837315\ttotal: 5.38s\tremaining: 31.5s\n",
      "146:\tlearn: 7.9793990\ttotal: 5.42s\tremaining: 31.4s\n",
      "147:\tlearn: 7.9754039\ttotal: 5.45s\tremaining: 31.4s\n",
      "148:\tlearn: 7.9710995\ttotal: 5.48s\tremaining: 31.3s\n",
      "149:\tlearn: 7.9673737\ttotal: 5.52s\tremaining: 31.3s\n",
      "150:\tlearn: 7.9637398\ttotal: 5.56s\tremaining: 31.2s\n",
      "151:\tlearn: 7.9598779\ttotal: 5.59s\tremaining: 31.2s\n",
      "152:\tlearn: 7.9575803\ttotal: 5.63s\tremaining: 31.2s\n",
      "153:\tlearn: 7.9535119\ttotal: 5.66s\tremaining: 31.1s\n",
      "154:\tlearn: 7.9493088\ttotal: 5.7s\tremaining: 31.1s\n",
      "155:\tlearn: 7.9458624\ttotal: 5.73s\tremaining: 31s\n",
      "156:\tlearn: 7.9420905\ttotal: 5.76s\tremaining: 31s\n",
      "157:\tlearn: 7.9375667\ttotal: 5.8s\tremaining: 30.9s\n",
      "158:\tlearn: 7.9333488\ttotal: 5.84s\tremaining: 30.9s\n",
      "159:\tlearn: 7.9297180\ttotal: 5.87s\tremaining: 30.8s\n",
      "160:\tlearn: 7.9255925\ttotal: 5.91s\tremaining: 30.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161:\tlearn: 7.9217818\ttotal: 5.96s\tremaining: 30.8s\n",
      "162:\tlearn: 7.9174213\ttotal: 5.99s\tremaining: 30.8s\n",
      "163:\tlearn: 7.9136945\ttotal: 6.03s\tremaining: 30.7s\n",
      "164:\tlearn: 7.9102096\ttotal: 6.06s\tremaining: 30.7s\n",
      "165:\tlearn: 7.9058266\ttotal: 6.1s\tremaining: 30.7s\n",
      "166:\tlearn: 7.9026569\ttotal: 6.15s\tremaining: 30.7s\n",
      "167:\tlearn: 7.8985856\ttotal: 6.18s\tremaining: 30.6s\n",
      "168:\tlearn: 7.8939678\ttotal: 6.22s\tremaining: 30.6s\n",
      "169:\tlearn: 7.8907048\ttotal: 6.25s\tremaining: 30.5s\n",
      "170:\tlearn: 7.8865442\ttotal: 6.29s\tremaining: 30.5s\n",
      "171:\tlearn: 7.8822719\ttotal: 6.33s\tremaining: 30.5s\n",
      "172:\tlearn: 7.8776767\ttotal: 6.36s\tremaining: 30.4s\n",
      "173:\tlearn: 7.8732853\ttotal: 6.4s\tremaining: 30.4s\n",
      "174:\tlearn: 7.8691805\ttotal: 6.43s\tremaining: 30.3s\n",
      "175:\tlearn: 7.8641992\ttotal: 6.47s\tremaining: 30.3s\n",
      "176:\tlearn: 7.8600134\ttotal: 6.51s\tremaining: 30.3s\n",
      "177:\tlearn: 7.8562704\ttotal: 6.55s\tremaining: 30.2s\n",
      "178:\tlearn: 7.8518626\ttotal: 6.58s\tremaining: 30.2s\n",
      "179:\tlearn: 7.8469228\ttotal: 6.63s\tremaining: 30.2s\n",
      "180:\tlearn: 7.8421104\ttotal: 6.67s\tremaining: 30.2s\n",
      "181:\tlearn: 7.8387574\ttotal: 6.71s\tremaining: 30.2s\n",
      "182:\tlearn: 7.8353000\ttotal: 6.74s\tremaining: 30.1s\n",
      "183:\tlearn: 7.8318553\ttotal: 6.78s\tremaining: 30.1s\n",
      "184:\tlearn: 7.8278047\ttotal: 6.82s\tremaining: 30s\n",
      "185:\tlearn: 7.8231785\ttotal: 6.85s\tremaining: 30s\n",
      "186:\tlearn: 7.8195429\ttotal: 6.89s\tremaining: 30s\n",
      "187:\tlearn: 7.8141496\ttotal: 6.92s\tremaining: 29.9s\n",
      "188:\tlearn: 7.8097314\ttotal: 6.96s\tremaining: 29.9s\n",
      "189:\tlearn: 7.8062846\ttotal: 7s\tremaining: 29.8s\n",
      "190:\tlearn: 7.8023532\ttotal: 7.06s\tremaining: 29.9s\n",
      "191:\tlearn: 7.7987247\ttotal: 7.1s\tremaining: 29.9s\n",
      "192:\tlearn: 7.7940503\ttotal: 7.14s\tremaining: 29.8s\n",
      "193:\tlearn: 7.7896375\ttotal: 7.17s\tremaining: 29.8s\n",
      "194:\tlearn: 7.7870186\ttotal: 7.21s\tremaining: 29.8s\n",
      "195:\tlearn: 7.7835514\ttotal: 7.24s\tremaining: 29.7s\n",
      "196:\tlearn: 7.7781570\ttotal: 7.28s\tremaining: 29.7s\n",
      "197:\tlearn: 7.7737668\ttotal: 7.32s\tremaining: 29.6s\n",
      "198:\tlearn: 7.7701085\ttotal: 7.35s\tremaining: 29.6s\n",
      "199:\tlearn: 7.7661481\ttotal: 7.38s\tremaining: 29.5s\n",
      "200:\tlearn: 7.7623863\ttotal: 7.42s\tremaining: 29.5s\n",
      "201:\tlearn: 7.7581051\ttotal: 7.46s\tremaining: 29.5s\n",
      "202:\tlearn: 7.7547385\ttotal: 7.49s\tremaining: 29.4s\n",
      "203:\tlearn: 7.7515972\ttotal: 7.52s\tremaining: 29.4s\n",
      "204:\tlearn: 7.7468067\ttotal: 7.56s\tremaining: 29.3s\n",
      "205:\tlearn: 7.7430194\ttotal: 7.6s\tremaining: 29.3s\n",
      "206:\tlearn: 7.7399535\ttotal: 7.64s\tremaining: 29.3s\n",
      "207:\tlearn: 7.7364315\ttotal: 7.68s\tremaining: 29.2s\n",
      "208:\tlearn: 7.7324928\ttotal: 7.72s\tremaining: 29.2s\n",
      "209:\tlearn: 7.7289563\ttotal: 7.75s\tremaining: 29.2s\n",
      "210:\tlearn: 7.7247522\ttotal: 7.79s\tremaining: 29.1s\n",
      "211:\tlearn: 7.7216622\ttotal: 7.82s\tremaining: 29.1s\n",
      "212:\tlearn: 7.7179455\ttotal: 7.86s\tremaining: 29s\n",
      "213:\tlearn: 7.7147288\ttotal: 7.89s\tremaining: 29s\n",
      "214:\tlearn: 7.7113425\ttotal: 7.93s\tremaining: 28.9s\n",
      "215:\tlearn: 7.7080245\ttotal: 7.97s\tremaining: 28.9s\n",
      "216:\tlearn: 7.7045409\ttotal: 8.01s\tremaining: 28.9s\n",
      "217:\tlearn: 7.7007891\ttotal: 8.05s\tremaining: 28.9s\n",
      "218:\tlearn: 7.6962257\ttotal: 8.09s\tremaining: 28.9s\n",
      "219:\tlearn: 7.6927233\ttotal: 8.13s\tremaining: 28.8s\n",
      "220:\tlearn: 7.6902025\ttotal: 8.16s\tremaining: 28.8s\n",
      "221:\tlearn: 7.6858242\ttotal: 8.2s\tremaining: 28.7s\n",
      "222:\tlearn: 7.6820415\ttotal: 8.23s\tremaining: 28.7s\n",
      "223:\tlearn: 7.6777869\ttotal: 8.27s\tremaining: 28.6s\n",
      "224:\tlearn: 7.6737756\ttotal: 8.3s\tremaining: 28.6s\n",
      "225:\tlearn: 7.6697549\ttotal: 8.34s\tremaining: 28.6s\n",
      "226:\tlearn: 7.6649508\ttotal: 8.38s\tremaining: 28.5s\n",
      "227:\tlearn: 7.6613749\ttotal: 8.41s\tremaining: 28.5s\n",
      "228:\tlearn: 7.6589485\ttotal: 8.45s\tremaining: 28.4s\n",
      "229:\tlearn: 7.6568959\ttotal: 8.5s\tremaining: 28.5s\n",
      "230:\tlearn: 7.6536731\ttotal: 8.54s\tremaining: 28.4s\n",
      "231:\tlearn: 7.6500768\ttotal: 8.57s\tremaining: 28.4s\n",
      "232:\tlearn: 7.6460595\ttotal: 8.61s\tremaining: 28.3s\n",
      "233:\tlearn: 7.6419415\ttotal: 8.67s\tremaining: 28.4s\n",
      "234:\tlearn: 7.6382685\ttotal: 8.7s\tremaining: 28.3s\n",
      "235:\tlearn: 7.6337531\ttotal: 8.75s\tremaining: 28.3s\n",
      "236:\tlearn: 7.6306138\ttotal: 8.79s\tremaining: 28.3s\n",
      "237:\tlearn: 7.6280255\ttotal: 8.82s\tremaining: 28.2s\n",
      "238:\tlearn: 7.6244661\ttotal: 8.86s\tremaining: 28.2s\n",
      "239:\tlearn: 7.6216636\ttotal: 8.89s\tremaining: 28.1s\n",
      "240:\tlearn: 7.6200167\ttotal: 8.92s\tremaining: 28.1s\n",
      "241:\tlearn: 7.6151911\ttotal: 8.96s\tremaining: 28.1s\n",
      "242:\tlearn: 7.6108325\ttotal: 9s\tremaining: 28s\n",
      "243:\tlearn: 7.6078863\ttotal: 9.03s\tremaining: 28s\n",
      "244:\tlearn: 7.6036557\ttotal: 9.06s\tremaining: 27.9s\n",
      "245:\tlearn: 7.6002785\ttotal: 9.1s\tremaining: 27.9s\n",
      "246:\tlearn: 7.5962774\ttotal: 9.14s\tremaining: 27.9s\n",
      "247:\tlearn: 7.5932216\ttotal: 9.17s\tremaining: 27.8s\n",
      "248:\tlearn: 7.5896450\ttotal: 9.21s\tremaining: 27.8s\n",
      "249:\tlearn: 7.5851919\ttotal: 9.24s\tremaining: 27.7s\n",
      "250:\tlearn: 7.5824741\ttotal: 9.28s\tremaining: 27.7s\n",
      "251:\tlearn: 7.5796976\ttotal: 9.31s\tremaining: 27.6s\n",
      "252:\tlearn: 7.5768166\ttotal: 9.35s\tremaining: 27.6s\n",
      "253:\tlearn: 7.5732010\ttotal: 9.39s\tremaining: 27.6s\n",
      "254:\tlearn: 7.5708341\ttotal: 9.42s\tremaining: 27.5s\n",
      "255:\tlearn: 7.5667788\ttotal: 9.46s\tremaining: 27.5s\n",
      "256:\tlearn: 7.5627123\ttotal: 9.49s\tremaining: 27.4s\n",
      "257:\tlearn: 7.5598502\ttotal: 9.53s\tremaining: 27.4s\n",
      "258:\tlearn: 7.5562429\ttotal: 9.56s\tremaining: 27.4s\n",
      "259:\tlearn: 7.5532796\ttotal: 9.6s\tremaining: 27.3s\n",
      "260:\tlearn: 7.5508345\ttotal: 9.63s\tremaining: 27.3s\n",
      "261:\tlearn: 7.5473822\ttotal: 9.67s\tremaining: 27.2s\n",
      "262:\tlearn: 7.5434762\ttotal: 9.71s\tremaining: 27.2s\n",
      "263:\tlearn: 7.5392145\ttotal: 9.74s\tremaining: 27.2s\n",
      "264:\tlearn: 7.5358676\ttotal: 9.78s\tremaining: 27.1s\n",
      "265:\tlearn: 7.5331566\ttotal: 9.82s\tremaining: 27.1s\n",
      "266:\tlearn: 7.5304436\ttotal: 9.86s\tremaining: 27.1s\n",
      "267:\tlearn: 7.5265757\ttotal: 9.89s\tremaining: 27s\n",
      "268:\tlearn: 7.5237759\ttotal: 9.93s\tremaining: 27s\n",
      "269:\tlearn: 7.5205731\ttotal: 9.97s\tremaining: 27s\n",
      "270:\tlearn: 7.5175170\ttotal: 10s\tremaining: 26.9s\n",
      "271:\tlearn: 7.5132448\ttotal: 10s\tremaining: 26.9s\n",
      "272:\tlearn: 7.5098715\ttotal: 10.1s\tremaining: 26.8s\n",
      "273:\tlearn: 7.5061611\ttotal: 10.1s\tremaining: 26.8s\n",
      "274:\tlearn: 7.5028671\ttotal: 10.2s\tremaining: 26.8s\n",
      "275:\tlearn: 7.4991918\ttotal: 10.2s\tremaining: 26.8s\n",
      "276:\tlearn: 7.4956160\ttotal: 10.2s\tremaining: 26.7s\n",
      "277:\tlearn: 7.4918114\ttotal: 10.3s\tremaining: 26.7s\n",
      "278:\tlearn: 7.4893778\ttotal: 10.3s\tremaining: 26.6s\n",
      "279:\tlearn: 7.4874621\ttotal: 10.3s\tremaining: 26.6s\n",
      "280:\tlearn: 7.4835160\ttotal: 10.4s\tremaining: 26.6s\n",
      "281:\tlearn: 7.4804315\ttotal: 10.4s\tremaining: 26.5s\n",
      "282:\tlearn: 7.4772341\ttotal: 10.5s\tremaining: 26.5s\n",
      "283:\tlearn: 7.4738084\ttotal: 10.5s\tremaining: 26.4s\n",
      "284:\tlearn: 7.4708893\ttotal: 10.5s\tremaining: 26.4s\n",
      "285:\tlearn: 7.4669866\ttotal: 10.6s\tremaining: 26.4s\n",
      "286:\tlearn: 7.4637390\ttotal: 10.6s\tremaining: 26.3s\n",
      "287:\tlearn: 7.4611713\ttotal: 10.6s\tremaining: 26.3s\n",
      "288:\tlearn: 7.4580262\ttotal: 10.7s\tremaining: 26.2s\n",
      "289:\tlearn: 7.4549020\ttotal: 10.7s\tremaining: 26.2s\n",
      "290:\tlearn: 7.4519481\ttotal: 10.7s\tremaining: 26.2s\n",
      "291:\tlearn: 7.4486596\ttotal: 10.8s\tremaining: 26.1s\n",
      "292:\tlearn: 7.4448909\ttotal: 10.8s\tremaining: 26.1s\n",
      "293:\tlearn: 7.4430817\ttotal: 10.8s\tremaining: 26.1s\n",
      "294:\tlearn: 7.4400007\ttotal: 10.9s\tremaining: 26s\n",
      "295:\tlearn: 7.4376031\ttotal: 10.9s\tremaining: 26s\n",
      "296:\tlearn: 7.4337183\ttotal: 11s\tremaining: 25.9s\n",
      "297:\tlearn: 7.4304752\ttotal: 11s\tremaining: 25.9s\n",
      "298:\tlearn: 7.4285657\ttotal: 11s\tremaining: 25.9s\n",
      "299:\tlearn: 7.4249765\ttotal: 11.1s\tremaining: 25.8s\n",
      "300:\tlearn: 7.4214926\ttotal: 11.1s\tremaining: 25.8s\n",
      "301:\tlearn: 7.4190140\ttotal: 11.1s\tremaining: 25.7s\n",
      "302:\tlearn: 7.4149958\ttotal: 11.2s\tremaining: 25.7s\n",
      "303:\tlearn: 7.4121071\ttotal: 11.2s\tremaining: 25.6s\n",
      "304:\tlearn: 7.4084107\ttotal: 11.2s\tremaining: 25.6s\n",
      "305:\tlearn: 7.4045920\ttotal: 11.3s\tremaining: 25.6s\n",
      "306:\tlearn: 7.4003343\ttotal: 11.3s\tremaining: 25.5s\n",
      "307:\tlearn: 7.3976150\ttotal: 11.3s\tremaining: 25.5s\n",
      "308:\tlearn: 7.3945297\ttotal: 11.4s\tremaining: 25.4s\n",
      "309:\tlearn: 7.3919726\ttotal: 11.4s\tremaining: 25.4s\n",
      "310:\tlearn: 7.3883700\ttotal: 11.4s\tremaining: 25.4s\n",
      "311:\tlearn: 7.3849380\ttotal: 11.5s\tremaining: 25.3s\n",
      "312:\tlearn: 7.3823370\ttotal: 11.5s\tremaining: 25.3s\n",
      "313:\tlearn: 7.3800128\ttotal: 11.6s\tremaining: 25.2s\n",
      "314:\tlearn: 7.3776078\ttotal: 11.6s\tremaining: 25.2s\n",
      "315:\tlearn: 7.3754391\ttotal: 11.6s\tremaining: 25.2s\n",
      "316:\tlearn: 7.3727598\ttotal: 11.7s\tremaining: 25.1s\n",
      "317:\tlearn: 7.3708816\ttotal: 11.7s\tremaining: 25.1s\n",
      "318:\tlearn: 7.3671892\ttotal: 11.7s\tremaining: 25s\n",
      "319:\tlearn: 7.3649064\ttotal: 11.8s\tremaining: 25s\n",
      "320:\tlearn: 7.3629668\ttotal: 11.8s\tremaining: 25s\n",
      "321:\tlearn: 7.3588325\ttotal: 11.8s\tremaining: 24.9s\n",
      "322:\tlearn: 7.3550184\ttotal: 11.9s\tremaining: 24.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323:\tlearn: 7.3515195\ttotal: 11.9s\tremaining: 24.9s\n",
      "324:\tlearn: 7.3478705\ttotal: 12s\tremaining: 24.8s\n",
      "325:\tlearn: 7.3442619\ttotal: 12s\tremaining: 24.8s\n",
      "326:\tlearn: 7.3419726\ttotal: 12s\tremaining: 24.8s\n",
      "327:\tlearn: 7.3389415\ttotal: 12.1s\tremaining: 24.7s\n",
      "328:\tlearn: 7.3358740\ttotal: 12.1s\tremaining: 24.7s\n",
      "329:\tlearn: 7.3340688\ttotal: 12.2s\tremaining: 24.7s\n",
      "330:\tlearn: 7.3310191\ttotal: 12.2s\tremaining: 24.7s\n",
      "331:\tlearn: 7.3277753\ttotal: 12.3s\tremaining: 24.7s\n",
      "332:\tlearn: 7.3245141\ttotal: 12.3s\tremaining: 24.6s\n",
      "333:\tlearn: 7.3207673\ttotal: 12.3s\tremaining: 24.6s\n",
      "334:\tlearn: 7.3180332\ttotal: 12.4s\tremaining: 24.5s\n",
      "335:\tlearn: 7.3156336\ttotal: 12.4s\tremaining: 24.5s\n",
      "336:\tlearn: 7.3131030\ttotal: 12.4s\tremaining: 24.5s\n",
      "337:\tlearn: 7.3096779\ttotal: 12.5s\tremaining: 24.4s\n",
      "338:\tlearn: 7.3057138\ttotal: 12.5s\tremaining: 24.4s\n",
      "339:\tlearn: 7.3027979\ttotal: 12.5s\tremaining: 24.3s\n",
      "340:\tlearn: 7.2995973\ttotal: 12.6s\tremaining: 24.3s\n",
      "341:\tlearn: 7.2971900\ttotal: 12.6s\tremaining: 24.3s\n",
      "342:\tlearn: 7.2943328\ttotal: 12.7s\tremaining: 24.2s\n",
      "343:\tlearn: 7.2917796\ttotal: 12.7s\tremaining: 24.2s\n",
      "344:\tlearn: 7.2886955\ttotal: 12.7s\tremaining: 24.2s\n",
      "345:\tlearn: 7.2856343\ttotal: 12.8s\tremaining: 24.1s\n",
      "346:\tlearn: 7.2824608\ttotal: 12.8s\tremaining: 24.1s\n",
      "347:\tlearn: 7.2792252\ttotal: 12.8s\tremaining: 24s\n",
      "348:\tlearn: 7.2765183\ttotal: 12.9s\tremaining: 24s\n",
      "349:\tlearn: 7.2734829\ttotal: 12.9s\tremaining: 24s\n",
      "350:\tlearn: 7.2707060\ttotal: 12.9s\tremaining: 23.9s\n",
      "351:\tlearn: 7.2679671\ttotal: 13s\tremaining: 23.9s\n",
      "352:\tlearn: 7.2654748\ttotal: 13s\tremaining: 23.8s\n",
      "353:\tlearn: 7.2617930\ttotal: 13s\tremaining: 23.8s\n",
      "354:\tlearn: 7.2586709\ttotal: 13.1s\tremaining: 23.8s\n",
      "355:\tlearn: 7.2561144\ttotal: 13.1s\tremaining: 23.7s\n",
      "356:\tlearn: 7.2536060\ttotal: 13.1s\tremaining: 23.7s\n",
      "357:\tlearn: 7.2502287\ttotal: 13.2s\tremaining: 23.6s\n",
      "358:\tlearn: 7.2476532\ttotal: 13.2s\tremaining: 23.6s\n",
      "359:\tlearn: 7.2447726\ttotal: 13.3s\tremaining: 23.6s\n",
      "360:\tlearn: 7.2424733\ttotal: 13.3s\tremaining: 23.6s\n",
      "361:\tlearn: 7.2392479\ttotal: 13.3s\tremaining: 23.5s\n",
      "362:\tlearn: 7.2354384\ttotal: 13.4s\tremaining: 23.5s\n",
      "363:\tlearn: 7.2338841\ttotal: 13.4s\tremaining: 23.4s\n",
      "364:\tlearn: 7.2302866\ttotal: 13.5s\tremaining: 23.4s\n",
      "365:\tlearn: 7.2279912\ttotal: 13.5s\tremaining: 23.4s\n",
      "366:\tlearn: 7.2246129\ttotal: 13.5s\tremaining: 23.4s\n",
      "367:\tlearn: 7.2218214\ttotal: 13.6s\tremaining: 23.3s\n",
      "368:\tlearn: 7.2179578\ttotal: 13.6s\tremaining: 23.3s\n",
      "369:\tlearn: 7.2159470\ttotal: 13.6s\tremaining: 23.2s\n",
      "370:\tlearn: 7.2125423\ttotal: 13.7s\tremaining: 23.2s\n",
      "371:\tlearn: 7.2099028\ttotal: 13.7s\tremaining: 23.2s\n",
      "372:\tlearn: 7.2069659\ttotal: 13.8s\tremaining: 23.1s\n",
      "373:\tlearn: 7.2039917\ttotal: 13.8s\tremaining: 23.1s\n",
      "374:\tlearn: 7.2016400\ttotal: 13.8s\tremaining: 23s\n",
      "375:\tlearn: 7.1985516\ttotal: 13.9s\tremaining: 23s\n",
      "376:\tlearn: 7.1950645\ttotal: 13.9s\tremaining: 23s\n",
      "377:\tlearn: 7.1918992\ttotal: 13.9s\tremaining: 22.9s\n",
      "378:\tlearn: 7.1879794\ttotal: 14s\tremaining: 22.9s\n",
      "379:\tlearn: 7.1851438\ttotal: 14s\tremaining: 22.8s\n",
      "380:\tlearn: 7.1833383\ttotal: 14s\tremaining: 22.8s\n",
      "381:\tlearn: 7.1806427\ttotal: 14.1s\tremaining: 22.8s\n",
      "382:\tlearn: 7.1773933\ttotal: 14.1s\tremaining: 22.7s\n",
      "383:\tlearn: 7.1748359\ttotal: 14.1s\tremaining: 22.7s\n",
      "384:\tlearn: 7.1724545\ttotal: 14.2s\tremaining: 22.6s\n",
      "385:\tlearn: 7.1693980\ttotal: 14.2s\tremaining: 22.6s\n",
      "386:\tlearn: 7.1656610\ttotal: 14.2s\tremaining: 22.6s\n",
      "387:\tlearn: 7.1625252\ttotal: 14.3s\tremaining: 22.5s\n",
      "388:\tlearn: 7.1606021\ttotal: 14.3s\tremaining: 22.5s\n",
      "389:\tlearn: 7.1584531\ttotal: 14.4s\tremaining: 22.5s\n",
      "390:\tlearn: 7.1562313\ttotal: 14.4s\tremaining: 22.4s\n",
      "391:\tlearn: 7.1532491\ttotal: 14.4s\tremaining: 22.4s\n",
      "392:\tlearn: 7.1513247\ttotal: 14.5s\tremaining: 22.4s\n",
      "393:\tlearn: 7.1482149\ttotal: 14.5s\tremaining: 22.4s\n",
      "394:\tlearn: 7.1459329\ttotal: 14.6s\tremaining: 22.3s\n",
      "395:\tlearn: 7.1425635\ttotal: 14.6s\tremaining: 22.3s\n",
      "396:\tlearn: 7.1399053\ttotal: 14.7s\tremaining: 22.3s\n",
      "397:\tlearn: 7.1373024\ttotal: 14.7s\tremaining: 22.2s\n",
      "398:\tlearn: 7.1337288\ttotal: 14.7s\tremaining: 22.2s\n",
      "399:\tlearn: 7.1296193\ttotal: 14.8s\tremaining: 22.1s\n",
      "400:\tlearn: 7.1272716\ttotal: 14.8s\tremaining: 22.1s\n",
      "401:\tlearn: 7.1249371\ttotal: 14.8s\tremaining: 22.1s\n",
      "402:\tlearn: 7.1222991\ttotal: 14.9s\tremaining: 22s\n",
      "403:\tlearn: 7.1189072\ttotal: 14.9s\tremaining: 22s\n",
      "404:\tlearn: 7.1151599\ttotal: 15s\tremaining: 22s\n",
      "405:\tlearn: 7.1126747\ttotal: 15s\tremaining: 22s\n",
      "406:\tlearn: 7.1100995\ttotal: 15.1s\tremaining: 22s\n",
      "407:\tlearn: 7.1069981\ttotal: 15.1s\tremaining: 21.9s\n",
      "408:\tlearn: 7.1044721\ttotal: 15.1s\tremaining: 21.9s\n",
      "409:\tlearn: 7.1019295\ttotal: 15.2s\tremaining: 21.8s\n",
      "410:\tlearn: 7.0997083\ttotal: 15.2s\tremaining: 21.8s\n",
      "411:\tlearn: 7.0973450\ttotal: 15.3s\tremaining: 21.8s\n",
      "412:\tlearn: 7.0947090\ttotal: 15.3s\tremaining: 21.7s\n",
      "413:\tlearn: 7.0917014\ttotal: 15.3s\tremaining: 21.7s\n",
      "414:\tlearn: 7.0888985\ttotal: 15.4s\tremaining: 21.7s\n",
      "415:\tlearn: 7.0863287\ttotal: 15.4s\tremaining: 21.6s\n",
      "416:\tlearn: 7.0851266\ttotal: 15.4s\tremaining: 21.6s\n",
      "417:\tlearn: 7.0829251\ttotal: 15.5s\tremaining: 21.6s\n",
      "418:\tlearn: 7.0808597\ttotal: 15.5s\tremaining: 21.5s\n",
      "419:\tlearn: 7.0785790\ttotal: 15.6s\tremaining: 21.5s\n",
      "420:\tlearn: 7.0753164\ttotal: 15.6s\tremaining: 21.4s\n",
      "421:\tlearn: 7.0729010\ttotal: 15.6s\tremaining: 21.4s\n",
      "422:\tlearn: 7.0707381\ttotal: 15.7s\tremaining: 21.4s\n",
      "423:\tlearn: 7.0690316\ttotal: 15.7s\tremaining: 21.3s\n",
      "424:\tlearn: 7.0658177\ttotal: 15.7s\tremaining: 21.3s\n",
      "425:\tlearn: 7.0641210\ttotal: 15.8s\tremaining: 21.3s\n",
      "426:\tlearn: 7.0621101\ttotal: 15.8s\tremaining: 21.2s\n",
      "427:\tlearn: 7.0607286\ttotal: 15.9s\tremaining: 21.2s\n",
      "428:\tlearn: 7.0578352\ttotal: 15.9s\tremaining: 21.2s\n",
      "429:\tlearn: 7.0544480\ttotal: 16s\tremaining: 21.2s\n",
      "430:\tlearn: 7.0520961\ttotal: 16s\tremaining: 21.1s\n",
      "431:\tlearn: 7.0501296\ttotal: 16s\tremaining: 21.1s\n",
      "432:\tlearn: 7.0475009\ttotal: 16.1s\tremaining: 21.1s\n",
      "433:\tlearn: 7.0456797\ttotal: 16.1s\tremaining: 21s\n",
      "434:\tlearn: 7.0429559\ttotal: 16.2s\tremaining: 21s\n",
      "435:\tlearn: 7.0402120\ttotal: 16.2s\tremaining: 21s\n",
      "436:\tlearn: 7.0379341\ttotal: 16.2s\tremaining: 20.9s\n",
      "437:\tlearn: 7.0356807\ttotal: 16.3s\tremaining: 20.9s\n",
      "438:\tlearn: 7.0334796\ttotal: 16.3s\tremaining: 20.9s\n",
      "439:\tlearn: 7.0299248\ttotal: 16.4s\tremaining: 20.8s\n",
      "440:\tlearn: 7.0277144\ttotal: 16.4s\tremaining: 20.8s\n",
      "441:\tlearn: 7.0254566\ttotal: 16.5s\tremaining: 20.8s\n",
      "442:\tlearn: 7.0224251\ttotal: 16.5s\tremaining: 20.7s\n",
      "443:\tlearn: 7.0192537\ttotal: 16.5s\tremaining: 20.7s\n",
      "444:\tlearn: 7.0173806\ttotal: 16.6s\tremaining: 20.7s\n",
      "445:\tlearn: 7.0138191\ttotal: 16.6s\tremaining: 20.6s\n",
      "446:\tlearn: 7.0116537\ttotal: 16.7s\tremaining: 20.6s\n",
      "447:\tlearn: 7.0086388\ttotal: 16.7s\tremaining: 20.6s\n",
      "448:\tlearn: 7.0052191\ttotal: 16.7s\tremaining: 20.5s\n",
      "449:\tlearn: 7.0022704\ttotal: 16.8s\tremaining: 20.5s\n",
      "450:\tlearn: 6.9999365\ttotal: 16.8s\tremaining: 20.5s\n",
      "451:\tlearn: 6.9969315\ttotal: 16.9s\tremaining: 20.4s\n",
      "452:\tlearn: 6.9946432\ttotal: 16.9s\tremaining: 20.4s\n",
      "453:\tlearn: 6.9928208\ttotal: 16.9s\tremaining: 20.4s\n",
      "454:\tlearn: 6.9898594\ttotal: 17s\tremaining: 20.4s\n",
      "455:\tlearn: 6.9884540\ttotal: 17s\tremaining: 20.3s\n",
      "456:\tlearn: 6.9855441\ttotal: 17.1s\tremaining: 20.3s\n",
      "457:\tlearn: 6.9841052\ttotal: 17.1s\tremaining: 20.2s\n",
      "458:\tlearn: 6.9836506\ttotal: 17.1s\tremaining: 20.2s\n",
      "459:\tlearn: 6.9815205\ttotal: 17.2s\tremaining: 20.2s\n",
      "460:\tlearn: 6.9789685\ttotal: 17.2s\tremaining: 20.1s\n",
      "461:\tlearn: 6.9763008\ttotal: 17.2s\tremaining: 20.1s\n",
      "462:\tlearn: 6.9742793\ttotal: 17.3s\tremaining: 20s\n",
      "463:\tlearn: 6.9720959\ttotal: 17.3s\tremaining: 20s\n",
      "464:\tlearn: 6.9702420\ttotal: 17.4s\tremaining: 20s\n",
      "465:\tlearn: 6.9673924\ttotal: 17.4s\tremaining: 19.9s\n",
      "466:\tlearn: 6.9644207\ttotal: 17.4s\tremaining: 19.9s\n",
      "467:\tlearn: 6.9626237\ttotal: 17.5s\tremaining: 19.9s\n",
      "468:\tlearn: 6.9602773\ttotal: 17.5s\tremaining: 19.8s\n",
      "469:\tlearn: 6.9580574\ttotal: 17.6s\tremaining: 19.8s\n",
      "470:\tlearn: 6.9553319\ttotal: 17.6s\tremaining: 19.8s\n",
      "471:\tlearn: 6.9530043\ttotal: 17.6s\tremaining: 19.7s\n",
      "472:\tlearn: 6.9499668\ttotal: 17.7s\tremaining: 19.7s\n",
      "473:\tlearn: 6.9483134\ttotal: 17.7s\tremaining: 19.6s\n",
      "474:\tlearn: 6.9460820\ttotal: 17.7s\tremaining: 19.6s\n",
      "475:\tlearn: 6.9429887\ttotal: 17.8s\tremaining: 19.6s\n",
      "476:\tlearn: 6.9404669\ttotal: 17.8s\tremaining: 19.5s\n",
      "477:\tlearn: 6.9382586\ttotal: 17.9s\tremaining: 19.5s\n",
      "478:\tlearn: 6.9344254\ttotal: 17.9s\tremaining: 19.5s\n",
      "479:\tlearn: 6.9324766\ttotal: 17.9s\tremaining: 19.4s\n",
      "480:\tlearn: 6.9306769\ttotal: 18s\tremaining: 19.4s\n",
      "481:\tlearn: 6.9284737\ttotal: 18s\tremaining: 19.3s\n",
      "482:\tlearn: 6.9265133\ttotal: 18s\tremaining: 19.3s\n",
      "483:\tlearn: 6.9238694\ttotal: 18.1s\tremaining: 19.3s\n",
      "484:\tlearn: 6.9208842\ttotal: 18.1s\tremaining: 19.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "485:\tlearn: 6.9180205\ttotal: 18.1s\tremaining: 19.2s\n",
      "486:\tlearn: 6.9160327\ttotal: 18.2s\tremaining: 19.2s\n",
      "487:\tlearn: 6.9128464\ttotal: 18.2s\tremaining: 19.1s\n",
      "488:\tlearn: 6.9098995\ttotal: 18.3s\tremaining: 19.1s\n",
      "489:\tlearn: 6.9075974\ttotal: 18.3s\tremaining: 19.1s\n",
      "490:\tlearn: 6.9060483\ttotal: 18.4s\tremaining: 19s\n",
      "491:\tlearn: 6.9031179\ttotal: 18.4s\tremaining: 19s\n",
      "492:\tlearn: 6.8995704\ttotal: 18.4s\tremaining: 19s\n",
      "493:\tlearn: 6.8973254\ttotal: 18.5s\tremaining: 18.9s\n",
      "494:\tlearn: 6.8953861\ttotal: 18.5s\tremaining: 18.9s\n",
      "495:\tlearn: 6.8929750\ttotal: 18.6s\tremaining: 18.9s\n",
      "496:\tlearn: 6.8906446\ttotal: 18.6s\tremaining: 18.8s\n",
      "497:\tlearn: 6.8881815\ttotal: 18.6s\tremaining: 18.8s\n",
      "498:\tlearn: 6.8857490\ttotal: 18.7s\tremaining: 18.7s\n",
      "499:\tlearn: 6.8834179\ttotal: 18.7s\tremaining: 18.7s\n",
      "500:\tlearn: 6.8817005\ttotal: 18.7s\tremaining: 18.7s\n",
      "501:\tlearn: 6.8788225\ttotal: 18.8s\tremaining: 18.6s\n",
      "502:\tlearn: 6.8763548\ttotal: 18.8s\tremaining: 18.6s\n",
      "503:\tlearn: 6.8741474\ttotal: 18.9s\tremaining: 18.6s\n",
      "504:\tlearn: 6.8711537\ttotal: 18.9s\tremaining: 18.5s\n",
      "505:\tlearn: 6.8687750\ttotal: 19s\tremaining: 18.5s\n",
      "506:\tlearn: 6.8668660\ttotal: 19s\tremaining: 18.5s\n",
      "507:\tlearn: 6.8647255\ttotal: 19s\tremaining: 18.4s\n",
      "508:\tlearn: 6.8615144\ttotal: 19.1s\tremaining: 18.4s\n",
      "509:\tlearn: 6.8592691\ttotal: 19.1s\tremaining: 18.4s\n",
      "510:\tlearn: 6.8575183\ttotal: 19.2s\tremaining: 18.4s\n",
      "511:\tlearn: 6.8559074\ttotal: 19.2s\tremaining: 18.3s\n",
      "512:\tlearn: 6.8535138\ttotal: 19.3s\tremaining: 18.3s\n",
      "513:\tlearn: 6.8511679\ttotal: 19.3s\tremaining: 18.3s\n",
      "514:\tlearn: 6.8481949\ttotal: 19.3s\tremaining: 18.2s\n",
      "515:\tlearn: 6.8450619\ttotal: 19.4s\tremaining: 18.2s\n",
      "516:\tlearn: 6.8428071\ttotal: 19.4s\tremaining: 18.1s\n",
      "517:\tlearn: 6.8406838\ttotal: 19.5s\tremaining: 18.1s\n",
      "518:\tlearn: 6.8374790\ttotal: 19.5s\tremaining: 18.1s\n",
      "519:\tlearn: 6.8355096\ttotal: 19.5s\tremaining: 18s\n",
      "520:\tlearn: 6.8325773\ttotal: 19.6s\tremaining: 18s\n",
      "521:\tlearn: 6.8293779\ttotal: 19.6s\tremaining: 18s\n",
      "522:\tlearn: 6.8271942\ttotal: 19.7s\tremaining: 17.9s\n",
      "523:\tlearn: 6.8247688\ttotal: 19.7s\tremaining: 17.9s\n",
      "524:\tlearn: 6.8220792\ttotal: 19.7s\tremaining: 17.9s\n",
      "525:\tlearn: 6.8195751\ttotal: 19.8s\tremaining: 17.8s\n",
      "526:\tlearn: 6.8173232\ttotal: 19.8s\tremaining: 17.8s\n",
      "527:\tlearn: 6.8170378\ttotal: 19.9s\tremaining: 17.8s\n",
      "528:\tlearn: 6.8148758\ttotal: 19.9s\tremaining: 17.7s\n",
      "529:\tlearn: 6.8129627\ttotal: 20s\tremaining: 17.7s\n",
      "530:\tlearn: 6.8105385\ttotal: 20s\tremaining: 17.7s\n",
      "531:\tlearn: 6.8084376\ttotal: 20s\tremaining: 17.6s\n",
      "532:\tlearn: 6.8060327\ttotal: 20.1s\tremaining: 17.6s\n",
      "533:\tlearn: 6.8033243\ttotal: 20.1s\tremaining: 17.6s\n",
      "534:\tlearn: 6.8004557\ttotal: 20.2s\tremaining: 17.5s\n",
      "535:\tlearn: 6.7978547\ttotal: 20.2s\tremaining: 17.5s\n",
      "536:\tlearn: 6.7944879\ttotal: 20.3s\tremaining: 17.5s\n",
      "537:\tlearn: 6.7912289\ttotal: 20.3s\tremaining: 17.4s\n",
      "538:\tlearn: 6.7879416\ttotal: 20.4s\tremaining: 17.4s\n",
      "539:\tlearn: 6.7849036\ttotal: 20.4s\tremaining: 17.4s\n",
      "540:\tlearn: 6.7817772\ttotal: 20.5s\tremaining: 17.4s\n",
      "541:\tlearn: 6.7794658\ttotal: 20.5s\tremaining: 17.3s\n",
      "542:\tlearn: 6.7769435\ttotal: 20.5s\tremaining: 17.3s\n",
      "543:\tlearn: 6.7752861\ttotal: 20.6s\tremaining: 17.2s\n",
      "544:\tlearn: 6.7723902\ttotal: 20.6s\tremaining: 17.2s\n",
      "545:\tlearn: 6.7690486\ttotal: 20.7s\tremaining: 17.2s\n",
      "546:\tlearn: 6.7659003\ttotal: 20.7s\tremaining: 17.1s\n",
      "547:\tlearn: 6.7635134\ttotal: 20.7s\tremaining: 17.1s\n",
      "548:\tlearn: 6.7610345\ttotal: 20.8s\tremaining: 17.1s\n",
      "549:\tlearn: 6.7576626\ttotal: 20.8s\tremaining: 17s\n",
      "550:\tlearn: 6.7558028\ttotal: 20.9s\tremaining: 17s\n",
      "551:\tlearn: 6.7527925\ttotal: 20.9s\tremaining: 17s\n",
      "552:\tlearn: 6.7496663\ttotal: 21s\tremaining: 17s\n",
      "553:\tlearn: 6.7467847\ttotal: 21s\tremaining: 16.9s\n",
      "554:\tlearn: 6.7441599\ttotal: 21.1s\tremaining: 16.9s\n",
      "555:\tlearn: 6.7439555\ttotal: 21.1s\tremaining: 16.8s\n",
      "556:\tlearn: 6.7408083\ttotal: 21.1s\tremaining: 16.8s\n",
      "557:\tlearn: 6.7382668\ttotal: 21.2s\tremaining: 16.8s\n",
      "558:\tlearn: 6.7364923\ttotal: 21.2s\tremaining: 16.7s\n",
      "559:\tlearn: 6.7344443\ttotal: 21.3s\tremaining: 16.7s\n",
      "560:\tlearn: 6.7323378\ttotal: 21.3s\tremaining: 16.7s\n",
      "561:\tlearn: 6.7302724\ttotal: 21.4s\tremaining: 16.6s\n",
      "562:\tlearn: 6.7282030\ttotal: 21.4s\tremaining: 16.6s\n",
      "563:\tlearn: 6.7262113\ttotal: 21.4s\tremaining: 16.6s\n",
      "564:\tlearn: 6.7238082\ttotal: 21.5s\tremaining: 16.5s\n",
      "565:\tlearn: 6.7212950\ttotal: 21.5s\tremaining: 16.5s\n",
      "566:\tlearn: 6.7195695\ttotal: 21.6s\tremaining: 16.5s\n",
      "567:\tlearn: 6.7171584\ttotal: 21.6s\tremaining: 16.4s\n",
      "568:\tlearn: 6.7150119\ttotal: 21.6s\tremaining: 16.4s\n",
      "569:\tlearn: 6.7130189\ttotal: 21.7s\tremaining: 16.4s\n",
      "570:\tlearn: 6.7104273\ttotal: 21.7s\tremaining: 16.3s\n",
      "571:\tlearn: 6.7086792\ttotal: 21.8s\tremaining: 16.3s\n",
      "572:\tlearn: 6.7054594\ttotal: 21.8s\tremaining: 16.2s\n",
      "573:\tlearn: 6.7026809\ttotal: 21.8s\tremaining: 16.2s\n",
      "574:\tlearn: 6.7010132\ttotal: 21.9s\tremaining: 16.2s\n",
      "575:\tlearn: 6.6988616\ttotal: 21.9s\tremaining: 16.1s\n",
      "576:\tlearn: 6.6965931\ttotal: 21.9s\tremaining: 16.1s\n",
      "577:\tlearn: 6.6935796\ttotal: 22s\tremaining: 16s\n",
      "578:\tlearn: 6.6912415\ttotal: 22s\tremaining: 16s\n",
      "579:\tlearn: 6.6880612\ttotal: 22s\tremaining: 16s\n",
      "580:\tlearn: 6.6847237\ttotal: 22.1s\tremaining: 15.9s\n",
      "581:\tlearn: 6.6824574\ttotal: 22.1s\tremaining: 15.9s\n",
      "582:\tlearn: 6.6795159\ttotal: 22.2s\tremaining: 15.9s\n",
      "583:\tlearn: 6.6775422\ttotal: 22.2s\tremaining: 15.8s\n",
      "584:\tlearn: 6.6754610\ttotal: 22.2s\tremaining: 15.8s\n",
      "585:\tlearn: 6.6733205\ttotal: 22.3s\tremaining: 15.7s\n",
      "586:\tlearn: 6.6711988\ttotal: 22.3s\tremaining: 15.7s\n",
      "587:\tlearn: 6.6689904\ttotal: 22.3s\tremaining: 15.7s\n",
      "588:\tlearn: 6.6675234\ttotal: 22.4s\tremaining: 15.6s\n",
      "589:\tlearn: 6.6658677\ttotal: 22.4s\tremaining: 15.6s\n",
      "590:\tlearn: 6.6633381\ttotal: 22.5s\tremaining: 15.5s\n",
      "591:\tlearn: 6.6610038\ttotal: 22.5s\tremaining: 15.5s\n",
      "592:\tlearn: 6.6591181\ttotal: 22.5s\tremaining: 15.5s\n",
      "593:\tlearn: 6.6566028\ttotal: 22.6s\tremaining: 15.4s\n",
      "594:\tlearn: 6.6538551\ttotal: 22.6s\tremaining: 15.4s\n",
      "595:\tlearn: 6.6518958\ttotal: 22.6s\tremaining: 15.3s\n",
      "596:\tlearn: 6.6498192\ttotal: 22.7s\tremaining: 15.3s\n",
      "597:\tlearn: 6.6479535\ttotal: 22.7s\tremaining: 15.3s\n",
      "598:\tlearn: 6.6447244\ttotal: 22.7s\tremaining: 15.2s\n",
      "599:\tlearn: 6.6422960\ttotal: 22.8s\tremaining: 15.2s\n",
      "600:\tlearn: 6.6406819\ttotal: 22.8s\tremaining: 15.1s\n",
      "601:\tlearn: 6.6386205\ttotal: 22.8s\tremaining: 15.1s\n",
      "602:\tlearn: 6.6369199\ttotal: 22.9s\tremaining: 15.1s\n",
      "603:\tlearn: 6.6345037\ttotal: 22.9s\tremaining: 15s\n",
      "604:\tlearn: 6.6326511\ttotal: 23s\tremaining: 15s\n",
      "605:\tlearn: 6.6297681\ttotal: 23s\tremaining: 15s\n",
      "606:\tlearn: 6.6269878\ttotal: 23s\tremaining: 14.9s\n",
      "607:\tlearn: 6.6234682\ttotal: 23.1s\tremaining: 14.9s\n",
      "608:\tlearn: 6.6219146\ttotal: 23.1s\tremaining: 14.8s\n",
      "609:\tlearn: 6.6198868\ttotal: 23.1s\tremaining: 14.8s\n",
      "610:\tlearn: 6.6177896\ttotal: 23.2s\tremaining: 14.8s\n",
      "611:\tlearn: 6.6176165\ttotal: 23.2s\tremaining: 14.7s\n",
      "612:\tlearn: 6.6158505\ttotal: 23.3s\tremaining: 14.7s\n",
      "613:\tlearn: 6.6139763\ttotal: 23.3s\tremaining: 14.6s\n",
      "614:\tlearn: 6.6119250\ttotal: 23.3s\tremaining: 14.6s\n",
      "615:\tlearn: 6.6102885\ttotal: 23.4s\tremaining: 14.6s\n",
      "616:\tlearn: 6.6076398\ttotal: 23.4s\tremaining: 14.5s\n",
      "617:\tlearn: 6.6058348\ttotal: 23.4s\tremaining: 14.5s\n",
      "618:\tlearn: 6.6039706\ttotal: 23.5s\tremaining: 14.4s\n",
      "619:\tlearn: 6.6023530\ttotal: 23.5s\tremaining: 14.4s\n",
      "620:\tlearn: 6.5997120\ttotal: 23.5s\tremaining: 14.4s\n",
      "621:\tlearn: 6.5983260\ttotal: 23.6s\tremaining: 14.3s\n",
      "622:\tlearn: 6.5961958\ttotal: 23.6s\tremaining: 14.3s\n",
      "623:\tlearn: 6.5944035\ttotal: 23.6s\tremaining: 14.2s\n",
      "624:\tlearn: 6.5922178\ttotal: 23.7s\tremaining: 14.2s\n",
      "625:\tlearn: 6.5892767\ttotal: 23.7s\tremaining: 14.2s\n",
      "626:\tlearn: 6.5873507\ttotal: 23.7s\tremaining: 14.1s\n",
      "627:\tlearn: 6.5856201\ttotal: 23.8s\tremaining: 14.1s\n",
      "628:\tlearn: 6.5834058\ttotal: 23.8s\tremaining: 14s\n",
      "629:\tlearn: 6.5814795\ttotal: 23.8s\tremaining: 14s\n",
      "630:\tlearn: 6.5791876\ttotal: 23.9s\tremaining: 14s\n",
      "631:\tlearn: 6.5774625\ttotal: 23.9s\tremaining: 13.9s\n",
      "632:\tlearn: 6.5748439\ttotal: 24s\tremaining: 13.9s\n",
      "633:\tlearn: 6.5725311\ttotal: 24s\tremaining: 13.8s\n",
      "634:\tlearn: 6.5705002\ttotal: 24s\tremaining: 13.8s\n",
      "635:\tlearn: 6.5672042\ttotal: 24.1s\tremaining: 13.8s\n",
      "636:\tlearn: 6.5648211\ttotal: 24.1s\tremaining: 13.7s\n",
      "637:\tlearn: 6.5624672\ttotal: 24.1s\tremaining: 13.7s\n",
      "638:\tlearn: 6.5605816\ttotal: 24.2s\tremaining: 13.6s\n",
      "639:\tlearn: 6.5587486\ttotal: 24.2s\tremaining: 13.6s\n",
      "640:\tlearn: 6.5559600\ttotal: 24.2s\tremaining: 13.6s\n",
      "641:\tlearn: 6.5541419\ttotal: 24.3s\tremaining: 13.5s\n",
      "642:\tlearn: 6.5518314\ttotal: 24.3s\tremaining: 13.5s\n",
      "643:\tlearn: 6.5490106\ttotal: 24.3s\tremaining: 13.5s\n",
      "644:\tlearn: 6.5462492\ttotal: 24.4s\tremaining: 13.4s\n",
      "645:\tlearn: 6.5442304\ttotal: 24.4s\tremaining: 13.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "646:\tlearn: 6.5420461\ttotal: 24.4s\tremaining: 13.3s\n",
      "647:\tlearn: 6.5397508\ttotal: 24.5s\tremaining: 13.3s\n",
      "648:\tlearn: 6.5367646\ttotal: 24.5s\tremaining: 13.3s\n",
      "649:\tlearn: 6.5345431\ttotal: 24.5s\tremaining: 13.2s\n",
      "650:\tlearn: 6.5324312\ttotal: 24.6s\tremaining: 13.2s\n",
      "651:\tlearn: 6.5311748\ttotal: 24.6s\tremaining: 13.1s\n",
      "652:\tlearn: 6.5294871\ttotal: 24.6s\tremaining: 13.1s\n",
      "653:\tlearn: 6.5281000\ttotal: 24.7s\tremaining: 13.1s\n",
      "654:\tlearn: 6.5258460\ttotal: 24.7s\tremaining: 13s\n",
      "655:\tlearn: 6.5232412\ttotal: 24.8s\tremaining: 13s\n",
      "656:\tlearn: 6.5207997\ttotal: 24.8s\tremaining: 12.9s\n",
      "657:\tlearn: 6.5189840\ttotal: 24.8s\tremaining: 12.9s\n",
      "658:\tlearn: 6.5167640\ttotal: 24.9s\tremaining: 12.9s\n",
      "659:\tlearn: 6.5149946\ttotal: 24.9s\tremaining: 12.8s\n",
      "660:\tlearn: 6.5125814\ttotal: 24.9s\tremaining: 12.8s\n",
      "661:\tlearn: 6.5110183\ttotal: 25s\tremaining: 12.8s\n",
      "662:\tlearn: 6.5092130\ttotal: 25s\tremaining: 12.7s\n",
      "663:\tlearn: 6.5077652\ttotal: 25.1s\tremaining: 12.7s\n",
      "664:\tlearn: 6.5052688\ttotal: 25.1s\tremaining: 12.6s\n",
      "665:\tlearn: 6.5028375\ttotal: 25.1s\tremaining: 12.6s\n",
      "666:\tlearn: 6.4998016\ttotal: 25.2s\tremaining: 12.6s\n",
      "667:\tlearn: 6.4975127\ttotal: 25.2s\tremaining: 12.5s\n",
      "668:\tlearn: 6.4947715\ttotal: 25.3s\tremaining: 12.5s\n",
      "669:\tlearn: 6.4919169\ttotal: 25.3s\tremaining: 12.5s\n",
      "670:\tlearn: 6.4900166\ttotal: 25.3s\tremaining: 12.4s\n",
      "671:\tlearn: 6.4875379\ttotal: 25.4s\tremaining: 12.4s\n",
      "672:\tlearn: 6.4845608\ttotal: 25.4s\tremaining: 12.3s\n",
      "673:\tlearn: 6.4824340\ttotal: 25.4s\tremaining: 12.3s\n",
      "674:\tlearn: 6.4809269\ttotal: 25.5s\tremaining: 12.3s\n",
      "675:\tlearn: 6.4807845\ttotal: 25.5s\tremaining: 12.2s\n",
      "676:\tlearn: 6.4783383\ttotal: 25.5s\tremaining: 12.2s\n",
      "677:\tlearn: 6.4762136\ttotal: 25.6s\tremaining: 12.2s\n",
      "678:\tlearn: 6.4737596\ttotal: 25.6s\tremaining: 12.1s\n",
      "679:\tlearn: 6.4721333\ttotal: 25.7s\tremaining: 12.1s\n",
      "680:\tlearn: 6.4696615\ttotal: 25.7s\tremaining: 12s\n",
      "681:\tlearn: 6.4679971\ttotal: 25.7s\tremaining: 12s\n",
      "682:\tlearn: 6.4647463\ttotal: 25.8s\tremaining: 12s\n",
      "683:\tlearn: 6.4620963\ttotal: 25.8s\tremaining: 11.9s\n",
      "684:\tlearn: 6.4604928\ttotal: 25.8s\tremaining: 11.9s\n",
      "685:\tlearn: 6.4581256\ttotal: 25.9s\tremaining: 11.8s\n",
      "686:\tlearn: 6.4553655\ttotal: 25.9s\tremaining: 11.8s\n",
      "687:\tlearn: 6.4531259\ttotal: 26s\tremaining: 11.8s\n",
      "688:\tlearn: 6.4519066\ttotal: 26s\tremaining: 11.7s\n",
      "689:\tlearn: 6.4517317\ttotal: 26s\tremaining: 11.7s\n",
      "690:\tlearn: 6.4490439\ttotal: 26.1s\tremaining: 11.7s\n",
      "691:\tlearn: 6.4466694\ttotal: 26.1s\tremaining: 11.6s\n",
      "692:\tlearn: 6.4445314\ttotal: 26.1s\tremaining: 11.6s\n",
      "693:\tlearn: 6.4410570\ttotal: 26.2s\tremaining: 11.6s\n",
      "694:\tlearn: 6.4391774\ttotal: 26.2s\tremaining: 11.5s\n",
      "695:\tlearn: 6.4360860\ttotal: 26.3s\tremaining: 11.5s\n",
      "696:\tlearn: 6.4331239\ttotal: 26.3s\tremaining: 11.4s\n",
      "697:\tlearn: 6.4317984\ttotal: 26.4s\tremaining: 11.4s\n",
      "698:\tlearn: 6.4297017\ttotal: 26.4s\tremaining: 11.4s\n",
      "699:\tlearn: 6.4276299\ttotal: 26.4s\tremaining: 11.3s\n",
      "700:\tlearn: 6.4253506\ttotal: 26.5s\tremaining: 11.3s\n",
      "701:\tlearn: 6.4224391\ttotal: 26.5s\tremaining: 11.3s\n",
      "702:\tlearn: 6.4203264\ttotal: 26.6s\tremaining: 11.2s\n",
      "703:\tlearn: 6.4176966\ttotal: 26.6s\tremaining: 11.2s\n",
      "704:\tlearn: 6.4156032\ttotal: 26.6s\tremaining: 11.1s\n",
      "705:\tlearn: 6.4141474\ttotal: 26.7s\tremaining: 11.1s\n",
      "706:\tlearn: 6.4117863\ttotal: 26.7s\tremaining: 11.1s\n",
      "707:\tlearn: 6.4099586\ttotal: 26.8s\tremaining: 11s\n",
      "708:\tlearn: 6.4077277\ttotal: 26.8s\tremaining: 11s\n",
      "709:\tlearn: 6.4055303\ttotal: 26.9s\tremaining: 11s\n",
      "710:\tlearn: 6.4033221\ttotal: 26.9s\tremaining: 10.9s\n",
      "711:\tlearn: 6.4015384\ttotal: 26.9s\tremaining: 10.9s\n",
      "712:\tlearn: 6.3994446\ttotal: 27s\tremaining: 10.9s\n",
      "713:\tlearn: 6.3966899\ttotal: 27s\tremaining: 10.8s\n",
      "714:\tlearn: 6.3942705\ttotal: 27.1s\tremaining: 10.8s\n",
      "715:\tlearn: 6.3927132\ttotal: 27.1s\tremaining: 10.7s\n",
      "716:\tlearn: 6.3918662\ttotal: 27.1s\tremaining: 10.7s\n",
      "717:\tlearn: 6.3897080\ttotal: 27.2s\tremaining: 10.7s\n",
      "718:\tlearn: 6.3871065\ttotal: 27.2s\tremaining: 10.6s\n",
      "719:\tlearn: 6.3849788\ttotal: 27.3s\tremaining: 10.6s\n",
      "720:\tlearn: 6.3834234\ttotal: 27.3s\tremaining: 10.6s\n",
      "721:\tlearn: 6.3811351\ttotal: 27.3s\tremaining: 10.5s\n",
      "722:\tlearn: 6.3798409\ttotal: 27.4s\tremaining: 10.5s\n",
      "723:\tlearn: 6.3774582\ttotal: 27.4s\tremaining: 10.5s\n",
      "724:\tlearn: 6.3747311\ttotal: 27.5s\tremaining: 10.4s\n",
      "725:\tlearn: 6.3716905\ttotal: 27.5s\tremaining: 10.4s\n",
      "726:\tlearn: 6.3708425\ttotal: 27.5s\tremaining: 10.3s\n",
      "727:\tlearn: 6.3683175\ttotal: 27.6s\tremaining: 10.3s\n",
      "728:\tlearn: 6.3669797\ttotal: 27.6s\tremaining: 10.3s\n",
      "729:\tlearn: 6.3650374\ttotal: 27.6s\tremaining: 10.2s\n",
      "730:\tlearn: 6.3626497\ttotal: 27.7s\tremaining: 10.2s\n",
      "731:\tlearn: 6.3605046\ttotal: 27.7s\tremaining: 10.2s\n",
      "732:\tlearn: 6.3592660\ttotal: 27.8s\tremaining: 10.1s\n",
      "733:\tlearn: 6.3574451\ttotal: 27.8s\tremaining: 10.1s\n",
      "734:\tlearn: 6.3552124\ttotal: 27.9s\tremaining: 10s\n",
      "735:\tlearn: 6.3535065\ttotal: 27.9s\tremaining: 10s\n",
      "736:\tlearn: 6.3512492\ttotal: 28s\tremaining: 9.97s\n",
      "737:\tlearn: 6.3484723\ttotal: 28s\tremaining: 9.94s\n",
      "738:\tlearn: 6.3462246\ttotal: 28s\tremaining: 9.9s\n",
      "739:\tlearn: 6.3437744\ttotal: 28.1s\tremaining: 9.86s\n",
      "740:\tlearn: 6.3411213\ttotal: 28.1s\tremaining: 9.82s\n",
      "741:\tlearn: 6.3383325\ttotal: 28.1s\tremaining: 9.79s\n",
      "742:\tlearn: 6.3362650\ttotal: 28.2s\tremaining: 9.75s\n",
      "743:\tlearn: 6.3336761\ttotal: 28.2s\tremaining: 9.71s\n",
      "744:\tlearn: 6.3315294\ttotal: 28.3s\tremaining: 9.67s\n",
      "745:\tlearn: 6.3294915\ttotal: 28.3s\tremaining: 9.63s\n",
      "746:\tlearn: 6.3279550\ttotal: 28.3s\tremaining: 9.6s\n",
      "747:\tlearn: 6.3256912\ttotal: 28.4s\tremaining: 9.56s\n",
      "748:\tlearn: 6.3240788\ttotal: 28.4s\tremaining: 9.52s\n",
      "749:\tlearn: 6.3222094\ttotal: 28.5s\tremaining: 9.49s\n",
      "750:\tlearn: 6.3202727\ttotal: 28.5s\tremaining: 9.45s\n",
      "751:\tlearn: 6.3188864\ttotal: 28.5s\tremaining: 9.41s\n",
      "752:\tlearn: 6.3164994\ttotal: 28.6s\tremaining: 9.38s\n",
      "753:\tlearn: 6.3147273\ttotal: 28.6s\tremaining: 9.34s\n",
      "754:\tlearn: 6.3126720\ttotal: 28.7s\tremaining: 9.3s\n",
      "755:\tlearn: 6.3111308\ttotal: 28.7s\tremaining: 9.26s\n",
      "756:\tlearn: 6.3075645\ttotal: 28.7s\tremaining: 9.23s\n",
      "757:\tlearn: 6.3059793\ttotal: 28.8s\tremaining: 9.19s\n",
      "758:\tlearn: 6.3038689\ttotal: 28.8s\tremaining: 9.15s\n",
      "759:\tlearn: 6.3012967\ttotal: 28.9s\tremaining: 9.12s\n",
      "760:\tlearn: 6.2995994\ttotal: 28.9s\tremaining: 9.09s\n",
      "761:\tlearn: 6.2980350\ttotal: 29s\tremaining: 9.05s\n",
      "762:\tlearn: 6.2955558\ttotal: 29s\tremaining: 9.02s\n",
      "763:\tlearn: 6.2937594\ttotal: 29.1s\tremaining: 8.98s\n",
      "764:\tlearn: 6.2928304\ttotal: 29.1s\tremaining: 8.94s\n",
      "765:\tlearn: 6.2912048\ttotal: 29.2s\tremaining: 8.91s\n",
      "766:\tlearn: 6.2882040\ttotal: 29.2s\tremaining: 8.87s\n",
      "767:\tlearn: 6.2857850\ttotal: 29.3s\tremaining: 8.84s\n",
      "768:\tlearn: 6.2833989\ttotal: 29.3s\tremaining: 8.81s\n",
      "769:\tlearn: 6.2817997\ttotal: 29.4s\tremaining: 8.77s\n",
      "770:\tlearn: 6.2799994\ttotal: 29.4s\tremaining: 8.74s\n",
      "771:\tlearn: 6.2768419\ttotal: 29.5s\tremaining: 8.7s\n",
      "772:\tlearn: 6.2749740\ttotal: 29.5s\tremaining: 8.67s\n",
      "773:\tlearn: 6.2732325\ttotal: 29.6s\tremaining: 8.63s\n",
      "774:\tlearn: 6.2714841\ttotal: 29.6s\tremaining: 8.6s\n",
      "775:\tlearn: 6.2692279\ttotal: 29.6s\tremaining: 8.56s\n",
      "776:\tlearn: 6.2667346\ttotal: 29.7s\tremaining: 8.53s\n",
      "777:\tlearn: 6.2649309\ttotal: 29.8s\tremaining: 8.49s\n",
      "778:\tlearn: 6.2625918\ttotal: 29.8s\tremaining: 8.46s\n",
      "779:\tlearn: 6.2604983\ttotal: 29.9s\tremaining: 8.43s\n",
      "780:\tlearn: 6.2574526\ttotal: 29.9s\tremaining: 8.39s\n",
      "781:\tlearn: 6.2550226\ttotal: 30s\tremaining: 8.35s\n",
      "782:\tlearn: 6.2531280\ttotal: 30s\tremaining: 8.31s\n",
      "783:\tlearn: 6.2507170\ttotal: 30s\tremaining: 8.28s\n",
      "784:\tlearn: 6.2492977\ttotal: 30.1s\tremaining: 8.24s\n",
      "785:\tlearn: 6.2472955\ttotal: 30.1s\tremaining: 8.2s\n",
      "786:\tlearn: 6.2444691\ttotal: 30.2s\tremaining: 8.16s\n",
      "787:\tlearn: 6.2429446\ttotal: 30.2s\tremaining: 8.14s\n",
      "788:\tlearn: 6.2415549\ttotal: 30.3s\tremaining: 8.1s\n",
      "789:\tlearn: 6.2388218\ttotal: 30.3s\tremaining: 8.06s\n",
      "790:\tlearn: 6.2368486\ttotal: 30.4s\tremaining: 8.02s\n",
      "791:\tlearn: 6.2352613\ttotal: 30.4s\tremaining: 7.98s\n",
      "792:\tlearn: 6.2340844\ttotal: 30.4s\tremaining: 7.95s\n",
      "793:\tlearn: 6.2307649\ttotal: 30.5s\tremaining: 7.91s\n",
      "794:\tlearn: 6.2287835\ttotal: 30.5s\tremaining: 7.87s\n",
      "795:\tlearn: 6.2262507\ttotal: 30.6s\tremaining: 7.83s\n",
      "796:\tlearn: 6.2260309\ttotal: 30.6s\tremaining: 7.8s\n",
      "797:\tlearn: 6.2247801\ttotal: 30.7s\tremaining: 7.76s\n",
      "798:\tlearn: 6.2218560\ttotal: 30.7s\tremaining: 7.72s\n",
      "799:\tlearn: 6.2195689\ttotal: 30.7s\tremaining: 7.69s\n",
      "800:\tlearn: 6.2174699\ttotal: 30.8s\tremaining: 7.65s\n",
      "801:\tlearn: 6.2155360\ttotal: 30.8s\tremaining: 7.61s\n",
      "802:\tlearn: 6.2138558\ttotal: 30.9s\tremaining: 7.57s\n",
      "803:\tlearn: 6.2117542\ttotal: 30.9s\tremaining: 7.54s\n",
      "804:\tlearn: 6.2106408\ttotal: 31s\tremaining: 7.5s\n",
      "805:\tlearn: 6.2088389\ttotal: 31s\tremaining: 7.46s\n",
      "806:\tlearn: 6.2065531\ttotal: 31s\tremaining: 7.42s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "807:\tlearn: 6.2042986\ttotal: 31.1s\tremaining: 7.39s\n",
      "808:\tlearn: 6.2029520\ttotal: 31.1s\tremaining: 7.35s\n",
      "809:\tlearn: 6.2006842\ttotal: 31.2s\tremaining: 7.31s\n",
      "810:\tlearn: 6.1986974\ttotal: 31.2s\tremaining: 7.27s\n",
      "811:\tlearn: 6.1969388\ttotal: 31.2s\tremaining: 7.24s\n",
      "812:\tlearn: 6.1944428\ttotal: 31.3s\tremaining: 7.2s\n",
      "813:\tlearn: 6.1926126\ttotal: 31.3s\tremaining: 7.16s\n",
      "814:\tlearn: 6.1903083\ttotal: 31.4s\tremaining: 7.12s\n",
      "815:\tlearn: 6.1882675\ttotal: 31.4s\tremaining: 7.08s\n",
      "816:\tlearn: 6.1880705\ttotal: 31.5s\tremaining: 7.04s\n",
      "817:\tlearn: 6.1857722\ttotal: 31.5s\tremaining: 7.01s\n",
      "818:\tlearn: 6.1830277\ttotal: 31.5s\tremaining: 6.97s\n",
      "819:\tlearn: 6.1808802\ttotal: 31.6s\tremaining: 6.93s\n",
      "820:\tlearn: 6.1789195\ttotal: 31.6s\tremaining: 6.89s\n",
      "821:\tlearn: 6.1769341\ttotal: 31.7s\tremaining: 6.85s\n",
      "822:\tlearn: 6.1753878\ttotal: 31.7s\tremaining: 6.82s\n",
      "823:\tlearn: 6.1728715\ttotal: 31.7s\tremaining: 6.78s\n",
      "824:\tlearn: 6.1703962\ttotal: 31.8s\tremaining: 6.74s\n",
      "825:\tlearn: 6.1668210\ttotal: 31.8s\tremaining: 6.7s\n",
      "826:\tlearn: 6.1650395\ttotal: 31.8s\tremaining: 6.66s\n",
      "827:\tlearn: 6.1634811\ttotal: 31.9s\tremaining: 6.62s\n",
      "828:\tlearn: 6.1611961\ttotal: 31.9s\tremaining: 6.59s\n",
      "829:\tlearn: 6.1589749\ttotal: 32s\tremaining: 6.55s\n",
      "830:\tlearn: 6.1570355\ttotal: 32s\tremaining: 6.51s\n",
      "831:\tlearn: 6.1554994\ttotal: 32s\tremaining: 6.47s\n",
      "832:\tlearn: 6.1530828\ttotal: 32.1s\tremaining: 6.43s\n",
      "833:\tlearn: 6.1505624\ttotal: 32.1s\tremaining: 6.39s\n",
      "834:\tlearn: 6.1485265\ttotal: 32.1s\tremaining: 6.35s\n",
      "835:\tlearn: 6.1465393\ttotal: 32.2s\tremaining: 6.31s\n",
      "836:\tlearn: 6.1448902\ttotal: 32.2s\tremaining: 6.28s\n",
      "837:\tlearn: 6.1424944\ttotal: 32.3s\tremaining: 6.24s\n",
      "838:\tlearn: 6.1394024\ttotal: 32.3s\tremaining: 6.2s\n",
      "839:\tlearn: 6.1382676\ttotal: 32.3s\tremaining: 6.16s\n",
      "840:\tlearn: 6.1363953\ttotal: 32.4s\tremaining: 6.12s\n",
      "841:\tlearn: 6.1340277\ttotal: 32.4s\tremaining: 6.08s\n",
      "842:\tlearn: 6.1317910\ttotal: 32.5s\tremaining: 6.04s\n",
      "843:\tlearn: 6.1297049\ttotal: 32.5s\tremaining: 6s\n",
      "844:\tlearn: 6.1273092\ttotal: 32.5s\tremaining: 5.97s\n",
      "845:\tlearn: 6.1251206\ttotal: 32.6s\tremaining: 5.93s\n",
      "846:\tlearn: 6.1231375\ttotal: 32.6s\tremaining: 5.89s\n",
      "847:\tlearn: 6.1212221\ttotal: 32.6s\tremaining: 5.85s\n",
      "848:\tlearn: 6.1184893\ttotal: 32.7s\tremaining: 5.81s\n",
      "849:\tlearn: 6.1163225\ttotal: 32.7s\tremaining: 5.78s\n",
      "850:\tlearn: 6.1134715\ttotal: 32.8s\tremaining: 5.74s\n",
      "851:\tlearn: 6.1113077\ttotal: 32.8s\tremaining: 5.7s\n",
      "852:\tlearn: 6.1100795\ttotal: 32.9s\tremaining: 5.66s\n",
      "853:\tlearn: 6.1079641\ttotal: 32.9s\tremaining: 5.62s\n",
      "854:\tlearn: 6.1063214\ttotal: 33s\tremaining: 5.59s\n",
      "855:\tlearn: 6.1045602\ttotal: 33s\tremaining: 5.55s\n",
      "856:\tlearn: 6.1025256\ttotal: 33s\tremaining: 5.51s\n",
      "857:\tlearn: 6.1010463\ttotal: 33.1s\tremaining: 5.47s\n",
      "858:\tlearn: 6.0984874\ttotal: 33.1s\tremaining: 5.43s\n",
      "859:\tlearn: 6.0963883\ttotal: 33.1s\tremaining: 5.39s\n",
      "860:\tlearn: 6.0938707\ttotal: 33.2s\tremaining: 5.36s\n",
      "861:\tlearn: 6.0926393\ttotal: 33.2s\tremaining: 5.32s\n",
      "862:\tlearn: 6.0903923\ttotal: 33.2s\tremaining: 5.28s\n",
      "863:\tlearn: 6.0899278\ttotal: 33.3s\tremaining: 5.24s\n",
      "864:\tlearn: 6.0876237\ttotal: 33.3s\tremaining: 5.2s\n",
      "865:\tlearn: 6.0850693\ttotal: 33.3s\tremaining: 5.16s\n",
      "866:\tlearn: 6.0833273\ttotal: 33.4s\tremaining: 5.12s\n",
      "867:\tlearn: 6.0813720\ttotal: 33.4s\tremaining: 5.08s\n",
      "868:\tlearn: 6.0797454\ttotal: 33.5s\tremaining: 5.04s\n",
      "869:\tlearn: 6.0770917\ttotal: 33.5s\tremaining: 5s\n",
      "870:\tlearn: 6.0756954\ttotal: 33.5s\tremaining: 4.96s\n",
      "871:\tlearn: 6.0734855\ttotal: 33.6s\tremaining: 4.92s\n",
      "872:\tlearn: 6.0721994\ttotal: 33.6s\tremaining: 4.89s\n",
      "873:\tlearn: 6.0703408\ttotal: 33.6s\tremaining: 4.85s\n",
      "874:\tlearn: 6.0689574\ttotal: 33.7s\tremaining: 4.81s\n",
      "875:\tlearn: 6.0663365\ttotal: 33.7s\tremaining: 4.77s\n",
      "876:\tlearn: 6.0637518\ttotal: 33.7s\tremaining: 4.73s\n",
      "877:\tlearn: 6.0616045\ttotal: 33.8s\tremaining: 4.69s\n",
      "878:\tlearn: 6.0602861\ttotal: 33.8s\tremaining: 4.65s\n",
      "879:\tlearn: 6.0588237\ttotal: 33.8s\tremaining: 4.61s\n",
      "880:\tlearn: 6.0572161\ttotal: 33.9s\tremaining: 4.58s\n",
      "881:\tlearn: 6.0551177\ttotal: 33.9s\tremaining: 4.54s\n",
      "882:\tlearn: 6.0529084\ttotal: 33.9s\tremaining: 4.5s\n",
      "883:\tlearn: 6.0510455\ttotal: 34s\tremaining: 4.46s\n",
      "884:\tlearn: 6.0485731\ttotal: 34s\tremaining: 4.42s\n",
      "885:\tlearn: 6.0466701\ttotal: 34.1s\tremaining: 4.38s\n",
      "886:\tlearn: 6.0442729\ttotal: 34.1s\tremaining: 4.34s\n",
      "887:\tlearn: 6.0411284\ttotal: 34.1s\tremaining: 4.3s\n",
      "888:\tlearn: 6.0387224\ttotal: 34.2s\tremaining: 4.27s\n",
      "889:\tlearn: 6.0367768\ttotal: 34.2s\tremaining: 4.23s\n",
      "890:\tlearn: 6.0350803\ttotal: 34.2s\tremaining: 4.19s\n",
      "891:\tlearn: 6.0331263\ttotal: 34.3s\tremaining: 4.15s\n",
      "892:\tlearn: 6.0310683\ttotal: 34.3s\tremaining: 4.11s\n",
      "893:\tlearn: 6.0297101\ttotal: 34.4s\tremaining: 4.07s\n",
      "894:\tlearn: 6.0272884\ttotal: 34.4s\tremaining: 4.03s\n",
      "895:\tlearn: 6.0251428\ttotal: 34.4s\tremaining: 4s\n",
      "896:\tlearn: 6.0226699\ttotal: 34.5s\tremaining: 3.96s\n",
      "897:\tlearn: 6.0211315\ttotal: 34.5s\tremaining: 3.92s\n",
      "898:\tlearn: 6.0197892\ttotal: 34.6s\tremaining: 3.88s\n",
      "899:\tlearn: 6.0184263\ttotal: 34.6s\tremaining: 3.84s\n",
      "900:\tlearn: 6.0169330\ttotal: 34.6s\tremaining: 3.8s\n",
      "901:\tlearn: 6.0149466\ttotal: 34.7s\tremaining: 3.77s\n",
      "902:\tlearn: 6.0127016\ttotal: 34.7s\tremaining: 3.73s\n",
      "903:\tlearn: 6.0103174\ttotal: 34.7s\tremaining: 3.69s\n",
      "904:\tlearn: 6.0083384\ttotal: 34.8s\tremaining: 3.65s\n",
      "905:\tlearn: 6.0072980\ttotal: 34.8s\tremaining: 3.61s\n",
      "906:\tlearn: 6.0058000\ttotal: 34.8s\tremaining: 3.57s\n",
      "907:\tlearn: 6.0031467\ttotal: 34.9s\tremaining: 3.53s\n",
      "908:\tlearn: 6.0016450\ttotal: 34.9s\tremaining: 3.49s\n",
      "909:\tlearn: 6.0000063\ttotal: 34.9s\tremaining: 3.46s\n",
      "910:\tlearn: 5.9987108\ttotal: 35s\tremaining: 3.42s\n",
      "911:\tlearn: 5.9958770\ttotal: 35s\tremaining: 3.38s\n",
      "912:\tlearn: 5.9935890\ttotal: 35s\tremaining: 3.34s\n",
      "913:\tlearn: 5.9912538\ttotal: 35.1s\tremaining: 3.3s\n",
      "914:\tlearn: 5.9891892\ttotal: 35.1s\tremaining: 3.26s\n",
      "915:\tlearn: 5.9873814\ttotal: 35.1s\tremaining: 3.22s\n",
      "916:\tlearn: 5.9854524\ttotal: 35.2s\tremaining: 3.18s\n",
      "917:\tlearn: 5.9836785\ttotal: 35.2s\tremaining: 3.15s\n",
      "918:\tlearn: 5.9814040\ttotal: 35.3s\tremaining: 3.11s\n",
      "919:\tlearn: 5.9802427\ttotal: 35.3s\tremaining: 3.07s\n",
      "920:\tlearn: 5.9788465\ttotal: 35.3s\tremaining: 3.03s\n",
      "921:\tlearn: 5.9766415\ttotal: 35.4s\tremaining: 2.99s\n",
      "922:\tlearn: 5.9743440\ttotal: 35.4s\tremaining: 2.95s\n",
      "923:\tlearn: 5.9722832\ttotal: 35.4s\tremaining: 2.91s\n",
      "924:\tlearn: 5.9705851\ttotal: 35.5s\tremaining: 2.87s\n",
      "925:\tlearn: 5.9677141\ttotal: 35.5s\tremaining: 2.84s\n",
      "926:\tlearn: 5.9653660\ttotal: 35.5s\tremaining: 2.8s\n",
      "927:\tlearn: 5.9634247\ttotal: 35.6s\tremaining: 2.76s\n",
      "928:\tlearn: 5.9607447\ttotal: 35.6s\tremaining: 2.72s\n",
      "929:\tlearn: 5.9586115\ttotal: 35.6s\tremaining: 2.68s\n",
      "930:\tlearn: 5.9569178\ttotal: 35.7s\tremaining: 2.64s\n",
      "931:\tlearn: 5.9547373\ttotal: 35.7s\tremaining: 2.6s\n",
      "932:\tlearn: 5.9533976\ttotal: 35.7s\tremaining: 2.57s\n",
      "933:\tlearn: 5.9506160\ttotal: 35.8s\tremaining: 2.53s\n",
      "934:\tlearn: 5.9487114\ttotal: 35.8s\tremaining: 2.49s\n",
      "935:\tlearn: 5.9479060\ttotal: 35.9s\tremaining: 2.45s\n",
      "936:\tlearn: 5.9463807\ttotal: 35.9s\tremaining: 2.41s\n",
      "937:\tlearn: 5.9448180\ttotal: 35.9s\tremaining: 2.38s\n",
      "938:\tlearn: 5.9428291\ttotal: 36s\tremaining: 2.34s\n",
      "939:\tlearn: 5.9402095\ttotal: 36s\tremaining: 2.3s\n",
      "940:\tlearn: 5.9382944\ttotal: 36s\tremaining: 2.26s\n",
      "941:\tlearn: 5.9362586\ttotal: 36.1s\tremaining: 2.22s\n",
      "942:\tlearn: 5.9342628\ttotal: 36.1s\tremaining: 2.18s\n",
      "943:\tlearn: 5.9323555\ttotal: 36.1s\tremaining: 2.14s\n",
      "944:\tlearn: 5.9304334\ttotal: 36.2s\tremaining: 2.1s\n",
      "945:\tlearn: 5.9295785\ttotal: 36.2s\tremaining: 2.07s\n",
      "946:\tlearn: 5.9276735\ttotal: 36.2s\tremaining: 2.03s\n",
      "947:\tlearn: 5.9254066\ttotal: 36.3s\tremaining: 1.99s\n",
      "948:\tlearn: 5.9235859\ttotal: 36.3s\tremaining: 1.95s\n",
      "949:\tlearn: 5.9218587\ttotal: 36.4s\tremaining: 1.91s\n",
      "950:\tlearn: 5.9194257\ttotal: 36.4s\tremaining: 1.87s\n",
      "951:\tlearn: 5.9176585\ttotal: 36.4s\tremaining: 1.84s\n",
      "952:\tlearn: 5.9163357\ttotal: 36.5s\tremaining: 1.8s\n",
      "953:\tlearn: 5.9138380\ttotal: 36.5s\tremaining: 1.76s\n",
      "954:\tlearn: 5.9118816\ttotal: 36.5s\tremaining: 1.72s\n",
      "955:\tlearn: 5.9100381\ttotal: 36.6s\tremaining: 1.68s\n",
      "956:\tlearn: 5.9078207\ttotal: 36.6s\tremaining: 1.64s\n",
      "957:\tlearn: 5.9064676\ttotal: 36.6s\tremaining: 1.61s\n",
      "958:\tlearn: 5.9042505\ttotal: 36.7s\tremaining: 1.57s\n",
      "959:\tlearn: 5.9023845\ttotal: 36.7s\tremaining: 1.53s\n",
      "960:\tlearn: 5.9000165\ttotal: 36.7s\tremaining: 1.49s\n",
      "961:\tlearn: 5.8984742\ttotal: 36.8s\tremaining: 1.45s\n",
      "962:\tlearn: 5.8966209\ttotal: 36.8s\tremaining: 1.42s\n",
      "963:\tlearn: 5.8950425\ttotal: 36.9s\tremaining: 1.38s\n",
      "964:\tlearn: 5.8931565\ttotal: 36.9s\tremaining: 1.34s\n",
      "965:\tlearn: 5.8913125\ttotal: 36.9s\tremaining: 1.3s\n",
      "966:\tlearn: 5.8894271\ttotal: 37s\tremaining: 1.26s\n",
      "967:\tlearn: 5.8874388\ttotal: 37s\tremaining: 1.22s\n",
      "968:\tlearn: 5.8862210\ttotal: 37s\tremaining: 1.18s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "969:\tlearn: 5.8847764\ttotal: 37.1s\tremaining: 1.15s\n",
      "970:\tlearn: 5.8827821\ttotal: 37.1s\tremaining: 1.11s\n",
      "971:\tlearn: 5.8807989\ttotal: 37.1s\tremaining: 1.07s\n",
      "972:\tlearn: 5.8790856\ttotal: 37.2s\tremaining: 1.03s\n",
      "973:\tlearn: 5.8774693\ttotal: 37.2s\tremaining: 993ms\n",
      "974:\tlearn: 5.8751519\ttotal: 37.2s\tremaining: 955ms\n",
      "975:\tlearn: 5.8735908\ttotal: 37.3s\tremaining: 917ms\n",
      "976:\tlearn: 5.8716546\ttotal: 37.3s\tremaining: 878ms\n",
      "977:\tlearn: 5.8696849\ttotal: 37.3s\tremaining: 840ms\n",
      "978:\tlearn: 5.8676031\ttotal: 37.4s\tremaining: 802ms\n",
      "979:\tlearn: 5.8657630\ttotal: 37.4s\tremaining: 764ms\n",
      "980:\tlearn: 5.8641760\ttotal: 37.5s\tremaining: 725ms\n",
      "981:\tlearn: 5.8629108\ttotal: 37.5s\tremaining: 687ms\n",
      "982:\tlearn: 5.8612318\ttotal: 37.5s\tremaining: 649ms\n",
      "983:\tlearn: 5.8590696\ttotal: 37.6s\tremaining: 611ms\n",
      "984:\tlearn: 5.8573661\ttotal: 37.6s\tremaining: 572ms\n",
      "985:\tlearn: 5.8562728\ttotal: 37.6s\tremaining: 534ms\n",
      "986:\tlearn: 5.8544353\ttotal: 37.7s\tremaining: 496ms\n",
      "987:\tlearn: 5.8525713\ttotal: 37.7s\tremaining: 458ms\n",
      "988:\tlearn: 5.8503058\ttotal: 37.7s\tremaining: 420ms\n",
      "989:\tlearn: 5.8487639\ttotal: 37.8s\tremaining: 382ms\n",
      "990:\tlearn: 5.8467157\ttotal: 37.8s\tremaining: 343ms\n",
      "991:\tlearn: 5.8447704\ttotal: 37.8s\tremaining: 305ms\n",
      "992:\tlearn: 5.8442363\ttotal: 37.9s\tremaining: 267ms\n",
      "993:\tlearn: 5.8415805\ttotal: 37.9s\tremaining: 229ms\n",
      "994:\tlearn: 5.8414650\ttotal: 37.9s\tremaining: 191ms\n",
      "995:\tlearn: 5.8401034\ttotal: 38s\tremaining: 153ms\n",
      "996:\tlearn: 5.8381260\ttotal: 38s\tremaining: 114ms\n",
      "997:\tlearn: 5.8362236\ttotal: 38s\tremaining: 76.2ms\n",
      "998:\tlearn: 5.8350606\ttotal: 38.1s\tremaining: 38.1ms\n",
      "999:\tlearn: 5.8332704\ttotal: 38.1s\tremaining: 0us\n"
     ]
    }
   ],
   "source": [
    "# 전체 학습데이터로 재학습\n",
    "for i, m in enumerate(models):\n",
    "    m.fit(X_train_select, y_train)\n",
    "    models[i] = m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 예측모델\n",
    "pred = models[0].predict(X_test_select) * weights[0] + models[1].predict(X_test_select) * weights[1] + models[2].predict(X_test_select) * weights[2] +dnn_model.predict(X_test_select).flatten() * weights[3] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "842573936f8fbc2fc01cbbd7ca485be6cb3601bb"
   },
   "source": [
    "### Make Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'submission_0610_2207.csv' is ready to submit.\n"
     ]
    }
   ],
   "source": [
    "t = pd.Timestamp.now()\n",
    "fname = f\"submission_{t.month:02}{t.day:02}_{t.hour:02}{t.minute:02}.csv\"\n",
    "submissions = pd.concat([pd.Series(IDtest, name=\"custid\"), pd.Series(pred, name=\"age\")] ,axis=1)\n",
    "submissions.to_csv(fname, index=False)\n",
    "print(\"'{}' is ready to submit.\" .format(fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 앙상블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = pd.read_csv('../submissions/submission_0615_1105_xgb.csv', encoding='cp949')\n",
    "second = pd.read_csv('../submissions/submission_0615_1217_lgbm.csv', encoding='cp949')\n",
    "third = pd.read_csv('../submissions/submission_0615_catboosthyper.csv', encoding='cp949')\n",
    "four = pd.read_csv('../submissions/submission_06152016_8.5.csv', encoding='cp949')\n",
    "five = pd.read_csv('../submissions/submission_0615_1900_ridge.csv', encoding='cp949')\n",
    "sub=first.copy()\n",
    "sub['age']=first['age']*0.3+second['age']*0.3+third['age']*0.2+four['age']*0.1+five['age']*0.1\n",
    "sub.to_csv('all_sub1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "first = pd.read_csv('../submissions/merge_123_LGBM.csv', encoding='cp949')\n",
    "second = pd.read_csv('../submissions/merge_123_XGBM.csv', encoding='cp949')\n",
    "third= pd.read_csv('../submissions/merge_123_BR.csv', encoding='cp949')\n",
    "four = pd.read_csv('../submissions/merge_123_CATB.csv', encoding='cp949')\n",
    "five = pd.read_csv('../submissions/교수님_DNN.csv', encoding='cp949')\n",
    "sub=first.copy()\n",
    "sub['age']=first['age']*0.4+second['age']*0.2+third['age']*0.1+four['age']*0.2+five['age']*0.1\n",
    "sub.to_csv('all_sub2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 아래 폴더가 있다는 가정 하에 수행\n",
    "folder = 'submissions'\n",
    "nf = 0\n",
    "for f in os.listdir(folder):\n",
    "    ext = os.path.splitext(f)[-1]\n",
    "    if ext == '.csv': \n",
    "        s = pd.read_csv(folder+\"/\"+f)\n",
    "    else: \n",
    "        continue\n",
    "    if len(s.columns) !=2:\n",
    "        continue\n",
    "    if nf == 0: \n",
    "        slist = s\n",
    "    else: \n",
    "        slist = pd.merge(slist, s, on=\"custid\")\n",
    "    nf += 1\n",
    "\n",
    "p = 7.4    # 이 값에 따라 성능이 달라짐 (p=1: 산술평균, p>1: 멱평균)\n",
    "if nf >= 2:\n",
    "    pred = 0\n",
    "    for j in range(nf): pred = pred + slist.iloc[:,j+1]**p \n",
    "    pred = pred / nf    \n",
    "    pred = pred**(1/p)\n",
    "\n",
    "    submission = pd.DataFrame({'custid': slist.custid, 'age': pred})\n",
    "    t = pd.Timestamp.now()\n",
    "    fname = f\"p{p}mean_submission_{t.month:02}{t.day:02}_{t.hour:02}{t.minute:02}.csv\"\n",
    "    submission.to_csv(fname, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "끝"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
